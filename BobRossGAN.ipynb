{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bvKDwp_4jeB",
        "outputId": "27cab4f8-4e41-4b2c-8d42-7238cce5f83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n",
        "!ls resized_images | wc -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTMHeCkRlGQ0",
        "outputId": "7efcdc61-f61d-4f01-e66c-469e40bc0f04"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bobross       BobRossGAN.ipynb\tgenerated_images\n",
            "bobross2.zip  GAN_weights\tresized_images\n",
            "2612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggDWTYAw9Ltx"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PKWLG0lf9Iyz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGL-KJh56PSl",
        "outputId": "d265ef1b-29ba-4a98-99a4-c9345a3da8df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GANS Course/Bob Ross Generator\n"
          ]
        }
      ],
      "source": [
        "cd 'drive/MyDrive/GANS Course/Bob Ross Generator'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8auyIUt6-pv",
        "outputId": "9560becf-cae5-4c75-8f12-983f66152979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GANS Course/Bob Ross Generator\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D-KVKCW9P8-"
      },
      "source": [
        "# Setting up and formatting images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_K7_jaja6W3f"
      },
      "outputs": [],
      "source": [
        "#!unzip bobross2.zip -d bobross"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9wSgqDW98q8U"
      },
      "outputs": [],
      "source": [
        "images_path = \"/content/drive/MyDrive/GANS Course/Bob Ross Generator/bobross/bobross2/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7tC9RXx-5sg",
        "outputId": "68a66487-435f-4999-ce51-83baae78b68f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘resized_images’: File exists\n",
            "bobross       BobRossGAN.ipynb\tgenerated_images\n",
            "bobross2.zip  GAN_weights\tresized_images\n"
          ]
        }
      ],
      "source": [
        "!mkdir resized_images\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qCvdtuMaIy1N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oNtxczfx88CS"
      },
      "outputs": [],
      "source": [
        "\n",
        "#reshape_size = (64, 64)\n",
        "\n",
        "#i = 0\n",
        "#for image in os.listdir(images_path):\n",
        "#  print(image)\n",
        "#  img = cv2.imread(images_path + image)\n",
        " # img = cv2.resize(img, reshape_size)\n",
        " # cv2.imwrite(\"resized_images/%d.png\" % i, img)\n",
        " # i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E-NwNmcVBatr"
      },
      "outputs": [],
      "source": [
        "#!ls resized_images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4rNT9WJAFvz"
      },
      "source": [
        "# Parameters for NN and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sXF2nHQAJME",
        "outputId": "e5afbf11-17df-4dcd-c7b0-608850408ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "img_width = 64\n",
        "img_height = 64\n",
        "channels = 3\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam_0001 = Adam(lr=0.0001)\n",
        "adam_0004 = Adam(lr=0.0004)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ4Z-i5zB0Dm"
      },
      "source": [
        "# Building Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hMxpxddB3qr",
        "outputId": "26828189-bd01-4cb9-e865-805a1d41c353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16384)             1654784   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16384)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 16, 16, 128)      524416    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 64, 64, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 64, 64, 3)         3459      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,707,203\n",
            "Trainable params: 2,707,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(256 * 8 * 8, input_dim = latent_dim))\n",
        "  model.add(LeakyReLU(alpha = 0.2))\n",
        "  model.add(Reshape((8, 8, 256)))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding = 'same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding = 'same'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "generator = build_generator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOMVaXTgEupW"
      },
      "source": [
        "# Building Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acWtW8OXEnuO",
        "outputId": "71ce7a4c-ee44-4e89-bf04-bd6c7f8fcc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 64, 64, 256)       0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1048576)           0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1048576)           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1048577   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,566,977\n",
            "Trainable params: 1,566,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3,3), padding='same', input_shape=img_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  model.add(Conv2D(128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  model.add(Conv2D(256, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=adam_0004, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX225EXFHk2-"
      },
      "source": [
        "# Connecting Neural Networks to build GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7X5hT0hHpAG",
        "outputId": "2f54f41a-60de-4870-bb9d-2b8a05d5a871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 64, 64, 3)         2707203   \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 1)                 1566977   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,274,180\n",
            "Trainable params: 2,707,203\n",
            "Non-trainable params: 1,566,977\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_GAN(generator_mod, discriminator_mod):\n",
        "  GAN = Sequential()\n",
        "  discriminator_mod.trainable = False\n",
        "  GAN.add(generator_mod)\n",
        "  GAN.add(discriminator_mod)\n",
        "  GAN.summary()\n",
        "  GAN.compile(loss = 'binary_crossentropy', optimizer = adam_0001)\n",
        "  return GAN\n",
        "\n",
        "GAN = build_GAN(generator, discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ35l8DmygIT",
        "outputId": "06233f26-87ad-4cda-b318-4fc607d86110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GANS Course/Bob Ross Generator\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 64, 64, 3)         2707203   \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 1)                 1566977   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,274,180\n",
            "Trainable params: 2,707,203\n",
            "Non-trainable params: 1,566,977\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "# Loading existing GAN weights\n",
        "generator = load_model('GAN_weights/gen_0.00000250_weights')\n",
        "discriminator = load_model('GAN_weights/dis_0.00000250_weights')\n",
        "GAN = build_GAN(generator, discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WetfjTufJP-C"
      },
      "source": [
        "# Outputting Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lXVsN1RxJNl7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000250\n",
        "\n",
        "def save_imgs():\n",
        "  r, c = 3, 3\n",
        "  noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "  gen_imgs = generator.predict(noise)\n",
        "  global save_name\n",
        "  save_name += 0.00000010\n",
        "\n",
        "  gen_imgs = (gen_imgs + 1) / 2\n",
        "\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  cnt = 0\n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      axs[i,j].imshow(gen_imgs[cnt])\n",
        "      axs[i,j].axis('off')\n",
        "      cnt += 1\n",
        "  fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "  plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zra9sS1yQblC",
        "outputId": "f6c1368e-21fb-472e-e867-125f27315392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘generated_images’: File exists\n",
            "bobross       BobRossGAN.ipynb\tgenerated_images\n",
            "bobross2.zip  GAN_weights\tresized_images\n"
          ]
        }
      ],
      "source": [
        "!mkdir generated_images\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPThafdaJlQw"
      },
      "source": [
        "# Training GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MDqCtwsuLZ95"
      },
      "outputs": [],
      "source": [
        "def normalise_image_data(im_data):\n",
        "  return (im_data / 127.5) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hsfv_h_eJiyC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls resized_images | head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_2iORSOgRIH",
        "outputId": "b3de1adb-4e85-432f-c752-33025de6ae9c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.png\n",
            "1000.png\n",
            "1001.png\n",
            "1002.png\n",
            "1003.png\n",
            "1004.png\n",
            "1005.png\n",
            "1006.png\n",
            "1007.png\n",
            "1008.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbqEkPpYKW_Z",
        "outputId": "1cf2144a-8298-4936-b34b-58b9aebc60dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.png\n",
            "88.png\n",
            "87.png\n",
            "89.png\n",
            "90.png\n",
            "91.png\n",
            "92.png\n",
            "94.png\n",
            "93.png\n",
            "95.png\n",
            "96.png\n",
            "97.png\n",
            "98.png\n",
            "99.png\n",
            "100.png\n",
            "101.png\n",
            "103.png\n",
            "102.png\n",
            "104.png\n",
            "105.png\n",
            "106.png\n",
            "107.png\n",
            "108.png\n",
            "109.png\n",
            "110.png\n",
            "111.png\n",
            "113.png\n",
            "112.png\n",
            "114.png\n",
            "115.png\n",
            "116.png\n",
            "117.png\n",
            "118.png\n",
            "119.png\n",
            "120.png\n",
            "121.png\n",
            "122.png\n",
            "123.png\n",
            "124.png\n",
            "126.png\n",
            "125.png\n",
            "128.png\n",
            "127.png\n",
            "129.png\n",
            "130.png\n",
            "131.png\n",
            "132.png\n",
            "133.png\n",
            "134.png\n",
            "135.png\n",
            "136.png\n",
            "137.png\n",
            "139.png\n",
            "138.png\n",
            "141.png\n",
            "140.png\n",
            "142.png\n",
            "143.png\n",
            "144.png\n",
            "145.png\n",
            "146.png\n",
            "148.png\n",
            "147.png\n",
            "149.png\n",
            "150.png\n",
            "151.png\n",
            "152.png\n",
            "153.png\n",
            "155.png\n",
            "154.png\n",
            "157.png\n",
            "156.png\n",
            "158.png\n",
            "159.png\n",
            "160.png\n",
            "161.png\n",
            "162.png\n",
            "163.png\n",
            "164.png\n",
            "165.png\n",
            "166.png\n",
            "167.png\n",
            "168.png\n",
            "169.png\n",
            "170.png\n",
            "172.png\n",
            "171.png\n",
            "173.png\n",
            "174.png\n",
            "175.png\n",
            "176.png\n",
            "177.png\n",
            "178.png\n",
            "179.png\n",
            "180.png\n",
            "181.png\n",
            "182.png\n",
            "183.png\n",
            "185.png\n",
            "184.png\n",
            "187.png\n",
            "186.png\n",
            "189.png\n",
            "188.png\n",
            "190.png\n",
            "191.png\n",
            "192.png\n",
            "193.png\n",
            "194.png\n",
            "195.png\n",
            "196.png\n",
            "197.png\n",
            "198.png\n",
            "200.png\n",
            "199.png\n",
            "202.png\n",
            "201.png\n",
            "203.png\n",
            "204.png\n",
            "205.png\n",
            "207.png\n",
            "206.png\n",
            "208.png\n",
            "209.png\n",
            "211.png\n",
            "210.png\n",
            "212.png\n",
            "214.png\n",
            "213.png\n",
            "215.png\n",
            "216.png\n",
            "218.png\n",
            "217.png\n",
            "219.png\n",
            "220.png\n",
            "221.png\n",
            "222.png\n",
            "223.png\n",
            "224.png\n",
            "225.png\n",
            "226.png\n",
            "227.png\n",
            "228.png\n",
            "229.png\n",
            "230.png\n",
            "231.png\n",
            "232.png\n",
            "233.png\n",
            "234.png\n",
            "235.png\n",
            "236.png\n",
            "237.png\n",
            "238.png\n",
            "239.png\n",
            "240.png\n",
            "241.png\n",
            "242.png\n",
            "243.png\n",
            "244.png\n",
            "245.png\n",
            "246.png\n",
            "247.png\n",
            "248.png\n",
            "249.png\n",
            "251.png\n",
            "250.png\n",
            "252.png\n",
            "253.png\n",
            "254.png\n",
            "255.png\n",
            "257.png\n",
            "256.png\n",
            "259.png\n",
            "258.png\n",
            "261.png\n",
            "260.png\n",
            "263.png\n",
            "262.png\n",
            "264.png\n",
            "265.png\n",
            "267.png\n",
            "266.png\n",
            "268.png\n",
            "270.png\n",
            "269.png\n",
            "272.png\n",
            "271.png\n",
            "273.png\n",
            "274.png\n",
            "275.png\n",
            "276.png\n",
            "277.png\n",
            "278.png\n",
            "280.png\n",
            "279.png\n",
            "281.png\n",
            "282.png\n",
            "283.png\n",
            "284.png\n",
            "285.png\n",
            "286.png\n",
            "287.png\n",
            "288.png\n",
            "289.png\n",
            "291.png\n",
            "290.png\n",
            "292.png\n",
            "293.png\n",
            "294.png\n",
            "296.png\n",
            "295.png\n",
            "297.png\n",
            "298.png\n",
            "299.png\n",
            "300.png\n",
            "301.png\n",
            "302.png\n",
            "304.png\n",
            "303.png\n",
            "305.png\n",
            "306.png\n",
            "307.png\n",
            "308.png\n",
            "309.png\n",
            "311.png\n",
            "310.png\n",
            "313.png\n",
            "312.png\n",
            "314.png\n",
            "316.png\n",
            "315.png\n",
            "317.png\n",
            "318.png\n",
            "319.png\n",
            "320.png\n",
            "321.png\n",
            "322.png\n",
            "323.png\n",
            "324.png\n",
            "325.png\n",
            "327.png\n",
            "326.png\n",
            "329.png\n",
            "328.png\n",
            "330.png\n",
            "331.png\n",
            "332.png\n",
            "333.png\n",
            "334.png\n",
            "336.png\n",
            "335.png\n",
            "338.png\n",
            "337.png\n",
            "339.png\n",
            "340.png\n",
            "341.png\n",
            "342.png\n",
            "343.png\n",
            "344.png\n",
            "345.png\n",
            "346.png\n",
            "347.png\n",
            "348.png\n",
            "349.png\n",
            "350.png\n",
            "351.png\n",
            "352.png\n",
            "353.png\n",
            "354.png\n",
            "355.png\n",
            "356.png\n",
            "357.png\n",
            "359.png\n",
            "362.png\n",
            "363.png\n",
            "364.png\n",
            "361.png\n",
            "367.png\n",
            "365.png\n",
            "366.png\n",
            "360.png\n",
            "358.png\n",
            "368.png\n",
            "371.png\n",
            "370.png\n",
            "372.png\n",
            "369.png\n",
            "373.png\n",
            "374.png\n",
            "375.png\n",
            "376.png\n",
            "381.png\n",
            "382.png\n",
            "379.png\n",
            "380.png\n",
            "383.png\n",
            "378.png\n",
            "385.png\n",
            "384.png\n",
            "377.png\n",
            "386.png\n",
            "387.png\n",
            "388.png\n",
            "389.png\n",
            "390.png\n",
            "391.png\n",
            "393.png\n",
            "392.png\n",
            "394.png\n",
            "395.png\n",
            "396.png\n",
            "397.png\n",
            "398.png\n",
            "399.png\n",
            "400.png\n",
            "401.png\n",
            "402.png\n",
            "403.png\n",
            "404.png\n",
            "405.png\n",
            "406.png\n",
            "407.png\n",
            "408.png\n",
            "409.png\n",
            "411.png\n",
            "410.png\n",
            "412.png\n",
            "413.png\n",
            "415.png\n",
            "414.png\n",
            "416.png\n",
            "418.png\n",
            "417.png\n",
            "420.png\n",
            "419.png\n",
            "422.png\n",
            "421.png\n",
            "423.png\n",
            "424.png\n",
            "426.png\n",
            "425.png\n",
            "427.png\n",
            "428.png\n",
            "430.png\n",
            "429.png\n",
            "431.png\n",
            "432.png\n",
            "433.png\n",
            "434.png\n",
            "435.png\n",
            "436.png\n",
            "437.png\n",
            "438.png\n",
            "439.png\n",
            "440.png\n",
            "441.png\n",
            "442.png\n",
            "444.png\n",
            "443.png\n",
            "445.png\n",
            "446.png\n",
            "447.png\n",
            "448.png\n",
            "449.png\n",
            "450.png\n",
            "451.png\n",
            "452.png\n",
            "454.png\n",
            "453.png\n",
            "456.png\n",
            "455.png\n",
            "458.png\n",
            "457.png\n",
            "459.png\n",
            "461.png\n",
            "460.png\n",
            "462.png\n",
            "463.png\n",
            "464.png\n",
            "467.png\n",
            "465.png\n",
            "466.png\n",
            "468.png\n",
            "469.png\n",
            "470.png\n",
            "471.png\n",
            "472.png\n",
            "473.png\n",
            "474.png\n",
            "475.png\n",
            "476.png\n",
            "477.png\n",
            "478.png\n",
            "479.png\n",
            "480.png\n",
            "481.png\n",
            "482.png\n",
            "483.png\n",
            "484.png\n",
            "485.png\n",
            "487.png\n",
            "486.png\n",
            "488.png\n",
            "489.png\n",
            "490.png\n",
            "491.png\n",
            "492.png\n",
            "493.png\n",
            "494.png\n",
            "495.png\n",
            "497.png\n",
            "496.png\n",
            "498.png\n",
            "499.png\n",
            "500.png\n",
            "501.png\n",
            "502.png\n",
            "503.png\n",
            "504.png\n",
            "505.png\n",
            "506.png\n",
            "507.png\n",
            "508.png\n",
            "509.png\n",
            "510.png\n",
            "511.png\n",
            "513.png\n",
            "512.png\n",
            "514.png\n",
            "515.png\n",
            "516.png\n",
            "517.png\n",
            "518.png\n",
            "519.png\n",
            "520.png\n",
            "521.png\n",
            "522.png\n",
            "523.png\n",
            "524.png\n",
            "525.png\n",
            "526.png\n",
            "528.png\n",
            "527.png\n",
            "529.png\n",
            "531.png\n",
            "530.png\n",
            "532.png\n",
            "533.png\n",
            "534.png\n",
            "535.png\n",
            "536.png\n",
            "537.png\n",
            "538.png\n",
            "539.png\n",
            "540.png\n",
            "541.png\n",
            "542.png\n",
            "543.png\n",
            "544.png\n",
            "546.png\n",
            "545.png\n",
            "548.png\n",
            "547.png\n",
            "549.png\n",
            "550.png\n",
            "552.png\n",
            "551.png\n",
            "553.png\n",
            "554.png\n",
            "555.png\n",
            "556.png\n",
            "557.png\n",
            "558.png\n",
            "559.png\n",
            "560.png\n",
            "561.png\n",
            "562.png\n",
            "563.png\n",
            "564.png\n",
            "565.png\n",
            "566.png\n",
            "567.png\n",
            "568.png\n",
            "569.png\n",
            "570.png\n",
            "571.png\n",
            "572.png\n",
            "574.png\n",
            "573.png\n",
            "576.png\n",
            "575.png\n",
            "577.png\n",
            "578.png\n",
            "579.png\n",
            "581.png\n",
            "580.png\n",
            "582.png\n",
            "583.png\n",
            "585.png\n",
            "584.png\n",
            "587.png\n",
            "588.png\n",
            "586.png\n",
            "590.png\n",
            "589.png\n",
            "591.png\n",
            "592.png\n",
            "594.png\n",
            "593.png\n",
            "595.png\n",
            "596.png\n",
            "597.png\n",
            "598.png\n",
            "599.png\n",
            "600.png\n",
            "601.png\n",
            "603.png\n",
            "602.png\n",
            "604.png\n",
            "605.png\n",
            "606.png\n",
            "607.png\n",
            "608.png\n",
            "610.png\n",
            "609.png\n",
            "611.png\n",
            "612.png\n",
            "613.png\n",
            "614.png\n",
            "615.png\n",
            "616.png\n",
            "617.png\n",
            "618.png\n",
            "619.png\n",
            "620.png\n",
            "622.png\n",
            "621.png\n",
            "624.png\n",
            "623.png\n",
            "625.png\n",
            "626.png\n",
            "627.png\n",
            "628.png\n",
            "629.png\n",
            "630.png\n",
            "631.png\n",
            "632.png\n",
            "633.png\n",
            "634.png\n",
            "635.png\n",
            "637.png\n",
            "636.png\n",
            "639.png\n",
            "638.png\n",
            "640.png\n",
            "641.png\n",
            "642.png\n",
            "643.png\n",
            "644.png\n",
            "645.png\n",
            "646.png\n",
            "647.png\n",
            "649.png\n",
            "648.png\n",
            "651.png\n",
            "650.png\n",
            "652.png\n",
            "653.png\n",
            "654.png\n",
            "655.png\n",
            "656.png\n",
            "657.png\n",
            "658.png\n",
            "659.png\n",
            "660.png\n",
            "661.png\n",
            "662.png\n",
            "663.png\n",
            "664.png\n",
            "665.png\n",
            "666.png\n",
            "667.png\n",
            "668.png\n",
            "669.png\n",
            "670.png\n",
            "671.png\n",
            "672.png\n",
            "673.png\n",
            "674.png\n",
            "675.png\n",
            "676.png\n",
            "677.png\n",
            "679.png\n",
            "678.png\n",
            "681.png\n",
            "683.png\n",
            "682.png\n",
            "685.png\n",
            "684.png\n",
            "680.png\n",
            "686.png\n",
            "687.png\n",
            "688.png\n",
            "689.png\n",
            "690.png\n",
            "691.png\n",
            "692.png\n",
            "693.png\n",
            "694.png\n",
            "695.png\n",
            "696.png\n",
            "697.png\n",
            "698.png\n",
            "699.png\n",
            "700.png\n",
            "701.png\n",
            "702.png\n",
            "703.png\n",
            "704.png\n",
            "706.png\n",
            "705.png\n",
            "707.png\n",
            "708.png\n",
            "710.png\n",
            "711.png\n",
            "709.png\n",
            "713.png\n",
            "712.png\n",
            "715.png\n",
            "714.png\n",
            "716.png\n",
            "717.png\n",
            "718.png\n",
            "719.png\n",
            "720.png\n",
            "721.png\n",
            "722.png\n",
            "724.png\n",
            "723.png\n",
            "725.png\n",
            "726.png\n",
            "727.png\n",
            "728.png\n",
            "729.png\n",
            "730.png\n",
            "731.png\n",
            "733.png\n",
            "735.png\n",
            "734.png\n",
            "732.png\n",
            "736.png\n",
            "737.png\n",
            "738.png\n",
            "739.png\n",
            "740.png\n",
            "741.png\n",
            "742.png\n",
            "743.png\n",
            "744.png\n",
            "745.png\n",
            "746.png\n",
            "747.png\n",
            "748.png\n",
            "750.png\n",
            "749.png\n",
            "751.png\n",
            "752.png\n",
            "753.png\n",
            "754.png\n",
            "755.png\n",
            "757.png\n",
            "756.png\n",
            "758.png\n",
            "759.png\n",
            "760.png\n",
            "761.png\n",
            "762.png\n",
            "764.png\n",
            "763.png\n",
            "765.png\n",
            "766.png\n",
            "768.png\n",
            "767.png\n",
            "770.png\n",
            "771.png\n",
            "769.png\n",
            "773.png\n",
            "772.png\n",
            "775.png\n",
            "774.png\n",
            "776.png\n",
            "777.png\n",
            "778.png\n",
            "779.png\n",
            "780.png\n",
            "782.png\n",
            "781.png\n",
            "783.png\n",
            "784.png\n",
            "785.png\n",
            "786.png\n",
            "787.png\n",
            "788.png\n",
            "789.png\n",
            "790.png\n",
            "792.png\n",
            "791.png\n",
            "793.png\n",
            "794.png\n",
            "796.png\n",
            "795.png\n",
            "797.png\n",
            "798.png\n",
            "799.png\n",
            "800.png\n",
            "801.png\n",
            "803.png\n",
            "802.png\n",
            "805.png\n",
            "804.png\n",
            "806.png\n",
            "807.png\n",
            "808.png\n",
            "809.png\n",
            "810.png\n",
            "811.png\n",
            "812.png\n",
            "813.png\n",
            "814.png\n",
            "815.png\n",
            "816.png\n",
            "817.png\n",
            "818.png\n",
            "820.png\n",
            "819.png\n",
            "822.png\n",
            "821.png\n",
            "824.png\n",
            "823.png\n",
            "826.png\n",
            "825.png\n",
            "827.png\n",
            "828.png\n",
            "829.png\n",
            "830.png\n",
            "831.png\n",
            "832.png\n",
            "833.png\n",
            "834.png\n",
            "835.png\n",
            "836.png\n",
            "837.png\n",
            "838.png\n",
            "839.png\n",
            "840.png\n",
            "841.png\n",
            "843.png\n",
            "842.png\n",
            "844.png\n",
            "845.png\n",
            "846.png\n",
            "847.png\n",
            "848.png\n",
            "849.png\n",
            "850.png\n",
            "851.png\n",
            "852.png\n",
            "854.png\n",
            "853.png\n",
            "856.png\n",
            "855.png\n",
            "858.png\n",
            "857.png\n",
            "860.png\n",
            "859.png\n",
            "861.png\n",
            "862.png\n",
            "864.png\n",
            "863.png\n",
            "865.png\n",
            "866.png\n",
            "867.png\n",
            "868.png\n",
            "869.png\n",
            "870.png\n",
            "871.png\n",
            "872.png\n",
            "873.png\n",
            "874.png\n",
            "875.png\n",
            "876.png\n",
            "877.png\n",
            "878.png\n",
            "880.png\n",
            "879.png\n",
            "881.png\n",
            "882.png\n",
            "883.png\n",
            "884.png\n",
            "885.png\n",
            "886.png\n",
            "887.png\n",
            "889.png\n",
            "888.png\n",
            "890.png\n",
            "891.png\n",
            "892.png\n",
            "893.png\n",
            "894.png\n",
            "895.png\n",
            "896.png\n",
            "897.png\n",
            "898.png\n",
            "900.png\n",
            "899.png\n",
            "901.png\n",
            "902.png\n",
            "903.png\n",
            "904.png\n",
            "905.png\n",
            "906.png\n",
            "907.png\n",
            "908.png\n",
            "909.png\n",
            "910.png\n",
            "912.png\n",
            "911.png\n",
            "913.png\n",
            "914.png\n",
            "916.png\n",
            "915.png\n",
            "917.png\n",
            "919.png\n",
            "918.png\n",
            "920.png\n",
            "921.png\n",
            "922.png\n",
            "924.png\n",
            "923.png\n",
            "925.png\n",
            "926.png\n",
            "927.png\n",
            "928.png\n",
            "929.png\n",
            "930.png\n",
            "931.png\n",
            "932.png\n",
            "933.png\n",
            "934.png\n",
            "935.png\n",
            "936.png\n",
            "937.png\n",
            "938.png\n",
            "939.png\n",
            "940.png\n",
            "941.png\n",
            "942.png\n",
            "943.png\n",
            "944.png\n",
            "945.png\n",
            "948.png\n",
            "949.png\n",
            "947.png\n",
            "946.png\n",
            "951.png\n",
            "950.png\n",
            "952.png\n",
            "953.png\n",
            "954.png\n",
            "955.png\n",
            "956.png\n",
            "957.png\n",
            "958.png\n",
            "959.png\n",
            "960.png\n",
            "961.png\n",
            "962.png\n",
            "963.png\n",
            "964.png\n",
            "965.png\n",
            "966.png\n",
            "967.png\n",
            "969.png\n",
            "968.png\n",
            "970.png\n",
            "971.png\n",
            "972.png\n",
            "973.png\n",
            "974.png\n",
            "976.png\n",
            "975.png\n",
            "978.png\n",
            "977.png\n",
            "979.png\n",
            "980.png\n",
            "981.png\n",
            "982.png\n",
            "984.png\n",
            "983.png\n",
            "986.png\n",
            "985.png\n",
            "987.png\n",
            "988.png\n",
            "990.png\n",
            "989.png\n",
            "991.png\n",
            "992.png\n",
            "993.png\n",
            "995.png\n",
            "997.png\n",
            "996.png\n",
            "994.png\n",
            "999.png\n",
            "998.png\n",
            "1000.png\n",
            "1001.png\n",
            "1002.png\n",
            "1003.png\n",
            "1004.png\n",
            "1005.png\n",
            "1006.png\n",
            "1008.png\n",
            "1007.png\n",
            "1009.png\n",
            "1010.png\n",
            "1011.png\n",
            "1013.png\n",
            "1012.png\n",
            "1015.png\n",
            "1014.png\n",
            "1016.png\n",
            "1017.png\n",
            "1018.png\n",
            "1019.png\n",
            "1020.png\n",
            "1021.png\n",
            "1022.png\n",
            "1024.png\n",
            "1023.png\n",
            "1025.png\n",
            "1026.png\n",
            "1027.png\n",
            "1029.png\n",
            "1028.png\n",
            "1031.png\n",
            "1030.png\n",
            "1032.png\n",
            "1033.png\n",
            "1035.png\n",
            "1034.png\n",
            "1036.png\n",
            "1037.png\n",
            "1038.png\n",
            "1039.png\n",
            "1040.png\n",
            "1041.png\n",
            "1042.png\n",
            "1043.png\n",
            "1044.png\n",
            "1046.png\n",
            "1045.png\n",
            "1047.png\n",
            "1048.png\n",
            "1049.png\n",
            "1051.png\n",
            "1050.png\n",
            "1052.png\n",
            "1053.png\n",
            "1054.png\n",
            "1055.png\n",
            "1056.png\n",
            "1057.png\n",
            "1058.png\n",
            "1059.png\n",
            "1060.png\n",
            "1061.png\n",
            "1062.png\n",
            "1063.png\n",
            "1064.png\n",
            "1065.png\n",
            "1066.png\n",
            "1067.png\n",
            "1068.png\n",
            "1069.png\n",
            "1070.png\n",
            "1071.png\n",
            "1072.png\n",
            "1073.png\n",
            "1075.png\n",
            "1074.png\n",
            "1076.png\n",
            "1077.png\n",
            "1078.png\n",
            "1079.png\n",
            "1081.png\n",
            "1080.png\n",
            "1083.png\n",
            "1082.png\n",
            "1084.png\n",
            "1085.png\n",
            "1698.png\n",
            "1699.png\n",
            "1700.png\n",
            "1701.png\n",
            "1702.png\n",
            "1703.png\n",
            "1704.png\n",
            "1705.png\n",
            "1706.png\n",
            "1707.png\n",
            "1708.png\n",
            "1709.png\n",
            "1710.png\n",
            "1711.png\n",
            "1712.png\n",
            "1713.png\n",
            "1714.png\n",
            "1715.png\n",
            "1716.png\n",
            "1717.png\n",
            "1718.png\n",
            "1719.png\n",
            "1720.png\n",
            "1721.png\n",
            "1722.png\n",
            "1723.png\n",
            "1724.png\n",
            "1725.png\n",
            "1726.png\n",
            "1727.png\n",
            "1728.png\n",
            "1729.png\n",
            "1730.png\n",
            "1731.png\n",
            "1732.png\n",
            "1733.png\n",
            "1734.png\n",
            "1735.png\n",
            "1736.png\n",
            "1737.png\n",
            "1738.png\n",
            "1739.png\n",
            "1740.png\n",
            "1741.png\n",
            "1742.png\n",
            "1743.png\n",
            "1744.png\n",
            "1745.png\n",
            "1746.png\n",
            "1747.png\n",
            "1748.png\n",
            "1749.png\n",
            "1750.png\n",
            "1751.png\n",
            "1752.png\n",
            "1753.png\n",
            "1754.png\n",
            "1755.png\n",
            "1756.png\n",
            "1757.png\n",
            "1758.png\n",
            "1759.png\n",
            "1760.png\n",
            "1761.png\n",
            "1762.png\n",
            "1763.png\n",
            "1764.png\n",
            "1765.png\n",
            "1766.png\n",
            "1767.png\n",
            "1768.png\n",
            "1769.png\n",
            "1770.png\n",
            "1771.png\n",
            "1772.png\n",
            "1773.png\n",
            "1774.png\n",
            "1775.png\n",
            "1776.png\n",
            "1777.png\n",
            "1778.png\n",
            "1779.png\n",
            "1780.png\n",
            "1781.png\n",
            "1782.png\n",
            "1783.png\n",
            "1784.png\n",
            "1785.png\n",
            "1786.png\n",
            "1787.png\n",
            "1788.png\n",
            "1789.png\n",
            "1790.png\n",
            "1791.png\n",
            "1792.png\n",
            "1793.png\n",
            "1794.png\n",
            "1795.png\n",
            "1796.png\n",
            "1797.png\n",
            "1798.png\n",
            "1799.png\n",
            "1800.png\n",
            "1801.png\n",
            "1802.png\n",
            "1803.png\n",
            "1804.png\n",
            "1805.png\n",
            "1806.png\n",
            "1807.png\n",
            "1808.png\n",
            "1809.png\n",
            "1810.png\n",
            "1811.png\n",
            "1812.png\n",
            "1813.png\n",
            "1814.png\n",
            "1815.png\n",
            "1816.png\n",
            "1817.png\n",
            "1818.png\n",
            "1819.png\n",
            "1820.png\n",
            "1821.png\n",
            "1822.png\n",
            "1823.png\n",
            "1824.png\n",
            "1825.png\n",
            "1826.png\n",
            "1827.png\n",
            "1828.png\n",
            "1829.png\n",
            "1830.png\n",
            "1831.png\n",
            "1832.png\n",
            "1833.png\n",
            "1834.png\n",
            "1835.png\n",
            "1836.png\n",
            "1837.png\n",
            "1838.png\n",
            "1839.png\n",
            "1840.png\n",
            "1841.png\n",
            "1842.png\n",
            "1843.png\n",
            "1844.png\n",
            "1845.png\n",
            "1846.png\n",
            "1847.png\n",
            "1848.png\n",
            "1849.png\n",
            "1850.png\n",
            "1851.png\n",
            "1852.png\n",
            "1853.png\n",
            "1854.png\n",
            "1855.png\n",
            "1856.png\n",
            "1857.png\n",
            "1858.png\n",
            "1859.png\n",
            "1860.png\n",
            "1861.png\n",
            "1862.png\n",
            "1863.png\n",
            "1864.png\n",
            "1865.png\n",
            "1866.png\n",
            "1867.png\n",
            "1868.png\n",
            "1869.png\n",
            "1870.png\n",
            "1871.png\n",
            "1872.png\n",
            "1873.png\n",
            "1874.png\n",
            "1875.png\n",
            "1876.png\n",
            "1877.png\n",
            "1878.png\n",
            "1879.png\n",
            "1880.png\n",
            "1881.png\n",
            "1882.png\n",
            "1883.png\n",
            "1884.png\n",
            "1885.png\n",
            "1886.png\n",
            "1887.png\n",
            "1888.png\n",
            "1889.png\n",
            "1890.png\n",
            "1891.png\n",
            "1892.png\n",
            "1893.png\n",
            "1894.png\n",
            "1895.png\n",
            "1896.png\n",
            "1897.png\n",
            "1898.png\n",
            "1899.png\n",
            "1900.png\n",
            "1901.png\n",
            "1902.png\n",
            "1903.png\n",
            "1904.png\n",
            "1905.png\n",
            "1906.png\n",
            "1907.png\n",
            "1908.png\n",
            "1909.png\n",
            "1910.png\n",
            "1911.png\n",
            "1912.png\n",
            "1913.png\n",
            "1914.png\n",
            "1915.png\n",
            "1916.png\n",
            "1917.png\n",
            "1918.png\n",
            "1919.png\n",
            "1920.png\n",
            "1921.png\n",
            "1922.png\n",
            "1923.png\n",
            "1924.png\n",
            "1925.png\n",
            "1926.png\n",
            "1927.png\n",
            "1928.png\n",
            "1929.png\n",
            "1930.png\n",
            "1931.png\n",
            "1932.png\n",
            "1933.png\n",
            "1934.png\n",
            "1935.png\n",
            "1936.png\n",
            "1937.png\n",
            "1938.png\n",
            "1939.png\n",
            "1940.png\n",
            "1941.png\n",
            "1942.png\n",
            "1943.png\n",
            "1944.png\n",
            "1945.png\n",
            "1946.png\n",
            "1947.png\n",
            "1948.png\n",
            "1949.png\n",
            "1950.png\n",
            "1951.png\n",
            "1952.png\n",
            "1953.png\n",
            "1954.png\n",
            "1955.png\n",
            "1956.png\n",
            "1957.png\n",
            "1958.png\n",
            "1959.png\n",
            "1960.png\n",
            "1961.png\n",
            "1962.png\n",
            "1963.png\n",
            "1964.png\n",
            "1965.png\n",
            "1966.png\n",
            "1967.png\n",
            "1968.png\n",
            "1969.png\n",
            "1970.png\n",
            "1971.png\n",
            "1972.png\n",
            "1973.png\n",
            "1974.png\n",
            "1975.png\n",
            "1976.png\n",
            "1977.png\n",
            "1978.png\n",
            "1979.png\n",
            "1980.png\n",
            "1981.png\n",
            "1982.png\n",
            "1983.png\n",
            "1984.png\n",
            "1985.png\n",
            "1986.png\n",
            "1987.png\n",
            "1988.png\n",
            "1989.png\n",
            "1990.png\n",
            "1991.png\n",
            "1992.png\n",
            "1993.png\n",
            "1994.png\n",
            "1995.png\n",
            "1996.png\n",
            "1997.png\n",
            "1998.png\n",
            "1999.png\n",
            "2000.png\n",
            "2001.png\n",
            "2002.png\n",
            "2003.png\n",
            "2004.png\n",
            "2005.png\n",
            "2006.png\n",
            "2007.png\n",
            "2008.png\n",
            "2009.png\n",
            "2010.png\n",
            "2011.png\n",
            "2012.png\n",
            "2013.png\n",
            "2014.png\n",
            "2015.png\n",
            "2016.png\n",
            "2017.png\n",
            "2018.png\n",
            "2019.png\n",
            "2020.png\n",
            "2021.png\n",
            "2022.png\n",
            "2023.png\n",
            "2024.png\n",
            "2025.png\n",
            "2026.png\n",
            "2027.png\n",
            "2028.png\n",
            "2029.png\n",
            "2030.png\n",
            "2031.png\n",
            "2032.png\n",
            "2033.png\n",
            "2034.png\n",
            "2035.png\n",
            "2036.png\n",
            "2037.png\n",
            "2038.png\n",
            "2039.png\n",
            "2040.png\n",
            "2041.png\n",
            "2042.png\n",
            "2043.png\n",
            "2044.png\n",
            "2045.png\n",
            "2046.png\n",
            "2047.png\n",
            "2048.png\n",
            "2049.png\n",
            "2050.png\n",
            "2051.png\n",
            "2052.png\n",
            "2053.png\n",
            "2054.png\n",
            "2055.png\n",
            "2056.png\n",
            "2057.png\n",
            "2058.png\n",
            "2059.png\n",
            "2060.png\n",
            "2061.png\n",
            "2062.png\n",
            "2063.png\n",
            "2064.png\n",
            "2065.png\n",
            "2066.png\n",
            "2067.png\n",
            "2068.png\n",
            "2069.png\n",
            "2070.png\n",
            "2071.png\n",
            "2072.png\n",
            "2073.png\n",
            "2074.png\n",
            "2075.png\n",
            "2076.png\n",
            "2077.png\n",
            "2078.png\n",
            "2079.png\n",
            "2080.png\n",
            "2081.png\n",
            "2082.png\n",
            "2083.png\n",
            "2084.png\n",
            "2085.png\n",
            "2086.png\n",
            "2087.png\n",
            "2088.png\n",
            "2089.png\n",
            "2090.png\n",
            "2091.png\n",
            "2092.png\n",
            "2093.png\n",
            "2094.png\n",
            "2095.png\n",
            "2096.png\n",
            "2097.png\n",
            "2098.png\n",
            "2099.png\n",
            "2100.png\n",
            "2101.png\n",
            "2102.png\n",
            "2103.png\n",
            "2104.png\n",
            "2105.png\n",
            "2106.png\n",
            "2107.png\n",
            "2108.png\n",
            "2109.png\n",
            "2110.png\n",
            "2111.png\n",
            "2112.png\n",
            "2113.png\n",
            "2114.png\n",
            "2115.png\n",
            "2116.png\n",
            "2117.png\n",
            "2118.png\n",
            "2119.png\n",
            "2120.png\n",
            "2121.png\n",
            "2122.png\n",
            "2123.png\n",
            "2124.png\n",
            "2125.png\n",
            "2126.png\n",
            "2127.png\n",
            "2128.png\n",
            "2129.png\n",
            "2130.png\n",
            "2131.png\n",
            "2132.png\n",
            "2133.png\n",
            "2134.png\n",
            "2135.png\n",
            "2136.png\n",
            "2137.png\n",
            "2138.png\n",
            "2139.png\n",
            "2140.png\n",
            "2141.png\n",
            "2142.png\n",
            "2143.png\n",
            "2144.png\n",
            "2145.png\n",
            "2146.png\n",
            "2147.png\n",
            "2148.png\n",
            "2149.png\n",
            "2150.png\n",
            "2151.png\n",
            "2152.png\n",
            "2153.png\n",
            "2154.png\n",
            "2155.png\n",
            "2156.png\n",
            "2157.png\n",
            "2158.png\n",
            "2159.png\n",
            "2160.png\n",
            "2161.png\n",
            "2162.png\n",
            "2163.png\n",
            "2164.png\n",
            "2165.png\n",
            "2166.png\n",
            "2167.png\n",
            "2168.png\n",
            "2169.png\n",
            "2170.png\n",
            "2171.png\n",
            "2172.png\n",
            "2173.png\n",
            "2174.png\n",
            "2175.png\n",
            "2176.png\n",
            "2177.png\n",
            "2178.png\n",
            "2179.png\n",
            "2180.png\n",
            "2181.png\n",
            "2182.png\n",
            "2183.png\n",
            "2184.png\n",
            "2185.png\n",
            "2186.png\n",
            "2187.png\n",
            "2188.png\n",
            "2189.png\n",
            "2190.png\n",
            "2191.png\n",
            "2192.png\n",
            "2193.png\n",
            "2194.png\n",
            "2195.png\n",
            "2196.png\n",
            "2197.png\n",
            "2198.png\n",
            "2199.png\n",
            "2200.png\n",
            "2201.png\n",
            "2202.png\n",
            "2203.png\n",
            "2204.png\n",
            "2205.png\n",
            "2206.png\n",
            "2207.png\n",
            "2208.png\n",
            "2209.png\n",
            "2210.png\n",
            "2211.png\n",
            "2212.png\n",
            "2213.png\n",
            "2214.png\n",
            "2215.png\n",
            "2216.png\n",
            "2217.png\n",
            "2218.png\n",
            "2219.png\n",
            "2220.png\n",
            "2221.png\n",
            "2222.png\n",
            "2223.png\n",
            "2224.png\n",
            "2225.png\n",
            "2226.png\n",
            "2227.png\n",
            "2228.png\n",
            "2229.png\n",
            "2230.png\n",
            "2231.png\n",
            "2232.png\n",
            "2233.png\n",
            "2234.png\n",
            "2235.png\n",
            "2236.png\n",
            "2237.png\n",
            "2238.png\n",
            "2239.png\n",
            "2240.png\n",
            "2241.png\n",
            "2242.png\n",
            "2243.png\n",
            "2244.png\n",
            "2245.png\n",
            "2246.png\n",
            "2247.png\n",
            "2248.png\n",
            "2249.png\n",
            "2250.png\n",
            "2251.png\n",
            "2252.png\n",
            "2253.png\n",
            "2254.png\n",
            "2255.png\n",
            "2256.png\n",
            "2257.png\n",
            "2258.png\n",
            "2259.png\n",
            "2260.png\n",
            "2261.png\n",
            "2262.png\n",
            "2263.png\n",
            "2264.png\n",
            "2265.png\n",
            "2266.png\n",
            "2267.png\n",
            "2268.png\n",
            "2269.png\n",
            "2270.png\n",
            "2271.png\n",
            "2272.png\n",
            "2273.png\n",
            "2274.png\n",
            "2275.png\n",
            "2276.png\n",
            "2277.png\n",
            "2278.png\n",
            "2279.png\n",
            "2280.png\n",
            "2281.png\n",
            "2282.png\n",
            "2283.png\n",
            "2284.png\n",
            "2285.png\n",
            "2286.png\n",
            "2287.png\n",
            "2288.png\n",
            "2289.png\n",
            "2290.png\n",
            "2291.png\n",
            "2292.png\n",
            "2293.png\n",
            "2294.png\n",
            "2295.png\n",
            "2296.png\n",
            "2297.png\n",
            "2298.png\n",
            "2299.png\n",
            "2300.png\n",
            "2301.png\n",
            "2302.png\n",
            "2303.png\n",
            "2304.png\n",
            "2305.png\n",
            "2306.png\n",
            "2307.png\n",
            "2308.png\n",
            "2309.png\n",
            "2310.png\n",
            "2311.png\n",
            "2312.png\n",
            "2313.png\n",
            "2314.png\n",
            "2315.png\n",
            "2316.png\n",
            "2317.png\n",
            "2318.png\n",
            "2319.png\n",
            "2320.png\n",
            "2321.png\n",
            "2322.png\n",
            "2323.png\n",
            "2324.png\n",
            "2325.png\n",
            "2326.png\n",
            "2327.png\n",
            "2328.png\n",
            "2329.png\n",
            "2330.png\n",
            "2331.png\n",
            "2332.png\n",
            "2333.png\n",
            "2334.png\n",
            "2335.png\n",
            "2336.png\n",
            "2337.png\n",
            "2338.png\n",
            "2339.png\n",
            "2340.png\n",
            "2341.png\n",
            "2342.png\n",
            "2343.png\n",
            "2344.png\n",
            "2345.png\n",
            "2346.png\n",
            "2347.png\n",
            "2348.png\n",
            "2349.png\n",
            "2350.png\n",
            "2351.png\n",
            "2352.png\n",
            "2353.png\n",
            "2354.png\n",
            "2355.png\n",
            "2356.png\n",
            "2357.png\n",
            "2358.png\n",
            "2359.png\n",
            "2360.png\n",
            "2361.png\n",
            "2362.png\n",
            "2363.png\n",
            "2364.png\n",
            "2365.png\n",
            "2366.png\n",
            "2367.png\n",
            "2368.png\n",
            "2369.png\n",
            "2370.png\n",
            "2371.png\n",
            "2372.png\n",
            "2373.png\n",
            "2374.png\n",
            "2375.png\n",
            "2376.png\n",
            "2377.png\n",
            "2378.png\n",
            "2379.png\n",
            "2380.png\n",
            "2381.png\n",
            "2382.png\n",
            "2383.png\n",
            "2384.png\n",
            "2385.png\n",
            "2386.png\n",
            "2387.png\n",
            "2388.png\n",
            "2389.png\n",
            "2390.png\n",
            "2391.png\n",
            "2392.png\n",
            "2393.png\n",
            "2394.png\n",
            "2395.png\n",
            "2396.png\n",
            "2397.png\n",
            "2398.png\n",
            "2399.png\n",
            "2400.png\n",
            "2401.png\n",
            "2402.png\n",
            "2403.png\n",
            "2404.png\n",
            "2405.png\n",
            "2406.png\n",
            "2407.png\n",
            "2408.png\n",
            "2409.png\n",
            "2410.png\n",
            "2411.png\n",
            "2412.png\n",
            "2413.png\n",
            "2414.png\n",
            "2415.png\n",
            "2416.png\n",
            "2417.png\n",
            "2418.png\n",
            "2419.png\n",
            "2420.png\n",
            "2421.png\n",
            "2422.png\n",
            "2423.png\n",
            "2424.png\n",
            "2425.png\n",
            "2426.png\n",
            "2427.png\n",
            "2428.png\n",
            "2429.png\n",
            "2430.png\n",
            "2431.png\n",
            "2432.png\n",
            "2433.png\n",
            "2434.png\n",
            "2435.png\n",
            "2436.png\n",
            "2437.png\n",
            "2438.png\n",
            "2439.png\n",
            "2440.png\n",
            "2441.png\n",
            "2442.png\n",
            "2443.png\n",
            "2444.png\n",
            "2445.png\n",
            "2446.png\n",
            "2447.png\n",
            "2448.png\n",
            "2449.png\n",
            "2450.png\n",
            "2451.png\n",
            "2452.png\n",
            "2453.png\n",
            "2454.png\n",
            "2455.png\n",
            "2456.png\n",
            "2457.png\n",
            "2458.png\n",
            "2459.png\n",
            "2460.png\n",
            "2461.png\n",
            "2462.png\n",
            "2463.png\n",
            "2464.png\n",
            "2465.png\n",
            "2466.png\n",
            "2467.png\n",
            "2468.png\n",
            "2469.png\n",
            "2470.png\n",
            "2471.png\n",
            "2472.png\n",
            "2473.png\n",
            "2474.png\n",
            "2475.png\n",
            "2476.png\n",
            "2477.png\n",
            "2478.png\n",
            "2479.png\n",
            "2480.png\n",
            "2481.png\n",
            "2482.png\n",
            "2483.png\n",
            "2484.png\n",
            "2485.png\n",
            "2486.png\n",
            "2487.png\n",
            "2488.png\n",
            "2489.png\n",
            "2490.png\n",
            "2491.png\n",
            "2492.png\n",
            "2493.png\n",
            "2494.png\n",
            "2495.png\n",
            "2496.png\n",
            "2497.png\n",
            "2498.png\n",
            "2499.png\n",
            "2500.png\n",
            "2501.png\n",
            "2502.png\n",
            "2503.png\n",
            "2504.png\n",
            "2505.png\n",
            "2506.png\n",
            "2507.png\n",
            "2508.png\n",
            "2509.png\n",
            "2510.png\n",
            "2511.png\n",
            "2512.png\n",
            "2513.png\n",
            "2514.png\n",
            "2515.png\n",
            "2516.png\n",
            "2517.png\n",
            "2518.png\n",
            "2519.png\n",
            "2520.png\n",
            "2521.png\n",
            "2522.png\n",
            "2523.png\n",
            "2524.png\n",
            "2525.png\n",
            "2526.png\n",
            "2527.png\n",
            "2528.png\n",
            "2529.png\n",
            "2530.png\n",
            "2531.png\n",
            "2532.png\n",
            "2533.png\n",
            "2534.png\n",
            "2535.png\n",
            "2536.png\n",
            "2537.png\n",
            "2538.png\n",
            "2539.png\n",
            "2540.png\n",
            "2541.png\n",
            "2542.png\n",
            "2543.png\n",
            "2544.png\n",
            "2545.png\n",
            "2546.png\n",
            "2547.png\n",
            "2548.png\n",
            "2549.png\n",
            "2550.png\n",
            "2551.png\n",
            "2552.png\n",
            "2553.png\n",
            "2554.png\n",
            "2555.png\n",
            "2556.png\n",
            "2557.png\n",
            "2558.png\n",
            "2559.png\n",
            "2560.png\n",
            "2561.png\n",
            "2562.png\n",
            "2563.png\n",
            "2564.png\n",
            "2565.png\n",
            "2566.png\n",
            "2567.png\n",
            "2568.png\n",
            "2569.png\n",
            "2570.png\n",
            "2571.png\n",
            "2572.png\n",
            "2573.png\n",
            "2574.png\n",
            "2575.png\n",
            "2576.png\n",
            "2577.png\n",
            "2578.png\n",
            "2579.png\n",
            "2580.png\n",
            "2581.png\n",
            "2582.png\n",
            "2583.png\n",
            "2584.png\n",
            "2585.png\n",
            "2586.png\n",
            "2587.png\n",
            "2588.png\n",
            "2589.png\n",
            "2590.png\n",
            "2591.png\n",
            "2592.png\n",
            "2593.png\n",
            "2594.png\n",
            "2595.png\n",
            "2596.png\n",
            "2597.png\n",
            "2598.png\n",
            "2599.png\n",
            "2600.png\n",
            "2601.png\n",
            "2602.png\n",
            "2603.png\n",
            "2604.png\n",
            "2605.png\n",
            "2606.png\n",
            "2607.png\n",
            "2608.png\n",
            "2609.png\n",
            "2610.png\n",
            "2611.png\n",
            "0.png\n",
            "1.png\n",
            "2.png\n",
            "3.png\n",
            "4.png\n",
            "5.png\n",
            "6.png\n",
            "7.png\n",
            "9.png\n",
            "8.png\n",
            "10.png\n",
            "12.png\n",
            "11.png\n",
            "13.png\n",
            "14.png\n",
            "15.png\n",
            "16.png\n",
            "17.png\n",
            "18.png\n",
            "19.png\n",
            "20.png\n",
            "21.png\n",
            "22.png\n",
            "23.png\n",
            "24.png\n",
            "25.png\n",
            "26.png\n",
            "27.png\n",
            "28.png\n",
            "29.png\n",
            "30.png\n",
            "31.png\n",
            "33.png\n",
            "32.png\n",
            "34.png\n",
            "35.png\n",
            "37.png\n",
            "36.png\n",
            "38.png\n",
            "39.png\n",
            "40.png\n",
            "41.png\n",
            "43.png\n",
            "42.png\n",
            "44.png\n",
            "45.png\n",
            "46.png\n",
            "47.png\n",
            "49.png\n",
            "48.png\n",
            "51.png\n",
            "50.png\n",
            "52.png\n",
            "54.png\n",
            "53.png\n",
            "55.png\n",
            "56.png\n",
            "57.png\n",
            "58.png\n",
            "59.png\n",
            "60.png\n",
            "61.png\n",
            "63.png\n",
            "62.png\n",
            "65.png\n",
            "64.png\n",
            "66.png\n",
            "67.png\n",
            "69.png\n",
            "68.png\n",
            "71.png\n",
            "70.png\n",
            "72.png\n",
            "73.png\n",
            "74.png\n",
            "75.png\n",
            "76.png\n",
            "77.png\n",
            "78.png\n",
            "79.png\n",
            "80.png\n",
            "81.png\n",
            "82.png\n",
            "83.png\n",
            "84.png\n",
            "85.png\n",
            "1086.png\n",
            "1087.png\n",
            "1088.png\n",
            "1089.png\n",
            "1090.png\n",
            "1091.png\n",
            "1092.png\n",
            "1093.png\n",
            "1094.png\n",
            "1095.png\n",
            "1096.png\n",
            "1097.png\n",
            "1098.png\n",
            "1099.png\n",
            "1100.png\n",
            "1101.png\n",
            "1102.png\n",
            "1103.png\n",
            "1104.png\n",
            "1105.png\n",
            "1106.png\n",
            "1107.png\n",
            "1108.png\n",
            "1109.png\n",
            "1110.png\n",
            "1111.png\n",
            "1112.png\n",
            "1113.png\n",
            "1114.png\n",
            "1115.png\n",
            "1116.png\n",
            "1117.png\n",
            "1118.png\n",
            "1119.png\n",
            "1120.png\n",
            "1121.png\n",
            "1122.png\n",
            "1123.png\n",
            "1124.png\n",
            "1125.png\n",
            "1126.png\n",
            "1127.png\n",
            "1128.png\n",
            "1129.png\n",
            "1130.png\n",
            "1131.png\n",
            "1132.png\n",
            "1133.png\n",
            "1134.png\n",
            "1135.png\n",
            "1136.png\n",
            "1137.png\n",
            "1138.png\n",
            "1139.png\n",
            "1140.png\n",
            "1141.png\n",
            "1142.png\n",
            "1143.png\n",
            "1144.png\n",
            "1145.png\n",
            "1146.png\n",
            "1147.png\n",
            "1148.png\n",
            "1149.png\n",
            "1150.png\n",
            "1151.png\n",
            "1152.png\n",
            "1153.png\n",
            "1154.png\n",
            "1155.png\n",
            "1156.png\n",
            "1157.png\n",
            "1158.png\n",
            "1159.png\n",
            "1160.png\n",
            "1161.png\n",
            "1162.png\n",
            "1163.png\n",
            "1164.png\n",
            "1165.png\n",
            "1166.png\n",
            "1167.png\n",
            "1168.png\n",
            "1169.png\n",
            "1170.png\n",
            "1171.png\n",
            "1172.png\n",
            "1173.png\n",
            "1174.png\n",
            "1175.png\n",
            "1176.png\n",
            "1177.png\n",
            "1178.png\n",
            "1179.png\n",
            "1180.png\n",
            "1181.png\n",
            "1182.png\n",
            "1183.png\n",
            "1184.png\n",
            "1185.png\n",
            "1186.png\n",
            "1187.png\n",
            "1188.png\n",
            "1189.png\n",
            "1190.png\n",
            "1191.png\n",
            "1192.png\n",
            "1193.png\n",
            "1194.png\n",
            "1195.png\n",
            "1196.png\n",
            "1197.png\n",
            "1198.png\n",
            "1199.png\n",
            "1200.png\n",
            "1201.png\n",
            "1202.png\n",
            "1203.png\n",
            "1204.png\n",
            "1205.png\n",
            "1206.png\n",
            "1207.png\n",
            "1208.png\n",
            "1209.png\n",
            "1210.png\n",
            "1211.png\n",
            "1212.png\n",
            "1213.png\n",
            "1214.png\n",
            "1215.png\n",
            "1216.png\n",
            "1217.png\n",
            "1218.png\n",
            "1219.png\n",
            "1220.png\n",
            "1221.png\n",
            "1222.png\n",
            "1223.png\n",
            "1224.png\n",
            "1225.png\n",
            "1226.png\n",
            "1227.png\n",
            "1228.png\n",
            "1229.png\n",
            "1230.png\n",
            "1231.png\n",
            "1232.png\n",
            "1233.png\n",
            "1234.png\n",
            "1235.png\n",
            "1236.png\n",
            "1237.png\n",
            "1238.png\n",
            "1239.png\n",
            "1240.png\n",
            "1241.png\n",
            "1242.png\n",
            "1243.png\n",
            "1244.png\n",
            "1245.png\n",
            "1246.png\n",
            "1247.png\n",
            "1248.png\n",
            "1249.png\n",
            "1250.png\n",
            "1251.png\n",
            "1252.png\n",
            "1253.png\n",
            "1254.png\n",
            "1255.png\n",
            "1256.png\n",
            "1257.png\n",
            "1258.png\n",
            "1259.png\n",
            "1260.png\n",
            "1261.png\n",
            "1262.png\n",
            "1263.png\n",
            "1264.png\n",
            "1265.png\n",
            "1266.png\n",
            "1267.png\n",
            "1268.png\n",
            "1269.png\n",
            "1270.png\n",
            "1271.png\n",
            "1272.png\n",
            "1273.png\n",
            "1274.png\n",
            "1275.png\n",
            "1276.png\n",
            "1277.png\n",
            "1278.png\n",
            "1279.png\n",
            "1280.png\n",
            "1281.png\n",
            "1282.png\n",
            "1283.png\n",
            "1284.png\n",
            "1285.png\n",
            "1286.png\n",
            "1287.png\n",
            "1288.png\n",
            "1289.png\n",
            "1290.png\n",
            "1291.png\n",
            "1292.png\n",
            "1293.png\n",
            "1294.png\n",
            "1295.png\n",
            "1296.png\n",
            "1297.png\n",
            "1298.png\n",
            "1299.png\n",
            "1300.png\n",
            "1301.png\n",
            "1302.png\n",
            "1303.png\n",
            "1304.png\n",
            "1305.png\n",
            "1306.png\n",
            "1307.png\n",
            "1308.png\n",
            "1309.png\n",
            "1310.png\n",
            "1311.png\n",
            "1312.png\n",
            "1313.png\n",
            "1314.png\n",
            "1315.png\n",
            "1316.png\n",
            "1317.png\n",
            "1318.png\n",
            "1319.png\n",
            "1320.png\n",
            "1321.png\n",
            "1322.png\n",
            "1323.png\n",
            "1324.png\n",
            "1325.png\n",
            "1326.png\n",
            "1327.png\n",
            "1328.png\n",
            "1329.png\n",
            "1330.png\n",
            "1331.png\n",
            "1332.png\n",
            "1333.png\n",
            "1334.png\n",
            "1335.png\n",
            "1336.png\n",
            "1337.png\n",
            "1338.png\n",
            "1339.png\n",
            "1340.png\n",
            "1341.png\n",
            "1342.png\n",
            "1343.png\n",
            "1344.png\n",
            "1345.png\n",
            "1346.png\n",
            "1347.png\n",
            "1348.png\n",
            "1349.png\n",
            "1350.png\n",
            "1351.png\n",
            "1352.png\n",
            "1353.png\n",
            "1354.png\n",
            "1355.png\n",
            "1356.png\n",
            "1357.png\n",
            "1358.png\n",
            "1359.png\n",
            "1360.png\n",
            "1361.png\n",
            "1362.png\n",
            "1363.png\n",
            "1364.png\n",
            "1365.png\n",
            "1366.png\n",
            "1367.png\n",
            "1368.png\n",
            "1369.png\n",
            "1370.png\n",
            "1371.png\n",
            "1372.png\n",
            "1373.png\n",
            "1374.png\n",
            "1375.png\n",
            "1376.png\n",
            "1377.png\n",
            "1378.png\n",
            "1379.png\n",
            "1380.png\n",
            "1381.png\n",
            "1382.png\n",
            "1383.png\n",
            "1384.png\n",
            "1385.png\n",
            "1386.png\n",
            "1387.png\n",
            "1388.png\n",
            "1389.png\n",
            "1390.png\n",
            "1391.png\n",
            "1392.png\n",
            "1393.png\n",
            "1394.png\n",
            "1395.png\n",
            "1396.png\n",
            "1397.png\n",
            "1398.png\n",
            "1399.png\n",
            "1400.png\n",
            "1401.png\n",
            "1402.png\n",
            "1403.png\n",
            "1404.png\n",
            "1405.png\n",
            "1406.png\n",
            "1407.png\n",
            "1408.png\n",
            "1409.png\n",
            "1410.png\n",
            "1411.png\n",
            "1412.png\n",
            "1413.png\n",
            "1414.png\n",
            "1415.png\n",
            "1416.png\n",
            "1417.png\n",
            "1418.png\n",
            "1419.png\n",
            "1420.png\n",
            "1421.png\n",
            "1422.png\n",
            "1423.png\n",
            "1424.png\n",
            "1425.png\n",
            "1426.png\n",
            "1427.png\n",
            "1428.png\n",
            "1429.png\n",
            "1430.png\n",
            "1431.png\n",
            "1432.png\n",
            "1433.png\n",
            "1434.png\n",
            "1435.png\n",
            "1436.png\n",
            "1437.png\n",
            "1438.png\n",
            "1439.png\n",
            "1440.png\n",
            "1441.png\n",
            "1442.png\n",
            "1443.png\n",
            "1444.png\n",
            "1445.png\n",
            "1446.png\n",
            "1447.png\n",
            "1448.png\n",
            "1449.png\n",
            "1450.png\n",
            "1451.png\n",
            "1452.png\n",
            "1453.png\n",
            "1454.png\n",
            "1455.png\n",
            "1456.png\n",
            "1457.png\n",
            "1458.png\n",
            "1459.png\n",
            "1460.png\n",
            "1461.png\n",
            "1462.png\n",
            "1463.png\n",
            "1464.png\n",
            "1465.png\n",
            "1466.png\n",
            "1467.png\n",
            "1468.png\n",
            "1469.png\n",
            "1470.png\n",
            "1471.png\n",
            "1472.png\n",
            "1473.png\n",
            "1474.png\n",
            "1475.png\n",
            "1476.png\n",
            "1477.png\n",
            "1478.png\n",
            "1479.png\n",
            "1480.png\n",
            "1481.png\n",
            "1482.png\n",
            "1483.png\n",
            "1484.png\n",
            "1485.png\n",
            "1486.png\n",
            "1487.png\n",
            "1488.png\n",
            "1489.png\n",
            "1490.png\n",
            "1491.png\n",
            "1492.png\n",
            "1493.png\n",
            "1494.png\n",
            "1495.png\n",
            "1496.png\n",
            "1497.png\n",
            "1498.png\n",
            "1499.png\n",
            "1500.png\n",
            "1501.png\n",
            "1502.png\n",
            "1503.png\n",
            "1504.png\n",
            "1505.png\n",
            "1506.png\n",
            "1507.png\n",
            "1508.png\n",
            "1509.png\n",
            "1510.png\n",
            "1511.png\n",
            "1512.png\n",
            "1513.png\n",
            "1514.png\n",
            "1515.png\n",
            "1516.png\n",
            "1517.png\n",
            "1518.png\n",
            "1519.png\n",
            "1520.png\n",
            "1521.png\n",
            "1522.png\n",
            "1523.png\n",
            "1524.png\n",
            "1525.png\n",
            "1526.png\n",
            "1527.png\n",
            "1528.png\n",
            "1529.png\n",
            "1530.png\n",
            "1531.png\n",
            "1532.png\n",
            "1533.png\n",
            "1534.png\n",
            "1535.png\n",
            "1536.png\n",
            "1537.png\n",
            "1538.png\n",
            "1539.png\n",
            "1540.png\n",
            "1541.png\n",
            "1542.png\n",
            "1543.png\n",
            "1544.png\n",
            "1545.png\n",
            "1546.png\n",
            "1547.png\n",
            "1548.png\n",
            "1549.png\n",
            "1550.png\n",
            "1551.png\n",
            "1552.png\n",
            "1553.png\n",
            "1554.png\n",
            "1555.png\n",
            "1556.png\n",
            "1557.png\n",
            "1558.png\n",
            "1559.png\n",
            "1560.png\n",
            "1561.png\n",
            "1562.png\n",
            "1563.png\n",
            "1564.png\n",
            "1565.png\n",
            "1566.png\n",
            "1567.png\n",
            "1568.png\n",
            "1569.png\n",
            "1570.png\n",
            "1571.png\n",
            "1572.png\n",
            "1573.png\n",
            "1574.png\n",
            "1575.png\n",
            "1576.png\n",
            "1577.png\n",
            "1578.png\n",
            "1579.png\n",
            "1580.png\n",
            "1581.png\n",
            "1582.png\n",
            "1583.png\n",
            "1584.png\n",
            "1585.png\n",
            "1586.png\n",
            "1587.png\n",
            "1588.png\n",
            "1589.png\n",
            "1590.png\n",
            "1591.png\n",
            "1592.png\n",
            "1593.png\n",
            "1594.png\n",
            "1595.png\n",
            "1596.png\n",
            "1597.png\n",
            "1598.png\n",
            "1599.png\n",
            "1600.png\n",
            "1601.png\n",
            "1602.png\n",
            "1603.png\n",
            "1604.png\n",
            "1605.png\n",
            "1606.png\n",
            "1607.png\n",
            "1608.png\n",
            "1609.png\n",
            "1610.png\n",
            "1611.png\n",
            "1612.png\n",
            "1613.png\n",
            "1614.png\n",
            "1615.png\n",
            "1616.png\n",
            "1617.png\n",
            "1618.png\n",
            "1619.png\n",
            "1620.png\n",
            "1621.png\n",
            "1622.png\n",
            "1623.png\n",
            "1624.png\n",
            "1625.png\n",
            "1626.png\n",
            "1627.png\n",
            "1628.png\n",
            "1629.png\n",
            "1630.png\n",
            "1631.png\n",
            "1632.png\n",
            "1633.png\n",
            "1634.png\n",
            "1635.png\n",
            "1636.png\n",
            "1637.png\n",
            "1638.png\n",
            "1639.png\n",
            "1640.png\n",
            "1641.png\n",
            "1642.png\n",
            "1643.png\n",
            "1644.png\n",
            "1645.png\n",
            "1646.png\n",
            "1647.png\n",
            "1648.png\n",
            "1649.png\n",
            "1650.png\n",
            "1651.png\n",
            "1652.png\n",
            "1653.png\n",
            "1654.png\n",
            "1655.png\n",
            "1656.png\n",
            "1657.png\n",
            "1658.png\n",
            "1659.png\n",
            "1660.png\n",
            "1661.png\n",
            "1662.png\n",
            "1663.png\n",
            "1664.png\n",
            "1665.png\n",
            "1666.png\n",
            "1667.png\n",
            "1668.png\n",
            "1669.png\n",
            "1670.png\n",
            "1671.png\n",
            "1672.png\n",
            "1673.png\n",
            "1674.png\n",
            "1675.png\n",
            "1676.png\n",
            "1677.png\n",
            "1678.png\n",
            "1679.png\n",
            "1680.png\n",
            "1681.png\n",
            "1682.png\n",
            "1683.png\n",
            "1684.png\n",
            "1685.png\n",
            "1686.png\n",
            "1687.png\n",
            "1688.png\n",
            "1689.png\n",
            "1690.png\n",
            "1691.png\n",
            "1692.png\n",
            "1693.png\n",
            "1694.png\n",
            "1695.png\n",
            "1696.png\n",
            "1697.png\n",
            "(2612, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "array = []\n",
        "path = 'resized_images/'\n",
        "\n",
        "for image in os.listdir(path):\n",
        "  print(image)\n",
        "  image = Image.open(path + image)\n",
        "  data = np.asarray(image)\n",
        "  array.append(data)\n",
        "\n",
        "X_train = np.array(array)\n",
        "X_train = normalise_image_data(X_train)\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid = np.ones((10, 1)) - 0.1\n",
        "valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Wev3iilrHr",
        "outputId": "0183f45c-452e-4ef1-ff42-30256b8b5796"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9],\n",
              "       [0.9]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPx1Y2AOHj2-",
        "outputId": "94373474-7862-4692-9664-7c1a7904528e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 50.33 [D loss: 1.495404, acc: 0.00%] [G loss: 0.328569]\n",
            "Epoch: 50.34 [D loss: 1.351813, acc: 0.00%] [G loss: 0.331076]\n",
            "Epoch: 50.35 [D loss: 1.357883, acc: 0.00%] [G loss: 0.329101]\n",
            "Epoch: 50.36 [D loss: 1.418420, acc: 0.00%] [G loss: 0.329446]\n",
            "Epoch: 50.37 [D loss: 1.367124, acc: 0.00%] [G loss: 0.329775]\n",
            "Epoch: 50.38 [D loss: 1.379764, acc: 0.00%] [G loss: 0.331238]\n",
            "Epoch: 50.39 [D loss: 1.425422, acc: 0.00%] [G loss: 0.329227]\n",
            "Epoch: 50.40 [D loss: 1.426207, acc: 0.00%] [G loss: 0.328148]\n",
            "Epoch: 50.41 [D loss: 1.422890, acc: 0.00%] [G loss: 0.329293]\n",
            "Epoch: 50.42 [D loss: 1.407373, acc: 0.00%] [G loss: 0.328505]\n",
            "Epoch: 50.43 [D loss: 1.389607, acc: 0.00%] [G loss: 0.326691]\n",
            "Epoch: 50.44 [D loss: 1.309667, acc: 0.00%] [G loss: 0.328540]\n",
            "Epoch: 50.45 [D loss: 1.350544, acc: 0.00%] [G loss: 0.327943]\n",
            "Epoch: 50.46 [D loss: 1.367547, acc: 0.00%] [G loss: 0.329746]\n",
            "Epoch: 50.47 [D loss: 1.374766, acc: 0.00%] [G loss: 0.328993]\n",
            "Epoch: 50.48 [D loss: 1.372183, acc: 0.00%] [G loss: 0.328275]\n",
            "Epoch: 50.49 [D loss: 1.368261, acc: 0.00%] [G loss: 0.329957]\n",
            "Epoch: 50.50 [D loss: 1.348578, acc: 0.00%] [G loss: 0.329541]\n",
            "Epoch: 50.51 [D loss: 1.330008, acc: 0.00%] [G loss: 0.332219]\n",
            "Epoch: 50.52 [D loss: 1.373100, acc: 0.00%] [G loss: 0.327324]\n",
            "Epoch: 50.53 [D loss: 1.324859, acc: 0.00%] [G loss: 0.328926]\n",
            "Epoch: 50.54 [D loss: 1.288812, acc: 0.00%] [G loss: 0.327372]\n",
            "Epoch: 50.55 [D loss: 1.372158, acc: 0.00%] [G loss: 0.328301]\n",
            "Epoch: 50.56 [D loss: 1.389942, acc: 0.00%] [G loss: 0.330110]\n",
            "Epoch: 50.57 [D loss: 1.334489, acc: 0.00%] [G loss: 0.328780]\n",
            "Epoch: 50.58 [D loss: 1.341863, acc: 0.00%] [G loss: 0.330181]\n",
            "Epoch: 50.59 [D loss: 1.419551, acc: 0.00%] [G loss: 0.329232]\n",
            "Epoch: 50.60 [D loss: 1.360611, acc: 0.00%] [G loss: 0.329230]\n",
            "Epoch: 50.61 [D loss: 1.398348, acc: 0.00%] [G loss: 0.327612]\n",
            "Epoch: 50.62 [D loss: 1.357538, acc: 0.00%] [G loss: 0.330534]\n",
            "Epoch: 50.63 [D loss: 1.397578, acc: 0.00%] [G loss: 0.328573]\n",
            "Epoch: 50.64 [D loss: 1.438684, acc: 0.00%] [G loss: 0.328925]\n",
            "Epoch: 50.65 [D loss: 1.382866, acc: 0.00%] [G loss: 0.328042]\n",
            "Epoch: 50.66 [D loss: 1.394570, acc: 0.00%] [G loss: 0.329088]\n",
            "Epoch: 50.67 [D loss: 1.378548, acc: 0.00%] [G loss: 0.329433]\n",
            "Epoch: 50.68 [D loss: 1.394827, acc: 0.00%] [G loss: 0.328944]\n",
            "Epoch: 50.69 [D loss: 1.363406, acc: 0.00%] [G loss: 0.327977]\n",
            "Epoch: 50.70 [D loss: 1.355350, acc: 0.00%] [G loss: 0.330471]\n",
            "Epoch: 50.71 [D loss: 1.393680, acc: 0.00%] [G loss: 0.326530]\n",
            "Epoch: 50.72 [D loss: 1.430147, acc: 0.00%] [G loss: 0.329866]\n",
            "Epoch: 50.73 [D loss: 1.363532, acc: 0.00%] [G loss: 0.327824]\n",
            "Epoch: 50.74 [D loss: 1.425497, acc: 0.00%] [G loss: 0.329823]\n",
            "Epoch: 50.75 [D loss: 1.388562, acc: 0.00%] [G loss: 0.331297]\n",
            "Epoch: 50.76 [D loss: 1.398412, acc: 0.00%] [G loss: 0.328902]\n",
            "Epoch: 50.77 [D loss: 1.399435, acc: 0.00%] [G loss: 0.329309]\n",
            "Epoch: 50.78 [D loss: 1.397855, acc: 0.00%] [G loss: 0.328988]\n",
            "Epoch: 50.79 [D loss: 1.410766, acc: 0.00%] [G loss: 0.328898]\n",
            "Epoch: 50.80 [D loss: 1.436145, acc: 0.00%] [G loss: 0.328442]\n",
            "Epoch: 50.81 [D loss: 1.392634, acc: 0.00%] [G loss: 0.328423]\n",
            "Epoch: 50.82 [D loss: 1.405129, acc: 0.00%] [G loss: 0.328774]\n",
            "Epoch: 50.83 [D loss: 1.349232, acc: 0.00%] [G loss: 0.329648]\n",
            "Epoch: 50.84 [D loss: 1.326438, acc: 0.00%] [G loss: 0.328069]\n",
            "Epoch: 50.85 [D loss: 1.365522, acc: 0.00%] [G loss: 0.329022]\n",
            "Epoch: 50.86 [D loss: 1.413669, acc: 0.00%] [G loss: 0.327970]\n",
            "Epoch: 50.87 [D loss: 1.365808, acc: 0.00%] [G loss: 0.329358]\n",
            "Epoch: 50.88 [D loss: 1.391144, acc: 0.00%] [G loss: 0.328786]\n",
            "Epoch: 50.89 [D loss: 1.381871, acc: 0.00%] [G loss: 0.331093]\n",
            "Epoch: 50.90 [D loss: 1.332933, acc: 0.00%] [G loss: 0.326843]\n",
            "Epoch: 50.91 [D loss: 1.337928, acc: 0.00%] [G loss: 0.329052]\n",
            "Epoch: 50.92 [D loss: 1.402120, acc: 0.00%] [G loss: 0.329038]\n",
            "Epoch: 50.93 [D loss: 1.377828, acc: 0.00%] [G loss: 0.326525]\n",
            "Epoch: 50.94 [D loss: 1.390777, acc: 0.00%] [G loss: 0.329222]\n",
            "Epoch: 50.95 [D loss: 1.380349, acc: 0.00%] [G loss: 0.328438]\n",
            "Epoch: 50.96 [D loss: 1.349480, acc: 0.00%] [G loss: 0.331170]\n",
            "Epoch: 50.97 [D loss: 1.410560, acc: 0.00%] [G loss: 0.327068]\n",
            "Epoch: 50.98 [D loss: 1.346998, acc: 0.00%] [G loss: 0.328879]\n",
            "Epoch: 50.99 [D loss: 1.323544, acc: 0.00%] [G loss: 0.326943]\n",
            "Epoch: 50.100 [D loss: 1.299158, acc: 0.00%] [G loss: 0.330128]\n",
            "Epoch: 50.101 [D loss: 1.336932, acc: 0.00%] [G loss: 0.326851]\n",
            "Epoch: 50.102 [D loss: 1.334392, acc: 0.00%] [G loss: 0.330264]\n",
            "Epoch: 50.103 [D loss: 1.402108, acc: 0.00%] [G loss: 0.329381]\n",
            "Epoch: 50.104 [D loss: 1.334485, acc: 0.00%] [G loss: 0.327855]\n",
            "Epoch: 50.105 [D loss: 1.373426, acc: 0.00%] [G loss: 0.328052]\n",
            "Epoch: 50.106 [D loss: 1.374503, acc: 0.00%] [G loss: 0.328605]\n",
            "Epoch: 50.107 [D loss: 1.417480, acc: 0.00%] [G loss: 0.329560]\n",
            "INFO:tensorflow:Assets written to: GAN_weights/dis_0.00000310_weights/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: GAN_weights/gen_0.00000310_weights/assets\n",
            "Epoch: 51.0 [D loss: 1.322334, acc: 0.00%] [G loss: 0.327863]\n",
            "Epoch: 51.1 [D loss: 1.369952, acc: 0.00%] [G loss: 0.329194]\n",
            "Epoch: 51.2 [D loss: 1.321135, acc: 0.00%] [G loss: 0.330823]\n",
            "Epoch: 51.3 [D loss: 1.374059, acc: 0.00%] [G loss: 0.328694]\n",
            "Epoch: 51.4 [D loss: 1.295256, acc: 0.00%] [G loss: 0.327646]\n",
            "Epoch: 51.5 [D loss: 1.288858, acc: 0.00%] [G loss: 0.326741]\n",
            "Epoch: 51.6 [D loss: 1.319994, acc: 0.00%] [G loss: 0.328269]\n",
            "Epoch: 51.7 [D loss: 1.377289, acc: 0.00%] [G loss: 0.327947]\n",
            "Epoch: 51.8 [D loss: 1.417342, acc: 0.00%] [G loss: 0.329379]\n",
            "Epoch: 51.9 [D loss: 1.401834, acc: 0.00%] [G loss: 0.328968]\n",
            "Epoch: 51.10 [D loss: 1.440580, acc: 0.00%] [G loss: 0.327436]\n",
            "Epoch: 51.11 [D loss: 1.462575, acc: 0.00%] [G loss: 0.327590]\n",
            "Epoch: 51.12 [D loss: 1.399415, acc: 0.00%] [G loss: 0.329188]\n",
            "Epoch: 51.13 [D loss: 1.400323, acc: 0.00%] [G loss: 0.329153]\n",
            "Epoch: 51.14 [D loss: 1.356415, acc: 0.00%] [G loss: 0.329223]\n",
            "Epoch: 51.15 [D loss: 1.344443, acc: 0.00%] [G loss: 0.328243]\n",
            "Epoch: 51.16 [D loss: 1.391101, acc: 0.00%] [G loss: 0.331053]\n",
            "Epoch: 51.17 [D loss: 1.248033, acc: 0.00%] [G loss: 0.328708]\n",
            "Epoch: 51.18 [D loss: 1.398534, acc: 0.00%] [G loss: 0.327343]\n",
            "Epoch: 51.19 [D loss: 1.341870, acc: 0.00%] [G loss: 0.327778]\n",
            "Epoch: 51.20 [D loss: 1.382595, acc: 0.00%] [G loss: 0.329524]\n",
            "Epoch: 51.21 [D loss: 1.397096, acc: 0.00%] [G loss: 0.328458]\n",
            "Epoch: 51.22 [D loss: 1.342230, acc: 0.00%] [G loss: 0.328638]\n",
            "Epoch: 51.23 [D loss: 1.372096, acc: 0.00%] [G loss: 0.326901]\n",
            "Epoch: 51.24 [D loss: 1.353162, acc: 0.00%] [G loss: 0.327484]\n",
            "Epoch: 51.25 [D loss: 1.338648, acc: 0.00%] [G loss: 0.328851]\n",
            "Epoch: 51.26 [D loss: 1.356622, acc: 0.00%] [G loss: 0.329980]\n",
            "Epoch: 51.27 [D loss: 1.383349, acc: 0.00%] [G loss: 0.330524]\n",
            "Epoch: 51.28 [D loss: 1.338504, acc: 0.00%] [G loss: 0.332453]\n",
            "Epoch: 51.29 [D loss: 1.378736, acc: 0.00%] [G loss: 0.328691]\n",
            "Epoch: 51.30 [D loss: 1.337657, acc: 0.00%] [G loss: 0.328454]\n",
            "Epoch: 51.31 [D loss: 1.422726, acc: 0.00%] [G loss: 0.328763]\n",
            "Epoch: 51.32 [D loss: 1.370914, acc: 0.00%] [G loss: 0.330585]\n",
            "Epoch: 51.33 [D loss: 1.374043, acc: 0.00%] [G loss: 0.328609]\n",
            "Epoch: 51.34 [D loss: 1.399111, acc: 0.00%] [G loss: 0.329667]\n",
            "Epoch: 51.35 [D loss: 1.352569, acc: 0.00%] [G loss: 0.330040]\n",
            "Epoch: 51.36 [D loss: 1.447261, acc: 0.00%] [G loss: 0.330674]\n",
            "Epoch: 51.37 [D loss: 1.433077, acc: 0.00%] [G loss: 0.329087]\n",
            "Epoch: 51.38 [D loss: 1.427416, acc: 0.00%] [G loss: 0.329347]\n",
            "Epoch: 51.39 [D loss: 1.380808, acc: 0.00%] [G loss: 0.328013]\n",
            "Epoch: 51.40 [D loss: 1.329077, acc: 0.00%] [G loss: 0.328794]\n",
            "Epoch: 51.41 [D loss: 1.341013, acc: 0.00%] [G loss: 0.328803]\n",
            "Epoch: 51.42 [D loss: 1.331238, acc: 0.00%] [G loss: 0.329755]\n",
            "Epoch: 51.43 [D loss: 1.363509, acc: 0.00%] [G loss: 0.330761]\n",
            "Epoch: 51.44 [D loss: 1.362221, acc: 0.00%] [G loss: 0.328105]\n",
            "Epoch: 51.45 [D loss: 1.393299, acc: 0.00%] [G loss: 0.329542]\n",
            "Epoch: 51.46 [D loss: 1.414215, acc: 0.00%] [G loss: 0.330839]\n",
            "Epoch: 51.47 [D loss: 1.466581, acc: 0.00%] [G loss: 0.329842]\n",
            "Epoch: 51.48 [D loss: 1.402429, acc: 0.00%] [G loss: 0.327919]\n",
            "Epoch: 51.49 [D loss: 1.384182, acc: 0.00%] [G loss: 0.329799]\n",
            "Epoch: 51.50 [D loss: 1.362001, acc: 0.00%] [G loss: 0.328932]\n",
            "Epoch: 51.51 [D loss: 1.399123, acc: 0.00%] [G loss: 0.332352]\n",
            "Epoch: 51.52 [D loss: 1.323855, acc: 0.00%] [G loss: 0.329186]\n",
            "Epoch: 51.53 [D loss: 1.295733, acc: 0.00%] [G loss: 0.329025]\n",
            "Epoch: 51.54 [D loss: 1.351949, acc: 0.00%] [G loss: 0.329748]\n",
            "Epoch: 51.55 [D loss: 1.379623, acc: 0.00%] [G loss: 0.328935]\n",
            "Epoch: 51.56 [D loss: 1.387814, acc: 0.00%] [G loss: 0.330656]\n",
            "Epoch: 51.57 [D loss: 1.368089, acc: 0.00%] [G loss: 0.329777]\n",
            "Epoch: 51.58 [D loss: 1.371456, acc: 0.00%] [G loss: 0.329271]\n",
            "Epoch: 51.59 [D loss: 1.355598, acc: 0.00%] [G loss: 0.329123]\n",
            "Epoch: 51.60 [D loss: 1.317312, acc: 0.00%] [G loss: 0.328032]\n",
            "Epoch: 51.61 [D loss: 1.361954, acc: 0.00%] [G loss: 0.328609]\n",
            "Epoch: 51.62 [D loss: 1.370472, acc: 0.00%] [G loss: 0.328528]\n",
            "Epoch: 51.63 [D loss: 1.345922, acc: 0.00%] [G loss: 0.329373]\n",
            "Epoch: 51.64 [D loss: 1.343498, acc: 0.00%] [G loss: 0.329281]\n",
            "Epoch: 51.65 [D loss: 1.331016, acc: 0.00%] [G loss: 0.329910]\n",
            "Epoch: 51.66 [D loss: 1.399683, acc: 0.00%] [G loss: 0.331312]\n",
            "Epoch: 51.67 [D loss: 1.383788, acc: 0.00%] [G loss: 0.329972]\n",
            "Epoch: 51.68 [D loss: 1.451149, acc: 0.00%] [G loss: 0.328049]\n",
            "Epoch: 51.69 [D loss: 1.445702, acc: 0.00%] [G loss: 0.331025]\n",
            "Epoch: 51.70 [D loss: 1.375365, acc: 0.00%] [G loss: 0.331071]\n",
            "Epoch: 51.71 [D loss: 1.348707, acc: 0.00%] [G loss: 0.329484]\n",
            "Epoch: 51.72 [D loss: 1.376379, acc: 0.00%] [G loss: 0.329694]\n",
            "Epoch: 51.73 [D loss: 1.384964, acc: 0.00%] [G loss: 0.329125]\n",
            "Epoch: 51.74 [D loss: 1.312134, acc: 0.00%] [G loss: 0.329351]\n",
            "Epoch: 51.75 [D loss: 1.311694, acc: 0.00%] [G loss: 0.329528]\n",
            "Epoch: 51.76 [D loss: 1.316462, acc: 0.00%] [G loss: 0.328680]\n",
            "Epoch: 51.77 [D loss: 1.302395, acc: 0.00%] [G loss: 0.331119]\n",
            "Epoch: 51.78 [D loss: 1.294326, acc: 0.00%] [G loss: 0.330939]\n",
            "Epoch: 51.79 [D loss: 1.442172, acc: 0.00%] [G loss: 0.329445]\n",
            "Epoch: 51.80 [D loss: 1.418937, acc: 0.00%] [G loss: 0.330001]\n",
            "Epoch: 51.81 [D loss: 1.399486, acc: 0.00%] [G loss: 0.327591]\n",
            "Epoch: 51.82 [D loss: 1.477450, acc: 0.00%] [G loss: 0.329843]\n",
            "Epoch: 51.83 [D loss: 1.427286, acc: 0.00%] [G loss: 0.328985]\n",
            "Epoch: 51.84 [D loss: 1.403015, acc: 0.00%] [G loss: 0.329888]\n",
            "Epoch: 51.85 [D loss: 1.420560, acc: 0.00%] [G loss: 0.330055]\n",
            "Epoch: 51.86 [D loss: 1.330272, acc: 0.00%] [G loss: 0.329975]\n",
            "Epoch: 51.87 [D loss: 1.341499, acc: 0.00%] [G loss: 0.328271]\n",
            "Epoch: 51.88 [D loss: 1.366361, acc: 0.00%] [G loss: 0.328576]\n",
            "Epoch: 51.89 [D loss: 1.362863, acc: 0.00%] [G loss: 0.328760]\n",
            "Epoch: 51.90 [D loss: 1.327332, acc: 0.00%] [G loss: 0.329780]\n",
            "Epoch: 51.91 [D loss: 1.368915, acc: 0.00%] [G loss: 0.327155]\n",
            "Epoch: 51.92 [D loss: 1.398027, acc: 0.00%] [G loss: 0.328496]\n",
            "Epoch: 51.93 [D loss: 1.417717, acc: 0.00%] [G loss: 0.330106]\n",
            "Epoch: 51.94 [D loss: 1.411257, acc: 0.00%] [G loss: 0.327095]\n",
            "Epoch: 51.95 [D loss: 1.365072, acc: 0.00%] [G loss: 0.330011]\n",
            "Epoch: 51.96 [D loss: 1.405716, acc: 0.00%] [G loss: 0.329335]\n",
            "Epoch: 51.97 [D loss: 1.438749, acc: 0.00%] [G loss: 0.328305]\n",
            "Epoch: 51.98 [D loss: 1.510236, acc: 0.00%] [G loss: 0.329099]\n",
            "Epoch: 51.99 [D loss: 1.403834, acc: 0.00%] [G loss: 0.328399]\n",
            "Epoch: 51.100 [D loss: 1.358131, acc: 0.00%] [G loss: 0.330670]\n",
            "Epoch: 51.101 [D loss: 1.361209, acc: 0.00%] [G loss: 0.329645]\n",
            "Epoch: 51.102 [D loss: 1.370316, acc: 0.00%] [G loss: 0.331131]\n",
            "Epoch: 51.103 [D loss: 1.373250, acc: 0.00%] [G loss: 0.328536]\n",
            "Epoch: 51.104 [D loss: 1.338645, acc: 0.00%] [G loss: 0.328255]\n",
            "Epoch: 51.105 [D loss: 1.362938, acc: 0.00%] [G loss: 0.328697]\n",
            "Epoch: 51.106 [D loss: 1.342769, acc: 0.00%] [G loss: 0.327397]\n",
            "Epoch: 51.107 [D loss: 1.355245, acc: 0.00%] [G loss: 0.328646]\n",
            "Epoch: 52.0 [D loss: 1.340285, acc: 0.00%] [G loss: 0.329587]\n",
            "Epoch: 52.1 [D loss: 1.332176, acc: 0.00%] [G loss: 0.330786]\n",
            "Epoch: 52.2 [D loss: 1.328793, acc: 0.00%] [G loss: 0.331892]\n",
            "Epoch: 52.3 [D loss: 1.451372, acc: 0.00%] [G loss: 0.328421]\n",
            "Epoch: 52.4 [D loss: 1.490202, acc: 0.00%] [G loss: 0.331536]\n",
            "Epoch: 52.5 [D loss: 1.419863, acc: 0.00%] [G loss: 0.329504]\n",
            "Epoch: 52.6 [D loss: 1.414246, acc: 0.00%] [G loss: 0.328820]\n",
            "Epoch: 52.7 [D loss: 1.389772, acc: 0.00%] [G loss: 0.329401]\n",
            "Epoch: 52.8 [D loss: 1.328148, acc: 0.00%] [G loss: 0.327979]\n",
            "Epoch: 52.9 [D loss: 1.353376, acc: 0.00%] [G loss: 0.329434]\n",
            "Epoch: 52.10 [D loss: 1.377372, acc: 0.00%] [G loss: 0.327401]\n",
            "Epoch: 52.11 [D loss: 1.310629, acc: 0.00%] [G loss: 0.328302]\n",
            "Epoch: 52.12 [D loss: 1.319724, acc: 0.00%] [G loss: 0.329209]\n",
            "Epoch: 52.13 [D loss: 1.349013, acc: 0.00%] [G loss: 0.327972]\n",
            "Epoch: 52.14 [D loss: 1.426749, acc: 0.00%] [G loss: 0.327219]\n",
            "Epoch: 52.15 [D loss: 1.374092, acc: 0.00%] [G loss: 0.328879]\n",
            "Epoch: 52.16 [D loss: 1.473466, acc: 0.00%] [G loss: 0.328605]\n",
            "Epoch: 52.17 [D loss: 1.404318, acc: 0.00%] [G loss: 0.332082]\n",
            "Epoch: 52.18 [D loss: 1.382761, acc: 0.00%] [G loss: 0.330444]\n",
            "Epoch: 52.19 [D loss: 1.403067, acc: 0.00%] [G loss: 0.330120]\n",
            "Epoch: 52.20 [D loss: 1.321118, acc: 0.00%] [G loss: 0.328751]\n",
            "Epoch: 52.21 [D loss: 1.351311, acc: 0.00%] [G loss: 0.329474]\n",
            "Epoch: 52.22 [D loss: 1.316274, acc: 0.00%] [G loss: 0.329007]\n",
            "Epoch: 52.23 [D loss: 1.327407, acc: 0.00%] [G loss: 0.329211]\n",
            "Epoch: 52.24 [D loss: 1.336132, acc: 0.00%] [G loss: 0.330030]\n",
            "Epoch: 52.25 [D loss: 1.352172, acc: 0.00%] [G loss: 0.330611]\n",
            "Epoch: 52.26 [D loss: 1.402580, acc: 0.00%] [G loss: 0.327197]\n",
            "Epoch: 52.27 [D loss: 1.392231, acc: 0.00%] [G loss: 0.330754]\n",
            "Epoch: 52.28 [D loss: 1.374155, acc: 0.00%] [G loss: 0.330184]\n",
            "Epoch: 52.29 [D loss: 1.460487, acc: 0.00%] [G loss: 0.329777]\n",
            "Epoch: 52.30 [D loss: 1.434319, acc: 0.00%] [G loss: 0.329495]\n",
            "Epoch: 52.31 [D loss: 1.448501, acc: 0.00%] [G loss: 0.328840]\n",
            "Epoch: 52.32 [D loss: 1.373574, acc: 0.00%] [G loss: 0.327817]\n",
            "Epoch: 52.33 [D loss: 1.401027, acc: 0.00%] [G loss: 0.327778]\n",
            "Epoch: 52.34 [D loss: 1.320176, acc: 0.00%] [G loss: 0.329893]\n",
            "Epoch: 52.35 [D loss: 1.292969, acc: 0.00%] [G loss: 0.329810]\n",
            "Epoch: 52.36 [D loss: 1.338257, acc: 0.00%] [G loss: 0.328876]\n",
            "Epoch: 52.37 [D loss: 1.305596, acc: 0.00%] [G loss: 0.331823]\n",
            "Epoch: 52.38 [D loss: 1.317075, acc: 0.00%] [G loss: 0.328801]\n",
            "Epoch: 52.39 [D loss: 1.326742, acc: 0.00%] [G loss: 0.329699]\n",
            "Epoch: 52.40 [D loss: 1.357395, acc: 0.00%] [G loss: 0.330393]\n",
            "Epoch: 52.41 [D loss: 1.412184, acc: 0.00%] [G loss: 0.331056]\n",
            "Epoch: 52.42 [D loss: 1.403406, acc: 0.00%] [G loss: 0.328457]\n",
            "Epoch: 52.43 [D loss: 1.439575, acc: 0.00%] [G loss: 0.328290]\n",
            "Epoch: 52.44 [D loss: 1.427529, acc: 0.00%] [G loss: 0.328777]\n",
            "Epoch: 52.45 [D loss: 1.452505, acc: 0.00%] [G loss: 0.332751]\n",
            "Epoch: 52.46 [D loss: 1.338386, acc: 0.00%] [G loss: 0.328726]\n",
            "Epoch: 52.47 [D loss: 1.414733, acc: 0.00%] [G loss: 0.329829]\n",
            "Epoch: 52.48 [D loss: 1.411554, acc: 0.00%] [G loss: 0.327842]\n",
            "Epoch: 52.49 [D loss: 1.343744, acc: 0.00%] [G loss: 0.331760]\n",
            "Epoch: 52.50 [D loss: 1.329371, acc: 0.00%] [G loss: 0.327916]\n",
            "Epoch: 52.51 [D loss: 1.350255, acc: 0.00%] [G loss: 0.329749]\n",
            "Epoch: 52.52 [D loss: 1.358911, acc: 0.00%] [G loss: 0.328412]\n",
            "Epoch: 52.53 [D loss: 1.354016, acc: 0.00%] [G loss: 0.328243]\n",
            "Epoch: 52.54 [D loss: 1.371019, acc: 0.00%] [G loss: 0.330032]\n",
            "Epoch: 52.55 [D loss: 1.344330, acc: 0.00%] [G loss: 0.331482]\n",
            "Epoch: 52.56 [D loss: 1.372560, acc: 0.00%] [G loss: 0.328628]\n",
            "Epoch: 52.57 [D loss: 1.400392, acc: 0.00%] [G loss: 0.329526]\n",
            "Epoch: 52.58 [D loss: 1.357588, acc: 0.00%] [G loss: 0.330060]\n",
            "Epoch: 52.59 [D loss: 1.349681, acc: 0.00%] [G loss: 0.329927]\n",
            "Epoch: 52.60 [D loss: 1.349147, acc: 0.00%] [G loss: 0.328014]\n",
            "Epoch: 52.61 [D loss: 1.316866, acc: 0.00%] [G loss: 0.327865]\n",
            "Epoch: 52.62 [D loss: 1.345222, acc: 0.00%] [G loss: 0.330796]\n",
            "Epoch: 52.63 [D loss: 1.332969, acc: 0.00%] [G loss: 0.331263]\n",
            "Epoch: 52.64 [D loss: 1.358050, acc: 0.00%] [G loss: 0.330013]\n",
            "Epoch: 52.65 [D loss: 1.370247, acc: 0.00%] [G loss: 0.328398]\n",
            "Epoch: 52.66 [D loss: 1.376096, acc: 0.00%] [G loss: 0.326919]\n",
            "Epoch: 52.67 [D loss: 1.409550, acc: 0.00%] [G loss: 0.329007]\n",
            "Epoch: 52.68 [D loss: 1.502559, acc: 0.00%] [G loss: 0.329797]\n",
            "Epoch: 52.69 [D loss: 1.417845, acc: 0.00%] [G loss: 0.327928]\n",
            "Epoch: 52.70 [D loss: 1.437720, acc: 0.00%] [G loss: 0.328228]\n",
            "Epoch: 52.71 [D loss: 1.412740, acc: 0.00%] [G loss: 0.327358]\n",
            "Epoch: 52.72 [D loss: 1.346736, acc: 0.00%] [G loss: 0.329776]\n",
            "Epoch: 52.73 [D loss: 1.380482, acc: 0.00%] [G loss: 0.330335]\n",
            "Epoch: 52.74 [D loss: 1.396734, acc: 0.00%] [G loss: 0.327658]\n",
            "Epoch: 52.75 [D loss: 1.342560, acc: 0.00%] [G loss: 0.329232]\n",
            "Epoch: 52.76 [D loss: 1.367519, acc: 0.00%] [G loss: 0.327847]\n",
            "Epoch: 52.77 [D loss: 1.337651, acc: 0.00%] [G loss: 0.327903]\n",
            "Epoch: 52.78 [D loss: 1.379690, acc: 0.00%] [G loss: 0.327990]\n",
            "Epoch: 52.79 [D loss: 1.362803, acc: 0.00%] [G loss: 0.329367]\n",
            "Epoch: 52.80 [D loss: 1.422143, acc: 0.00%] [G loss: 0.329538]\n",
            "Epoch: 52.81 [D loss: 1.406561, acc: 0.00%] [G loss: 0.328339]\n",
            "Epoch: 52.82 [D loss: 1.288877, acc: 0.00%] [G loss: 0.327149]\n",
            "Epoch: 52.83 [D loss: 1.341896, acc: 0.00%] [G loss: 0.329412]\n",
            "Epoch: 52.84 [D loss: 1.322437, acc: 0.00%] [G loss: 0.331199]\n",
            "Epoch: 52.85 [D loss: 1.312421, acc: 0.00%] [G loss: 0.329865]\n",
            "Epoch: 52.86 [D loss: 1.355949, acc: 0.00%] [G loss: 0.329244]\n",
            "Epoch: 52.87 [D loss: 1.410644, acc: 0.00%] [G loss: 0.327922]\n",
            "Epoch: 52.88 [D loss: 1.373709, acc: 0.00%] [G loss: 0.330685]\n",
            "Epoch: 52.89 [D loss: 1.386854, acc: 0.00%] [G loss: 0.328970]\n",
            "Epoch: 52.90 [D loss: 1.468262, acc: 0.00%] [G loss: 0.330341]\n",
            "Epoch: 52.91 [D loss: 1.458435, acc: 0.00%] [G loss: 0.328328]\n",
            "Epoch: 52.92 [D loss: 1.386565, acc: 0.00%] [G loss: 0.328947]\n",
            "Epoch: 52.93 [D loss: 1.387508, acc: 0.00%] [G loss: 0.328202]\n",
            "Epoch: 52.94 [D loss: 1.338986, acc: 0.00%] [G loss: 0.327370]\n",
            "Epoch: 52.95 [D loss: 1.354198, acc: 0.00%] [G loss: 0.330235]\n",
            "Epoch: 52.96 [D loss: 1.343359, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 52.97 [D loss: 1.329288, acc: 0.00%] [G loss: 0.328540]\n",
            "Epoch: 52.98 [D loss: 1.342500, acc: 0.00%] [G loss: 0.327761]\n",
            "Epoch: 52.99 [D loss: 1.290372, acc: 0.00%] [G loss: 0.328273]\n",
            "Epoch: 52.100 [D loss: 1.378116, acc: 0.00%] [G loss: 0.326812]\n",
            "Epoch: 52.101 [D loss: 1.305021, acc: 0.00%] [G loss: 0.328324]\n",
            "Epoch: 52.102 [D loss: 1.370140, acc: 0.00%] [G loss: 0.327711]\n",
            "Epoch: 52.103 [D loss: 1.358352, acc: 0.00%] [G loss: 0.330639]\n",
            "Epoch: 52.104 [D loss: 1.397005, acc: 0.00%] [G loss: 0.328091]\n",
            "Epoch: 52.105 [D loss: 1.434181, acc: 0.00%] [G loss: 0.328344]\n",
            "Epoch: 52.106 [D loss: 1.357497, acc: 0.00%] [G loss: 0.328935]\n",
            "Epoch: 52.107 [D loss: 1.352755, acc: 0.00%] [G loss: 0.328747]\n",
            "Epoch: 53.0 [D loss: 1.355590, acc: 0.00%] [G loss: 0.329752]\n",
            "Epoch: 53.1 [D loss: 1.355945, acc: 0.00%] [G loss: 0.331461]\n",
            "Epoch: 53.2 [D loss: 1.362964, acc: 0.00%] [G loss: 0.327721]\n",
            "Epoch: 53.3 [D loss: 1.341308, acc: 0.00%] [G loss: 0.329710]\n",
            "Epoch: 53.4 [D loss: 1.356526, acc: 0.00%] [G loss: 0.327970]\n",
            "Epoch: 53.5 [D loss: 1.391551, acc: 0.00%] [G loss: 0.328501]\n",
            "Epoch: 53.6 [D loss: 1.452811, acc: 0.00%] [G loss: 0.328295]\n",
            "Epoch: 53.7 [D loss: 1.372534, acc: 0.00%] [G loss: 0.330366]\n",
            "Epoch: 53.8 [D loss: 1.351741, acc: 0.00%] [G loss: 0.326299]\n",
            "Epoch: 53.9 [D loss: 1.338573, acc: 0.00%] [G loss: 0.328612]\n",
            "Epoch: 53.10 [D loss: 1.336780, acc: 0.00%] [G loss: 0.329049]\n",
            "Epoch: 53.11 [D loss: 1.379882, acc: 0.00%] [G loss: 0.328072]\n",
            "Epoch: 53.12 [D loss: 1.359522, acc: 0.00%] [G loss: 0.326826]\n",
            "Epoch: 53.13 [D loss: 1.379365, acc: 0.00%] [G loss: 0.328895]\n",
            "Epoch: 53.14 [D loss: 1.399686, acc: 0.00%] [G loss: 0.328026]\n",
            "Epoch: 53.15 [D loss: 1.373433, acc: 0.00%] [G loss: 0.327724]\n",
            "Epoch: 53.16 [D loss: 1.397092, acc: 0.00%] [G loss: 0.328906]\n",
            "Epoch: 53.17 [D loss: 1.351635, acc: 0.00%] [G loss: 0.331148]\n",
            "Epoch: 53.18 [D loss: 1.382498, acc: 0.00%] [G loss: 0.329453]\n",
            "Epoch: 53.19 [D loss: 1.388934, acc: 0.00%] [G loss: 0.327585]\n",
            "Epoch: 53.20 [D loss: 1.352381, acc: 0.00%] [G loss: 0.327936]\n",
            "Epoch: 53.21 [D loss: 1.385031, acc: 0.00%] [G loss: 0.329121]\n",
            "Epoch: 53.22 [D loss: 1.332902, acc: 0.00%] [G loss: 0.329536]\n",
            "Epoch: 53.23 [D loss: 1.380121, acc: 0.00%] [G loss: 0.329056]\n",
            "Epoch: 53.24 [D loss: 1.368172, acc: 0.00%] [G loss: 0.330468]\n",
            "Epoch: 53.25 [D loss: 1.331642, acc: 0.00%] [G loss: 0.327620]\n",
            "Epoch: 53.26 [D loss: 1.404747, acc: 0.00%] [G loss: 0.328809]\n",
            "Epoch: 53.27 [D loss: 1.343866, acc: 0.00%] [G loss: 0.326714]\n",
            "Epoch: 53.28 [D loss: 1.380429, acc: 0.00%] [G loss: 0.332794]\n",
            "Epoch: 53.29 [D loss: 1.353917, acc: 0.00%] [G loss: 0.329021]\n",
            "Epoch: 53.30 [D loss: 1.336678, acc: 0.00%] [G loss: 0.329513]\n",
            "Epoch: 53.31 [D loss: 1.291560, acc: 0.00%] [G loss: 0.328144]\n",
            "Epoch: 53.32 [D loss: 1.384622, acc: 0.00%] [G loss: 0.329706]\n",
            "Epoch: 53.33 [D loss: 1.363653, acc: 0.00%] [G loss: 0.329405]\n",
            "Epoch: 53.34 [D loss: 1.372775, acc: 0.00%] [G loss: 0.327883]\n",
            "Epoch: 53.35 [D loss: 1.432388, acc: 0.00%] [G loss: 0.328167]\n",
            "Epoch: 53.36 [D loss: 1.408748, acc: 0.00%] [G loss: 0.328062]\n",
            "Epoch: 53.37 [D loss: 1.478871, acc: 0.00%] [G loss: 0.328004]\n",
            "Epoch: 53.38 [D loss: 1.404541, acc: 0.00%] [G loss: 0.330179]\n",
            "Epoch: 53.39 [D loss: 1.426673, acc: 0.00%] [G loss: 0.328157]\n",
            "Epoch: 53.40 [D loss: 1.331707, acc: 0.00%] [G loss: 0.327931]\n",
            "Epoch: 53.41 [D loss: 1.431400, acc: 0.00%] [G loss: 0.328497]\n",
            "Epoch: 53.42 [D loss: 1.290897, acc: 0.00%] [G loss: 0.329972]\n",
            "Epoch: 53.43 [D loss: 1.360235, acc: 0.00%] [G loss: 0.330070]\n",
            "Epoch: 53.44 [D loss: 1.344077, acc: 0.00%] [G loss: 0.328863]\n",
            "Epoch: 53.45 [D loss: 1.309912, acc: 0.00%] [G loss: 0.327875]\n",
            "Epoch: 53.46 [D loss: 1.390027, acc: 0.00%] [G loss: 0.328872]\n",
            "Epoch: 53.47 [D loss: 1.393044, acc: 0.00%] [G loss: 0.327622]\n",
            "Epoch: 53.48 [D loss: 1.363417, acc: 0.00%] [G loss: 0.328603]\n",
            "Epoch: 53.49 [D loss: 1.386380, acc: 0.00%] [G loss: 0.328242]\n",
            "Epoch: 53.50 [D loss: 1.391346, acc: 0.00%] [G loss: 0.327349]\n",
            "Epoch: 53.51 [D loss: 1.453588, acc: 0.00%] [G loss: 0.330124]\n",
            "Epoch: 53.52 [D loss: 1.446004, acc: 0.00%] [G loss: 0.327496]\n",
            "Epoch: 53.53 [D loss: 1.315545, acc: 0.00%] [G loss: 0.327887]\n",
            "Epoch: 53.54 [D loss: 1.385070, acc: 0.00%] [G loss: 0.329165]\n",
            "Epoch: 53.55 [D loss: 1.335484, acc: 0.00%] [G loss: 0.330226]\n",
            "Epoch: 53.56 [D loss: 1.390782, acc: 0.00%] [G loss: 0.328926]\n",
            "Epoch: 53.57 [D loss: 1.296032, acc: 0.00%] [G loss: 0.329774]\n",
            "Epoch: 53.58 [D loss: 1.319429, acc: 0.00%] [G loss: 0.329624]\n",
            "Epoch: 53.59 [D loss: 1.294835, acc: 0.00%] [G loss: 0.332478]\n",
            "Epoch: 53.60 [D loss: 1.356316, acc: 0.00%] [G loss: 0.330539]\n",
            "Epoch: 53.61 [D loss: 1.401735, acc: 0.00%] [G loss: 0.328534]\n",
            "Epoch: 53.62 [D loss: 1.442944, acc: 0.00%] [G loss: 0.328472]\n",
            "Epoch: 53.63 [D loss: 1.380456, acc: 0.00%] [G loss: 0.329780]\n",
            "Epoch: 53.64 [D loss: 1.388222, acc: 0.00%] [G loss: 0.328494]\n",
            "Epoch: 53.65 [D loss: 1.410940, acc: 0.00%] [G loss: 0.329697]\n",
            "Epoch: 53.66 [D loss: 1.390146, acc: 0.00%] [G loss: 0.328537]\n",
            "Epoch: 53.67 [D loss: 1.376567, acc: 0.00%] [G loss: 0.330040]\n",
            "Epoch: 53.68 [D loss: 1.336536, acc: 0.00%] [G loss: 0.327375]\n",
            "Epoch: 53.69 [D loss: 1.314135, acc: 0.00%] [G loss: 0.327117]\n",
            "Epoch: 53.70 [D loss: 1.296212, acc: 0.00%] [G loss: 0.328269]\n",
            "Epoch: 53.71 [D loss: 1.295381, acc: 0.00%] [G loss: 0.329570]\n",
            "Epoch: 53.72 [D loss: 1.336985, acc: 0.00%] [G loss: 0.328454]\n",
            "Epoch: 53.73 [D loss: 1.377456, acc: 0.00%] [G loss: 0.329925]\n",
            "Epoch: 53.74 [D loss: 1.346353, acc: 0.00%] [G loss: 0.330619]\n",
            "Epoch: 53.75 [D loss: 1.400339, acc: 0.00%] [G loss: 0.328645]\n",
            "Epoch: 53.76 [D loss: 1.443915, acc: 0.00%] [G loss: 0.328628]\n",
            "Epoch: 53.77 [D loss: 1.444376, acc: 0.00%] [G loss: 0.330581]\n",
            "Epoch: 53.78 [D loss: 1.415791, acc: 0.00%] [G loss: 0.328398]\n",
            "Epoch: 53.79 [D loss: 1.435958, acc: 0.00%] [G loss: 0.328325]\n",
            "Epoch: 53.80 [D loss: 1.399984, acc: 0.00%] [G loss: 0.328134]\n",
            "Epoch: 53.81 [D loss: 1.328343, acc: 0.00%] [G loss: 0.327101]\n",
            "Epoch: 53.82 [D loss: 1.376140, acc: 0.00%] [G loss: 0.329844]\n",
            "Epoch: 53.83 [D loss: 1.321625, acc: 0.00%] [G loss: 0.329313]\n",
            "Epoch: 53.84 [D loss: 1.269709, acc: 0.00%] [G loss: 0.331578]\n",
            "Epoch: 53.85 [D loss: 1.360336, acc: 0.00%] [G loss: 0.328420]\n",
            "Epoch: 53.86 [D loss: 1.346923, acc: 0.00%] [G loss: 0.327728]\n",
            "Epoch: 53.87 [D loss: 1.339732, acc: 0.00%] [G loss: 0.329261]\n",
            "Epoch: 53.88 [D loss: 1.453122, acc: 0.00%] [G loss: 0.329154]\n",
            "Epoch: 53.89 [D loss: 1.400364, acc: 0.00%] [G loss: 0.330040]\n",
            "Epoch: 53.90 [D loss: 1.425979, acc: 0.00%] [G loss: 0.329386]\n",
            "Epoch: 53.91 [D loss: 1.430623, acc: 0.00%] [G loss: 0.328360]\n",
            "Epoch: 53.92 [D loss: 1.401820, acc: 0.00%] [G loss: 0.331686]\n",
            "Epoch: 53.93 [D loss: 1.397125, acc: 0.00%] [G loss: 0.332391]\n",
            "Epoch: 53.94 [D loss: 1.363397, acc: 0.00%] [G loss: 0.330756]\n",
            "Epoch: 53.95 [D loss: 1.321634, acc: 0.00%] [G loss: 0.328437]\n",
            "Epoch: 53.96 [D loss: 1.313957, acc: 0.00%] [G loss: 0.326947]\n",
            "Epoch: 53.97 [D loss: 1.315863, acc: 0.00%] [G loss: 0.331391]\n",
            "Epoch: 53.98 [D loss: 1.289814, acc: 0.00%] [G loss: 0.329306]\n",
            "Epoch: 53.99 [D loss: 1.368742, acc: 0.00%] [G loss: 0.327048]\n",
            "Epoch: 53.100 [D loss: 1.341979, acc: 0.00%] [G loss: 0.330385]\n",
            "Epoch: 53.101 [D loss: 1.424132, acc: 0.00%] [G loss: 0.330114]\n",
            "Epoch: 53.102 [D loss: 1.347827, acc: 0.00%] [G loss: 0.328800]\n",
            "Epoch: 53.103 [D loss: 1.372232, acc: 0.00%] [G loss: 0.329074]\n",
            "Epoch: 53.104 [D loss: 1.389955, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 53.105 [D loss: 1.370178, acc: 0.00%] [G loss: 0.328934]\n",
            "Epoch: 53.106 [D loss: 1.408022, acc: 0.00%] [G loss: 0.326981]\n",
            "Epoch: 53.107 [D loss: 1.401227, acc: 0.00%] [G loss: 0.329286]\n",
            "Epoch: 54.0 [D loss: 1.346428, acc: 0.00%] [G loss: 0.329610]\n",
            "Epoch: 54.1 [D loss: 1.344045, acc: 0.00%] [G loss: 0.328983]\n",
            "Epoch: 54.2 [D loss: 1.277017, acc: 0.00%] [G loss: 0.329309]\n",
            "Epoch: 54.3 [D loss: 1.395486, acc: 0.00%] [G loss: 0.328238]\n",
            "Epoch: 54.4 [D loss: 1.417149, acc: 0.00%] [G loss: 0.328653]\n",
            "Epoch: 54.5 [D loss: 1.416362, acc: 0.00%] [G loss: 0.330588]\n",
            "Epoch: 54.6 [D loss: 1.363960, acc: 0.00%] [G loss: 0.328909]\n",
            "Epoch: 54.7 [D loss: 1.429343, acc: 0.00%] [G loss: 0.328449]\n",
            "Epoch: 54.8 [D loss: 1.433091, acc: 0.00%] [G loss: 0.333046]\n",
            "Epoch: 54.9 [D loss: 1.391672, acc: 0.00%] [G loss: 0.330856]\n",
            "Epoch: 54.10 [D loss: 1.349370, acc: 0.00%] [G loss: 0.326739]\n",
            "Epoch: 54.11 [D loss: 1.413357, acc: 0.00%] [G loss: 0.327590]\n",
            "Epoch: 54.12 [D loss: 1.322193, acc: 0.00%] [G loss: 0.328428]\n",
            "Epoch: 54.13 [D loss: 1.360936, acc: 0.00%] [G loss: 0.329156]\n",
            "Epoch: 54.14 [D loss: 1.317032, acc: 0.00%] [G loss: 0.328635]\n",
            "Epoch: 54.15 [D loss: 1.318143, acc: 0.00%] [G loss: 0.327439]\n",
            "Epoch: 54.16 [D loss: 1.387914, acc: 0.00%] [G loss: 0.326931]\n",
            "Epoch: 54.17 [D loss: 1.416345, acc: 0.00%] [G loss: 0.332055]\n",
            "Epoch: 54.18 [D loss: 1.416599, acc: 0.00%] [G loss: 0.330075]\n",
            "Epoch: 54.19 [D loss: 1.425062, acc: 0.00%] [G loss: 0.331647]\n",
            "Epoch: 54.20 [D loss: 1.356322, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 54.21 [D loss: 1.372790, acc: 0.00%] [G loss: 0.328945]\n",
            "Epoch: 54.22 [D loss: 1.423308, acc: 0.00%] [G loss: 0.327403]\n",
            "Epoch: 54.23 [D loss: 1.330214, acc: 0.00%] [G loss: 0.328624]\n",
            "Epoch: 54.24 [D loss: 1.392014, acc: 0.00%] [G loss: 0.329861]\n",
            "Epoch: 54.25 [D loss: 1.278430, acc: 0.00%] [G loss: 0.327763]\n",
            "Epoch: 54.26 [D loss: 1.389129, acc: 0.00%] [G loss: 0.328460]\n",
            "Epoch: 54.27 [D loss: 1.345496, acc: 0.00%] [G loss: 0.328182]\n",
            "Epoch: 54.28 [D loss: 1.424708, acc: 0.00%] [G loss: 0.330966]\n",
            "Epoch: 54.29 [D loss: 1.361841, acc: 0.00%] [G loss: 0.329302]\n",
            "Epoch: 54.30 [D loss: 1.375252, acc: 0.00%] [G loss: 0.328193]\n",
            "Epoch: 54.31 [D loss: 1.370849, acc: 0.00%] [G loss: 0.328462]\n",
            "Epoch: 54.32 [D loss: 1.343958, acc: 0.00%] [G loss: 0.328347]\n",
            "Epoch: 54.33 [D loss: 1.373833, acc: 0.00%] [G loss: 0.329156]\n",
            "Epoch: 54.34 [D loss: 1.389905, acc: 0.00%] [G loss: 0.328312]\n",
            "Epoch: 54.35 [D loss: 1.376826, acc: 0.00%] [G loss: 0.328069]\n",
            "Epoch: 54.36 [D loss: 1.346731, acc: 0.00%] [G loss: 0.330945]\n",
            "Epoch: 54.37 [D loss: 1.409062, acc: 0.00%] [G loss: 0.329306]\n",
            "Epoch: 54.38 [D loss: 1.431738, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 54.39 [D loss: 1.416781, acc: 0.00%] [G loss: 0.328133]\n",
            "Epoch: 54.40 [D loss: 1.373912, acc: 0.00%] [G loss: 0.328692]\n",
            "Epoch: 54.41 [D loss: 1.361273, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 54.42 [D loss: 1.381112, acc: 0.00%] [G loss: 0.330376]\n",
            "Epoch: 54.43 [D loss: 1.391184, acc: 0.00%] [G loss: 0.331341]\n",
            "Epoch: 54.44 [D loss: 1.360981, acc: 0.00%] [G loss: 0.328355]\n",
            "Epoch: 54.45 [D loss: 1.342652, acc: 0.00%] [G loss: 0.327143]\n",
            "Epoch: 54.46 [D loss: 1.312200, acc: 0.00%] [G loss: 0.329220]\n",
            "Epoch: 54.47 [D loss: 1.323830, acc: 0.00%] [G loss: 0.327840]\n",
            "Epoch: 54.48 [D loss: 1.293052, acc: 0.00%] [G loss: 0.329135]\n",
            "Epoch: 54.49 [D loss: 1.364856, acc: 0.00%] [G loss: 0.328957]\n",
            "Epoch: 54.50 [D loss: 1.367437, acc: 0.00%] [G loss: 0.329180]\n",
            "Epoch: 54.51 [D loss: 1.378399, acc: 0.00%] [G loss: 0.328650]\n",
            "Epoch: 54.52 [D loss: 1.377408, acc: 0.00%] [G loss: 0.328909]\n",
            "Epoch: 54.53 [D loss: 1.430198, acc: 0.00%] [G loss: 0.328900]\n",
            "Epoch: 54.54 [D loss: 1.470191, acc: 0.00%] [G loss: 0.328320]\n",
            "Epoch: 54.55 [D loss: 1.378390, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 54.56 [D loss: 1.379771, acc: 0.00%] [G loss: 0.328499]\n",
            "Epoch: 54.57 [D loss: 1.420482, acc: 0.00%] [G loss: 0.328065]\n",
            "Epoch: 54.58 [D loss: 1.354340, acc: 0.00%] [G loss: 0.328002]\n",
            "Epoch: 54.59 [D loss: 1.337726, acc: 0.00%] [G loss: 0.327636]\n",
            "Epoch: 54.60 [D loss: 1.336836, acc: 0.00%] [G loss: 0.330363]\n",
            "Epoch: 54.61 [D loss: 1.380926, acc: 0.00%] [G loss: 0.328583]\n",
            "Epoch: 54.62 [D loss: 1.321703, acc: 0.00%] [G loss: 0.328551]\n",
            "Epoch: 54.63 [D loss: 1.346128, acc: 0.00%] [G loss: 0.327210]\n",
            "Epoch: 54.64 [D loss: 1.357538, acc: 0.00%] [G loss: 0.330657]\n",
            "Epoch: 54.65 [D loss: 1.384388, acc: 0.00%] [G loss: 0.328110]\n",
            "Epoch: 54.66 [D loss: 1.401671, acc: 0.00%] [G loss: 0.328482]\n",
            "Epoch: 54.67 [D loss: 1.384700, acc: 0.00%] [G loss: 0.328925]\n",
            "Epoch: 54.68 [D loss: 1.374263, acc: 0.00%] [G loss: 0.328414]\n",
            "Epoch: 54.69 [D loss: 1.306387, acc: 0.00%] [G loss: 0.327057]\n",
            "Epoch: 54.70 [D loss: 1.367583, acc: 0.00%] [G loss: 0.328638]\n",
            "Epoch: 54.71 [D loss: 1.416209, acc: 0.00%] [G loss: 0.328560]\n",
            "Epoch: 54.72 [D loss: 1.362936, acc: 0.00%] [G loss: 0.327881]\n",
            "Epoch: 54.73 [D loss: 1.354800, acc: 0.00%] [G loss: 0.328337]\n",
            "Epoch: 54.74 [D loss: 1.395010, acc: 0.00%] [G loss: 0.329060]\n",
            "Epoch: 54.75 [D loss: 1.376014, acc: 0.00%] [G loss: 0.328444]\n",
            "Epoch: 54.76 [D loss: 1.346445, acc: 0.00%] [G loss: 0.328517]\n",
            "Epoch: 54.77 [D loss: 1.340656, acc: 0.00%] [G loss: 0.330219]\n",
            "Epoch: 54.78 [D loss: 1.394369, acc: 0.00%] [G loss: 0.328941]\n",
            "Epoch: 54.79 [D loss: 1.396884, acc: 0.00%] [G loss: 0.328920]\n",
            "Epoch: 54.80 [D loss: 1.362956, acc: 0.00%] [G loss: 0.327691]\n",
            "Epoch: 54.81 [D loss: 1.341615, acc: 0.00%] [G loss: 0.328782]\n",
            "Epoch: 54.82 [D loss: 1.377876, acc: 0.00%] [G loss: 0.327846]\n",
            "Epoch: 54.83 [D loss: 1.360627, acc: 0.00%] [G loss: 0.329227]\n",
            "Epoch: 54.84 [D loss: 1.313786, acc: 0.00%] [G loss: 0.327257]\n",
            "Epoch: 54.85 [D loss: 1.362231, acc: 0.00%] [G loss: 0.328143]\n",
            "Epoch: 54.86 [D loss: 1.361206, acc: 0.00%] [G loss: 0.328867]\n",
            "Epoch: 54.87 [D loss: 1.463760, acc: 0.00%] [G loss: 0.328816]\n",
            "Epoch: 54.88 [D loss: 1.334983, acc: 0.00%] [G loss: 0.329868]\n",
            "Epoch: 54.89 [D loss: 1.346689, acc: 0.00%] [G loss: 0.327261]\n",
            "Epoch: 54.90 [D loss: 1.365151, acc: 0.00%] [G loss: 0.329214]\n",
            "Epoch: 54.91 [D loss: 1.379840, acc: 0.00%] [G loss: 0.329315]\n",
            "Epoch: 54.92 [D loss: 1.391501, acc: 0.00%] [G loss: 0.328014]\n",
            "Epoch: 54.93 [D loss: 1.327113, acc: 0.00%] [G loss: 0.330278]\n",
            "Epoch: 54.94 [D loss: 1.380981, acc: 0.00%] [G loss: 0.329352]\n",
            "Epoch: 54.95 [D loss: 1.356573, acc: 0.00%] [G loss: 0.331039]\n",
            "Epoch: 54.96 [D loss: 1.338512, acc: 0.00%] [G loss: 0.329164]\n",
            "Epoch: 54.97 [D loss: 1.343801, acc: 0.00%] [G loss: 0.330540]\n",
            "Epoch: 54.98 [D loss: 1.298341, acc: 0.00%] [G loss: 0.329004]\n",
            "Epoch: 54.99 [D loss: 1.364677, acc: 0.00%] [G loss: 0.329828]\n",
            "Epoch: 54.100 [D loss: 1.371626, acc: 0.00%] [G loss: 0.328383]\n",
            "Epoch: 54.101 [D loss: 1.432543, acc: 0.00%] [G loss: 0.327328]\n",
            "Epoch: 54.102 [D loss: 1.470841, acc: 0.00%] [G loss: 0.330140]\n",
            "Epoch: 54.103 [D loss: 1.493664, acc: 0.00%] [G loss: 0.329151]\n",
            "Epoch: 54.104 [D loss: 1.399902, acc: 0.00%] [G loss: 0.330962]\n",
            "Epoch: 54.105 [D loss: 1.392331, acc: 0.00%] [G loss: 0.328571]\n",
            "Epoch: 54.106 [D loss: 1.332732, acc: 0.00%] [G loss: 0.328128]\n",
            "Epoch: 54.107 [D loss: 1.302098, acc: 0.00%] [G loss: 0.328733]\n",
            "Epoch: 55.0 [D loss: 1.348084, acc: 0.00%] [G loss: 0.328134]\n",
            "Epoch: 55.1 [D loss: 1.326581, acc: 0.00%] [G loss: 0.329281]\n",
            "Epoch: 55.2 [D loss: 1.279335, acc: 0.00%] [G loss: 0.329356]\n",
            "Epoch: 55.3 [D loss: 1.378198, acc: 0.00%] [G loss: 0.328693]\n",
            "Epoch: 55.4 [D loss: 1.408971, acc: 0.00%] [G loss: 0.330479]\n",
            "Epoch: 55.5 [D loss: 1.370515, acc: 0.00%] [G loss: 0.330500]\n",
            "Epoch: 55.6 [D loss: 1.383037, acc: 0.00%] [G loss: 0.328362]\n",
            "Epoch: 55.7 [D loss: 1.376748, acc: 0.00%] [G loss: 0.330900]\n",
            "Epoch: 55.8 [D loss: 1.414105, acc: 0.00%] [G loss: 0.328019]\n",
            "Epoch: 55.9 [D loss: 1.324908, acc: 0.00%] [G loss: 0.327620]\n",
            "Epoch: 55.10 [D loss: 1.324252, acc: 0.00%] [G loss: 0.327595]\n",
            "Epoch: 55.11 [D loss: 1.337214, acc: 0.00%] [G loss: 0.329556]\n",
            "Epoch: 55.12 [D loss: 1.375721, acc: 0.00%] [G loss: 0.327810]\n",
            "Epoch: 55.13 [D loss: 1.400555, acc: 0.00%] [G loss: 0.327782]\n",
            "Epoch: 55.14 [D loss: 1.315109, acc: 0.00%] [G loss: 0.327974]\n",
            "Epoch: 55.15 [D loss: 1.390216, acc: 0.00%] [G loss: 0.330712]\n",
            "Epoch: 55.16 [D loss: 1.394744, acc: 0.00%] [G loss: 0.328097]\n",
            "Epoch: 55.17 [D loss: 1.386511, acc: 0.00%] [G loss: 0.328840]\n",
            "Epoch: 55.18 [D loss: 1.388733, acc: 0.00%] [G loss: 0.330222]\n",
            "Epoch: 55.19 [D loss: 1.416762, acc: 0.00%] [G loss: 0.328574]\n",
            "Epoch: 55.20 [D loss: 1.451745, acc: 0.00%] [G loss: 0.328726]\n",
            "Epoch: 55.21 [D loss: 1.372474, acc: 0.00%] [G loss: 0.328304]\n",
            "Epoch: 55.22 [D loss: 1.381225, acc: 0.00%] [G loss: 0.329295]\n",
            "Epoch: 55.23 [D loss: 1.395177, acc: 0.00%] [G loss: 0.329602]\n",
            "Epoch: 55.24 [D loss: 1.393640, acc: 0.00%] [G loss: 0.328649]\n",
            "Epoch: 55.25 [D loss: 1.405246, acc: 0.00%] [G loss: 0.329261]\n",
            "Epoch: 55.26 [D loss: 1.366490, acc: 0.00%] [G loss: 0.328082]\n",
            "Epoch: 55.27 [D loss: 1.403008, acc: 0.00%] [G loss: 0.327564]\n",
            "Epoch: 55.28 [D loss: 1.348999, acc: 0.00%] [G loss: 0.328181]\n",
            "Epoch: 55.29 [D loss: 1.329231, acc: 0.00%] [G loss: 0.330400]\n",
            "Epoch: 55.30 [D loss: 1.396342, acc: 0.00%] [G loss: 0.327454]\n",
            "Epoch: 55.31 [D loss: 1.356552, acc: 0.00%] [G loss: 0.329379]\n",
            "Epoch: 55.32 [D loss: 1.322675, acc: 0.00%] [G loss: 0.328490]\n",
            "Epoch: 55.33 [D loss: 1.280821, acc: 0.00%] [G loss: 0.327961]\n",
            "Epoch: 55.34 [D loss: 1.389798, acc: 0.00%] [G loss: 0.327965]\n",
            "Epoch: 55.35 [D loss: 1.341684, acc: 0.00%] [G loss: 0.327976]\n",
            "Epoch: 55.36 [D loss: 1.361991, acc: 0.00%] [G loss: 0.328497]\n",
            "Epoch: 55.37 [D loss: 1.283120, acc: 0.00%] [G loss: 0.328805]\n",
            "Epoch: 55.38 [D loss: 1.346638, acc: 0.00%] [G loss: 0.329841]\n",
            "Epoch: 55.39 [D loss: 1.409564, acc: 0.00%] [G loss: 0.327323]\n",
            "Epoch: 55.40 [D loss: 1.375253, acc: 0.00%] [G loss: 0.327663]\n",
            "Epoch: 55.41 [D loss: 1.378937, acc: 0.00%] [G loss: 0.329064]\n",
            "Epoch: 55.42 [D loss: 1.402757, acc: 0.00%] [G loss: 0.328308]\n",
            "Epoch: 55.43 [D loss: 1.360535, acc: 0.00%] [G loss: 0.327798]\n",
            "Epoch: 55.44 [D loss: 1.370435, acc: 0.00%] [G loss: 0.328670]\n",
            "Epoch: 55.45 [D loss: 1.357597, acc: 0.00%] [G loss: 0.329316]\n",
            "Epoch: 55.46 [D loss: 1.321868, acc: 0.00%] [G loss: 0.329571]\n",
            "Epoch: 55.47 [D loss: 1.368189, acc: 0.00%] [G loss: 0.330217]\n",
            "Epoch: 55.48 [D loss: 1.338972, acc: 0.00%] [G loss: 0.327326]\n",
            "Epoch: 55.49 [D loss: 1.362242, acc: 0.00%] [G loss: 0.327552]\n",
            "Epoch: 55.50 [D loss: 1.442141, acc: 0.00%] [G loss: 0.329842]\n",
            "Epoch: 55.51 [D loss: 1.315883, acc: 0.00%] [G loss: 0.328904]\n",
            "Epoch: 55.52 [D loss: 1.407488, acc: 0.00%] [G loss: 0.329058]\n",
            "Epoch: 55.53 [D loss: 1.343640, acc: 0.00%] [G loss: 0.328677]\n",
            "Epoch: 55.54 [D loss: 1.425151, acc: 0.00%] [G loss: 0.328063]\n",
            "Epoch: 55.55 [D loss: 1.349714, acc: 0.00%] [G loss: 0.328284]\n",
            "Epoch: 55.56 [D loss: 1.335787, acc: 0.00%] [G loss: 0.331626]\n",
            "Epoch: 55.57 [D loss: 1.383921, acc: 0.00%] [G loss: 0.331022]\n",
            "Epoch: 55.58 [D loss: 1.469881, acc: 0.00%] [G loss: 0.327738]\n",
            "Epoch: 55.59 [D loss: 1.382217, acc: 0.00%] [G loss: 0.329938]\n",
            "Epoch: 55.60 [D loss: 1.376653, acc: 0.00%] [G loss: 0.330586]\n",
            "Epoch: 55.61 [D loss: 1.387323, acc: 0.00%] [G loss: 0.330395]\n",
            "Epoch: 55.62 [D loss: 1.400702, acc: 0.00%] [G loss: 0.327828]\n",
            "Epoch: 55.63 [D loss: 1.372174, acc: 0.00%] [G loss: 0.329675]\n",
            "Epoch: 55.64 [D loss: 1.339727, acc: 0.00%] [G loss: 0.328951]\n",
            "Epoch: 55.65 [D loss: 1.329251, acc: 0.00%] [G loss: 0.330763]\n",
            "Epoch: 55.66 [D loss: 1.333909, acc: 0.00%] [G loss: 0.328996]\n",
            "Epoch: 55.67 [D loss: 1.404205, acc: 0.00%] [G loss: 0.328503]\n",
            "Epoch: 55.68 [D loss: 1.363223, acc: 0.00%] [G loss: 0.329289]\n",
            "Epoch: 55.69 [D loss: 1.453539, acc: 0.00%] [G loss: 0.328316]\n",
            "Epoch: 55.70 [D loss: 1.390492, acc: 0.00%] [G loss: 0.330381]\n",
            "Epoch: 55.71 [D loss: 1.358225, acc: 0.00%] [G loss: 0.327128]\n",
            "Epoch: 55.72 [D loss: 1.337907, acc: 0.00%] [G loss: 0.329700]\n",
            "Epoch: 55.73 [D loss: 1.409404, acc: 0.00%] [G loss: 0.329932]\n",
            "Epoch: 55.74 [D loss: 1.393443, acc: 0.00%] [G loss: 0.328204]\n",
            "Epoch: 55.75 [D loss: 1.350571, acc: 0.00%] [G loss: 0.328277]\n",
            "Epoch: 55.76 [D loss: 1.387194, acc: 0.00%] [G loss: 0.328610]\n",
            "Epoch: 55.77 [D loss: 1.278645, acc: 0.00%] [G loss: 0.329845]\n",
            "Epoch: 55.78 [D loss: 1.347913, acc: 0.00%] [G loss: 0.329077]\n",
            "Epoch: 55.79 [D loss: 1.263921, acc: 0.00%] [G loss: 0.327136]\n",
            "Epoch: 55.80 [D loss: 1.291542, acc: 0.00%] [G loss: 0.329616]\n",
            "Epoch: 55.81 [D loss: 1.318960, acc: 0.00%] [G loss: 0.329810]\n",
            "Epoch: 55.82 [D loss: 1.354264, acc: 0.00%] [G loss: 0.328240]\n",
            "Epoch: 55.83 [D loss: 1.429423, acc: 0.00%] [G loss: 0.328314]\n",
            "Epoch: 55.84 [D loss: 1.363629, acc: 0.00%] [G loss: 0.328399]\n",
            "Epoch: 55.85 [D loss: 1.415398, acc: 0.00%] [G loss: 0.328594]\n",
            "Epoch: 55.86 [D loss: 1.482274, acc: 0.00%] [G loss: 0.329719]\n",
            "Epoch: 55.87 [D loss: 1.430157, acc: 0.00%] [G loss: 0.327871]\n",
            "Epoch: 55.88 [D loss: 1.385714, acc: 0.00%] [G loss: 0.331794]\n",
            "Epoch: 55.89 [D loss: 1.329959, acc: 0.00%] [G loss: 0.327014]\n",
            "Epoch: 55.90 [D loss: 1.350245, acc: 0.00%] [G loss: 0.328608]\n",
            "Epoch: 55.91 [D loss: 1.328111, acc: 0.00%] [G loss: 0.329823]\n",
            "Epoch: 55.92 [D loss: 1.371948, acc: 0.00%] [G loss: 0.328848]\n",
            "Epoch: 55.93 [D loss: 1.333851, acc: 0.00%] [G loss: 0.328221]\n",
            "Epoch: 55.94 [D loss: 1.347224, acc: 0.00%] [G loss: 0.328432]\n",
            "Epoch: 55.95 [D loss: 1.341746, acc: 0.00%] [G loss: 0.328558]\n",
            "Epoch: 55.96 [D loss: 1.396281, acc: 0.00%] [G loss: 0.330481]\n",
            "Epoch: 55.97 [D loss: 1.349859, acc: 0.00%] [G loss: 0.327054]\n",
            "Epoch: 55.98 [D loss: 1.391140, acc: 0.00%] [G loss: 0.328357]\n",
            "Epoch: 55.99 [D loss: 1.369821, acc: 0.00%] [G loss: 0.327499]\n",
            "Epoch: 55.100 [D loss: 1.311745, acc: 0.00%] [G loss: 0.328366]\n",
            "Epoch: 55.101 [D loss: 1.295993, acc: 0.00%] [G loss: 0.329410]\n",
            "Epoch: 55.102 [D loss: 1.294285, acc: 0.00%] [G loss: 0.328846]\n",
            "Epoch: 55.103 [D loss: 1.298666, acc: 0.00%] [G loss: 0.329898]\n",
            "Epoch: 55.104 [D loss: 1.299151, acc: 0.00%] [G loss: 0.328030]\n",
            "Epoch: 55.105 [D loss: 1.322892, acc: 0.00%] [G loss: 0.328345]\n",
            "Epoch: 55.106 [D loss: 1.368776, acc: 0.00%] [G loss: 0.328256]\n",
            "Epoch: 55.107 [D loss: 1.396139, acc: 0.00%] [G loss: 0.329713]\n",
            "Epoch: 56.0 [D loss: 1.405549, acc: 0.00%] [G loss: 0.327237]\n",
            "Epoch: 56.1 [D loss: 1.338245, acc: 0.00%] [G loss: 0.329493]\n",
            "Epoch: 56.2 [D loss: 1.423184, acc: 0.00%] [G loss: 0.330808]\n",
            "Epoch: 56.3 [D loss: 1.396779, acc: 0.00%] [G loss: 0.328658]\n",
            "Epoch: 56.4 [D loss: 1.433558, acc: 0.00%] [G loss: 0.328448]\n",
            "Epoch: 56.5 [D loss: 1.408824, acc: 0.00%] [G loss: 0.329372]\n",
            "Epoch: 56.6 [D loss: 1.357959, acc: 0.00%] [G loss: 0.328346]\n",
            "Epoch: 56.7 [D loss: 1.345777, acc: 0.00%] [G loss: 0.332233]\n",
            "Epoch: 56.8 [D loss: 1.446600, acc: 0.00%] [G loss: 0.327805]\n",
            "Epoch: 56.9 [D loss: 1.417177, acc: 0.00%] [G loss: 0.328953]\n",
            "Epoch: 56.10 [D loss: 1.371399, acc: 0.00%] [G loss: 0.328071]\n",
            "Epoch: 56.11 [D loss: 1.408445, acc: 0.00%] [G loss: 0.327677]\n",
            "Epoch: 56.12 [D loss: 1.420601, acc: 0.00%] [G loss: 0.327326]\n",
            "Epoch: 56.13 [D loss: 1.353077, acc: 0.00%] [G loss: 0.329240]\n",
            "Epoch: 56.14 [D loss: 1.377838, acc: 0.00%] [G loss: 0.327330]\n",
            "Epoch: 56.15 [D loss: 1.394341, acc: 0.00%] [G loss: 0.328482]\n",
            "Epoch: 56.16 [D loss: 1.362218, acc: 0.00%] [G loss: 0.326883]\n",
            "Epoch: 56.17 [D loss: 1.346490, acc: 0.00%] [G loss: 0.328437]\n",
            "Epoch: 56.18 [D loss: 1.399380, acc: 0.00%] [G loss: 0.329942]\n",
            "Epoch: 56.19 [D loss: 1.358814, acc: 0.00%] [G loss: 0.328401]\n",
            "Epoch: 56.20 [D loss: 1.335542, acc: 0.00%] [G loss: 0.326569]\n",
            "Epoch: 56.21 [D loss: 1.358936, acc: 0.00%] [G loss: 0.329005]\n",
            "Epoch: 56.22 [D loss: 1.400959, acc: 0.00%] [G loss: 0.328015]\n",
            "Epoch: 56.23 [D loss: 1.363750, acc: 0.00%] [G loss: 0.328069]\n",
            "Epoch: 56.24 [D loss: 1.323813, acc: 0.00%] [G loss: 0.328991]\n",
            "Epoch: 56.25 [D loss: 1.409382, acc: 0.00%] [G loss: 0.327422]\n",
            "Epoch: 56.26 [D loss: 1.354725, acc: 0.00%] [G loss: 0.327032]\n",
            "Epoch: 56.27 [D loss: 1.303747, acc: 0.00%] [G loss: 0.327546]\n",
            "Epoch: 56.28 [D loss: 1.324727, acc: 0.00%] [G loss: 0.329303]\n",
            "Epoch: 56.29 [D loss: 1.366779, acc: 0.00%] [G loss: 0.330422]\n",
            "Epoch: 56.30 [D loss: 1.326672, acc: 0.00%] [G loss: 0.328187]\n",
            "Epoch: 56.31 [D loss: 1.339708, acc: 0.00%] [G loss: 0.327779]\n",
            "Epoch: 56.32 [D loss: 1.334796, acc: 0.00%] [G loss: 0.329375]\n",
            "Epoch: 56.33 [D loss: 1.419758, acc: 0.00%] [G loss: 0.328836]\n",
            "Epoch: 56.34 [D loss: 1.440999, acc: 0.00%] [G loss: 0.329518]\n",
            "Epoch: 56.35 [D loss: 1.396121, acc: 0.00%] [G loss: 0.330327]\n",
            "Epoch: 56.36 [D loss: 1.382832, acc: 0.00%] [G loss: 0.327342]\n",
            "Epoch: 56.37 [D loss: 1.426946, acc: 0.00%] [G loss: 0.328792]\n",
            "Epoch: 56.38 [D loss: 1.373832, acc: 0.00%] [G loss: 0.327381]\n",
            "Epoch: 56.39 [D loss: 1.377712, acc: 0.00%] [G loss: 0.330172]\n",
            "Epoch: 56.40 [D loss: 1.356617, acc: 0.00%] [G loss: 0.328758]\n",
            "Epoch: 56.41 [D loss: 1.388150, acc: 0.00%] [G loss: 0.327874]\n",
            "Epoch: 56.42 [D loss: 1.325756, acc: 0.00%] [G loss: 0.328840]\n",
            "Epoch: 56.43 [D loss: 1.281703, acc: 0.00%] [G loss: 0.329318]\n",
            "Epoch: 56.44 [D loss: 1.418505, acc: 0.00%] [G loss: 0.329985]\n",
            "Epoch: 56.45 [D loss: 1.370221, acc: 0.00%] [G loss: 0.328000]\n",
            "Epoch: 56.46 [D loss: 1.352864, acc: 0.00%] [G loss: 0.328149]\n",
            "Epoch: 56.47 [D loss: 1.405651, acc: 0.00%] [G loss: 0.328837]\n",
            "Epoch: 56.48 [D loss: 1.385145, acc: 0.00%] [G loss: 0.328535]\n",
            "Epoch: 56.49 [D loss: 1.400475, acc: 0.00%] [G loss: 0.329352]\n",
            "Epoch: 56.50 [D loss: 1.316606, acc: 0.00%] [G loss: 0.327690]\n",
            "Epoch: 56.51 [D loss: 1.364098, acc: 0.00%] [G loss: 0.328763]\n",
            "Epoch: 56.52 [D loss: 1.338197, acc: 0.00%] [G loss: 0.328405]\n",
            "Epoch: 56.53 [D loss: 1.348003, acc: 0.00%] [G loss: 0.328128]\n",
            "Epoch: 56.54 [D loss: 1.410617, acc: 0.00%] [G loss: 0.327074]\n",
            "Epoch: 56.55 [D loss: 1.398805, acc: 0.00%] [G loss: 0.328129]\n",
            "Epoch: 56.56 [D loss: 1.421484, acc: 0.00%] [G loss: 0.328192]\n",
            "Epoch: 56.57 [D loss: 1.348107, acc: 0.00%] [G loss: 0.328599]\n",
            "Epoch: 56.58 [D loss: 1.376190, acc: 0.00%] [G loss: 0.329086]\n",
            "Epoch: 56.59 [D loss: 1.326019, acc: 0.00%] [G loss: 0.326905]\n",
            "Epoch: 56.60 [D loss: 1.363357, acc: 0.00%] [G loss: 0.329989]\n",
            "Epoch: 56.61 [D loss: 1.353820, acc: 0.00%] [G loss: 0.327714]\n",
            "Epoch: 56.62 [D loss: 1.413321, acc: 0.00%] [G loss: 0.328303]\n",
            "Epoch: 56.63 [D loss: 1.369540, acc: 0.00%] [G loss: 0.327486]\n",
            "Epoch: 56.64 [D loss: 1.409770, acc: 0.00%] [G loss: 0.328754]\n",
            "Epoch: 56.65 [D loss: 1.385294, acc: 0.00%] [G loss: 0.329183]\n",
            "Epoch: 56.66 [D loss: 1.367540, acc: 0.00%] [G loss: 0.329518]\n",
            "Epoch: 56.67 [D loss: 1.357472, acc: 0.00%] [G loss: 0.328401]\n",
            "Epoch: 56.68 [D loss: 1.281004, acc: 0.00%] [G loss: 0.328882]\n",
            "Epoch: 56.69 [D loss: 1.331054, acc: 0.00%] [G loss: 0.328706]\n",
            "Epoch: 56.70 [D loss: 1.296063, acc: 0.00%] [G loss: 0.327941]\n",
            "Epoch: 56.71 [D loss: 1.324206, acc: 0.00%] [G loss: 0.327925]\n",
            "Epoch: 56.72 [D loss: 1.347779, acc: 0.00%] [G loss: 0.328282]\n",
            "Epoch: 56.73 [D loss: 1.377055, acc: 0.00%] [G loss: 0.329063]\n",
            "Epoch: 56.74 [D loss: 1.434276, acc: 0.00%] [G loss: 0.327944]\n",
            "Epoch: 56.75 [D loss: 1.350226, acc: 0.00%] [G loss: 0.328492]\n",
            "Epoch: 56.76 [D loss: 1.356281, acc: 0.00%] [G loss: 0.327956]\n",
            "Epoch: 56.77 [D loss: 1.371819, acc: 0.00%] [G loss: 0.326723]\n",
            "Epoch: 56.78 [D loss: 1.391847, acc: 0.00%] [G loss: 0.328197]\n",
            "Epoch: 56.79 [D loss: 1.337138, acc: 0.00%] [G loss: 0.330369]\n",
            "Epoch: 56.80 [D loss: 1.355646, acc: 0.00%] [G loss: 0.327790]\n",
            "Epoch: 56.81 [D loss: 1.434893, acc: 0.00%] [G loss: 0.329343]\n",
            "Epoch: 56.82 [D loss: 1.340649, acc: 0.00%] [G loss: 0.330854]\n",
            "Epoch: 56.83 [D loss: 1.351701, acc: 0.00%] [G loss: 0.327368]\n",
            "Epoch: 56.84 [D loss: 1.410944, acc: 0.00%] [G loss: 0.328267]\n",
            "Epoch: 56.85 [D loss: 1.383722, acc: 0.00%] [G loss: 0.327766]\n",
            "Epoch: 56.86 [D loss: 1.421275, acc: 0.00%] [G loss: 0.331101]\n",
            "Epoch: 56.87 [D loss: 1.322746, acc: 0.00%] [G loss: 0.328767]\n",
            "Epoch: 56.88 [D loss: 1.334015, acc: 0.00%] [G loss: 0.329103]\n",
            "Epoch: 56.89 [D loss: 1.348324, acc: 0.00%] [G loss: 0.328158]\n",
            "Epoch: 56.90 [D loss: 1.349476, acc: 0.00%] [G loss: 0.330214]\n",
            "Epoch: 56.91 [D loss: 1.326754, acc: 0.00%] [G loss: 0.329988]\n",
            "Epoch: 56.92 [D loss: 1.296361, acc: 0.00%] [G loss: 0.327993]\n",
            "Epoch: 56.93 [D loss: 1.351235, acc: 0.00%] [G loss: 0.329037]\n",
            "Epoch: 56.94 [D loss: 1.389216, acc: 0.00%] [G loss: 0.327614]\n",
            "Epoch: 56.95 [D loss: 1.354180, acc: 0.00%] [G loss: 0.327002]\n",
            "Epoch: 56.96 [D loss: 1.392628, acc: 0.00%] [G loss: 0.327706]\n",
            "Epoch: 56.97 [D loss: 1.439080, acc: 0.00%] [G loss: 0.328483]\n",
            "Epoch: 56.98 [D loss: 1.320635, acc: 0.00%] [G loss: 0.329745]\n",
            "Epoch: 56.99 [D loss: 1.355411, acc: 0.00%] [G loss: 0.330086]\n",
            "Epoch: 56.100 [D loss: 1.365374, acc: 0.00%] [G loss: 0.328981]\n",
            "Epoch: 56.101 [D loss: 1.373727, acc: 0.00%] [G loss: 0.328240]\n",
            "Epoch: 56.102 [D loss: 1.343640, acc: 0.00%] [G loss: 0.327379]\n",
            "Epoch: 56.103 [D loss: 1.361761, acc: 0.00%] [G loss: 0.330534]\n",
            "Epoch: 56.104 [D loss: 1.388419, acc: 0.00%] [G loss: 0.328203]\n",
            "Epoch: 56.105 [D loss: 1.376273, acc: 0.00%] [G loss: 0.329352]\n",
            "Epoch: 56.106 [D loss: 1.374613, acc: 0.00%] [G loss: 0.328169]\n",
            "Epoch: 56.107 [D loss: 1.378292, acc: 0.00%] [G loss: 0.327960]\n",
            "Epoch: 57.0 [D loss: 1.401185, acc: 0.00%] [G loss: 0.327703]\n",
            "Epoch: 57.1 [D loss: 1.374002, acc: 0.00%] [G loss: 0.327649]\n",
            "Epoch: 57.2 [D loss: 1.349347, acc: 0.00%] [G loss: 0.329707]\n",
            "Epoch: 57.3 [D loss: 1.399375, acc: 0.00%] [G loss: 0.327593]\n",
            "Epoch: 57.4 [D loss: 1.372005, acc: 0.00%] [G loss: 0.328975]\n",
            "Epoch: 57.5 [D loss: 1.358886, acc: 0.00%] [G loss: 0.328780]\n",
            "Epoch: 57.6 [D loss: 1.334502, acc: 0.00%] [G loss: 0.328015]\n",
            "Epoch: 57.7 [D loss: 1.371768, acc: 0.00%] [G loss: 0.328905]\n",
            "Epoch: 57.8 [D loss: 1.387062, acc: 0.00%] [G loss: 0.327579]\n",
            "Epoch: 57.9 [D loss: 1.369867, acc: 0.00%] [G loss: 0.328535]\n",
            "Epoch: 57.10 [D loss: 1.326433, acc: 0.00%] [G loss: 0.329096]\n",
            "Epoch: 57.11 [D loss: 1.421699, acc: 0.00%] [G loss: 0.327243]\n",
            "Epoch: 57.12 [D loss: 1.449615, acc: 0.00%] [G loss: 0.327412]\n",
            "Epoch: 57.13 [D loss: 1.441368, acc: 0.00%] [G loss: 0.330665]\n",
            "Epoch: 57.14 [D loss: 1.339662, acc: 0.00%] [G loss: 0.328734]\n",
            "Epoch: 57.15 [D loss: 1.326746, acc: 0.00%] [G loss: 0.328734]\n",
            "Epoch: 57.16 [D loss: 1.341951, acc: 0.00%] [G loss: 0.330541]\n",
            "Epoch: 57.17 [D loss: 1.335427, acc: 0.00%] [G loss: 0.330861]\n",
            "Epoch: 57.18 [D loss: 1.312722, acc: 0.00%] [G loss: 0.328343]\n",
            "Epoch: 57.19 [D loss: 1.341332, acc: 0.00%] [G loss: 0.327541]\n",
            "Epoch: 57.20 [D loss: 1.380904, acc: 0.00%] [G loss: 0.329138]\n",
            "Epoch: 57.21 [D loss: 1.439888, acc: 0.00%] [G loss: 0.329439]\n",
            "Epoch: 57.22 [D loss: 1.424567, acc: 0.00%] [G loss: 0.329925]\n",
            "Epoch: 57.23 [D loss: 1.429072, acc: 0.00%] [G loss: 0.329134]\n",
            "Epoch: 57.24 [D loss: 1.392046, acc: 0.00%] [G loss: 0.328462]\n",
            "Epoch: 57.25 [D loss: 1.338943, acc: 0.00%] [G loss: 0.327533]\n",
            "Epoch: 57.26 [D loss: 1.342805, acc: 0.00%] [G loss: 0.327734]\n",
            "Epoch: 57.27 [D loss: 1.271110, acc: 0.00%] [G loss: 0.328803]\n",
            "Epoch: 57.28 [D loss: 1.303891, acc: 0.00%] [G loss: 0.328773]\n",
            "Epoch: 57.29 [D loss: 1.303724, acc: 0.00%] [G loss: 0.328208]\n",
            "Epoch: 57.30 [D loss: 1.313337, acc: 0.00%] [G loss: 0.328901]\n",
            "Epoch: 57.31 [D loss: 1.355537, acc: 0.00%] [G loss: 0.329252]\n",
            "Epoch: 57.32 [D loss: 1.363524, acc: 0.00%] [G loss: 0.328749]\n",
            "Epoch: 57.33 [D loss: 1.398684, acc: 0.00%] [G loss: 0.330050]\n",
            "Epoch: 57.34 [D loss: 1.425065, acc: 0.00%] [G loss: 0.328088]\n",
            "Epoch: 57.35 [D loss: 1.417363, acc: 0.00%] [G loss: 0.330110]\n",
            "Epoch: 57.36 [D loss: 1.406746, acc: 0.00%] [G loss: 0.327566]\n",
            "Epoch: 57.37 [D loss: 1.364308, acc: 0.00%] [G loss: 0.328550]\n",
            "Epoch: 57.38 [D loss: 1.376171, acc: 0.00%] [G loss: 0.327923]\n",
            "Epoch: 57.39 [D loss: 1.344103, acc: 0.00%] [G loss: 0.327880]\n",
            "Epoch: 57.40 [D loss: 1.350736, acc: 0.00%] [G loss: 0.329846]\n",
            "Epoch: 57.41 [D loss: 1.371240, acc: 0.00%] [G loss: 0.329865]\n",
            "Epoch: 57.42 [D loss: 1.362431, acc: 0.00%] [G loss: 0.327998]\n",
            "Epoch: 57.43 [D loss: 1.360749, acc: 0.00%] [G loss: 0.330211]\n",
            "Epoch: 57.44 [D loss: 1.353970, acc: 0.00%] [G loss: 0.330424]\n",
            "Epoch: 57.45 [D loss: 1.377780, acc: 0.00%] [G loss: 0.329630]\n",
            "Epoch: 57.46 [D loss: 1.383160, acc: 0.00%] [G loss: 0.328004]\n",
            "Epoch: 57.47 [D loss: 1.371092, acc: 0.00%] [G loss: 0.328174]\n",
            "Epoch: 57.48 [D loss: 1.333174, acc: 0.00%] [G loss: 0.327186]\n",
            "Epoch: 57.49 [D loss: 1.367998, acc: 0.00%] [G loss: 0.327850]\n",
            "Epoch: 57.50 [D loss: 1.399956, acc: 0.00%] [G loss: 0.328488]\n",
            "Epoch: 57.51 [D loss: 1.415099, acc: 0.00%] [G loss: 0.328803]\n",
            "Epoch: 57.52 [D loss: 1.385868, acc: 0.00%] [G loss: 0.329943]\n",
            "Epoch: 57.53 [D loss: 1.418600, acc: 0.00%] [G loss: 0.329553]\n",
            "Epoch: 57.54 [D loss: 1.432650, acc: 0.00%] [G loss: 0.329115]\n",
            "Epoch: 57.55 [D loss: 1.433096, acc: 0.00%] [G loss: 0.330066]\n",
            "Epoch: 57.56 [D loss: 1.424170, acc: 0.00%] [G loss: 0.328798]\n",
            "Epoch: 57.57 [D loss: 1.461700, acc: 0.00%] [G loss: 0.329197]\n",
            "Epoch: 57.58 [D loss: 1.424177, acc: 0.00%] [G loss: 0.329245]\n",
            "Epoch: 57.59 [D loss: 1.332402, acc: 0.00%] [G loss: 0.327367]\n",
            "Epoch: 57.60 [D loss: 1.272948, acc: 0.00%] [G loss: 0.328049]\n",
            "Epoch: 57.61 [D loss: 1.396672, acc: 0.00%] [G loss: 0.328329]\n",
            "Epoch: 57.62 [D loss: 1.339540, acc: 0.00%] [G loss: 0.328267]\n",
            "Epoch: 57.63 [D loss: 1.351541, acc: 0.00%] [G loss: 0.329204]\n",
            "Epoch: 57.64 [D loss: 1.388509, acc: 0.00%] [G loss: 0.330416]\n",
            "Epoch: 57.65 [D loss: 1.357696, acc: 0.00%] [G loss: 0.329033]\n",
            "Epoch: 57.66 [D loss: 1.395122, acc: 0.00%] [G loss: 0.328343]\n",
            "Epoch: 57.67 [D loss: 1.423833, acc: 0.00%] [G loss: 0.327912]\n",
            "Epoch: 57.68 [D loss: 1.442581, acc: 0.00%] [G loss: 0.327672]\n",
            "Epoch: 57.69 [D loss: 1.446066, acc: 0.00%] [G loss: 0.330698]\n",
            "Epoch: 57.70 [D loss: 1.387359, acc: 0.00%] [G loss: 0.327243]\n",
            "Epoch: 57.71 [D loss: 1.365202, acc: 0.00%] [G loss: 0.328809]\n",
            "Epoch: 57.72 [D loss: 1.360575, acc: 0.00%] [G loss: 0.330070]\n",
            "Epoch: 57.73 [D loss: 1.357004, acc: 0.00%] [G loss: 0.327180]\n",
            "Epoch: 57.74 [D loss: 1.338639, acc: 0.00%] [G loss: 0.328208]\n",
            "Epoch: 57.75 [D loss: 1.374816, acc: 0.00%] [G loss: 0.327045]\n",
            "Epoch: 57.76 [D loss: 1.378582, acc: 0.00%] [G loss: 0.328146]\n",
            "Epoch: 57.77 [D loss: 1.363101, acc: 0.00%] [G loss: 0.328587]\n",
            "Epoch: 57.78 [D loss: 1.351533, acc: 0.00%] [G loss: 0.330424]\n",
            "Epoch: 57.79 [D loss: 1.358289, acc: 0.00%] [G loss: 0.328467]\n",
            "Epoch: 57.80 [D loss: 1.385364, acc: 0.00%] [G loss: 0.327216]\n",
            "Epoch: 57.81 [D loss: 1.426822, acc: 0.00%] [G loss: 0.328531]\n",
            "Epoch: 57.82 [D loss: 1.422702, acc: 0.00%] [G loss: 0.329735]\n",
            "Epoch: 57.83 [D loss: 1.438933, acc: 0.00%] [G loss: 0.328900]\n",
            "Epoch: 57.84 [D loss: 1.402353, acc: 0.00%] [G loss: 0.329625]\n",
            "Epoch: 57.85 [D loss: 1.376660, acc: 0.00%] [G loss: 0.327902]\n",
            "Epoch: 57.86 [D loss: 1.376381, acc: 0.00%] [G loss: 0.329229]\n",
            "Epoch: 57.87 [D loss: 1.303445, acc: 0.00%] [G loss: 0.327827]\n",
            "Epoch: 57.88 [D loss: 1.309831, acc: 0.00%] [G loss: 0.327448]\n",
            "Epoch: 57.89 [D loss: 1.342160, acc: 0.00%] [G loss: 0.328990]\n",
            "Epoch: 57.90 [D loss: 1.274272, acc: 0.00%] [G loss: 0.329514]\n",
            "Epoch: 57.91 [D loss: 1.312912, acc: 0.00%] [G loss: 0.330524]\n",
            "Epoch: 57.92 [D loss: 1.329451, acc: 0.00%] [G loss: 0.329032]\n",
            "Epoch: 57.93 [D loss: 1.373064, acc: 0.00%] [G loss: 0.329900]\n",
            "Epoch: 57.94 [D loss: 1.428250, acc: 0.00%] [G loss: 0.329060]\n",
            "Epoch: 57.95 [D loss: 1.406751, acc: 0.00%] [G loss: 0.329326]\n",
            "Epoch: 57.96 [D loss: 1.422600, acc: 0.00%] [G loss: 0.329012]\n",
            "Epoch: 57.97 [D loss: 1.344996, acc: 0.00%] [G loss: 0.330436]\n",
            "Epoch: 57.98 [D loss: 1.360434, acc: 0.00%] [G loss: 0.327026]\n",
            "Epoch: 57.99 [D loss: 1.371278, acc: 0.00%] [G loss: 0.327451]\n",
            "Epoch: 57.100 [D loss: 1.378409, acc: 0.00%] [G loss: 0.328366]\n",
            "Epoch: 57.101 [D loss: 1.364393, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 57.102 [D loss: 1.295893, acc: 0.00%] [G loss: 0.328028]\n",
            "Epoch: 57.103 [D loss: 1.376595, acc: 0.00%] [G loss: 0.327090]\n",
            "Epoch: 57.104 [D loss: 1.393223, acc: 0.00%] [G loss: 0.327104]\n",
            "Epoch: 57.105 [D loss: 1.397962, acc: 0.00%] [G loss: 0.327467]\n",
            "Epoch: 57.106 [D loss: 1.368540, acc: 0.00%] [G loss: 0.329359]\n",
            "Epoch: 57.107 [D loss: 1.396147, acc: 0.00%] [G loss: 0.329931]\n",
            "Epoch: 58.0 [D loss: 1.432117, acc: 0.00%] [G loss: 0.328263]\n",
            "Epoch: 58.1 [D loss: 1.355662, acc: 0.00%] [G loss: 0.328988]\n",
            "Epoch: 58.2 [D loss: 1.338761, acc: 0.00%] [G loss: 0.328730]\n",
            "Epoch: 58.3 [D loss: 1.336300, acc: 0.00%] [G loss: 0.327149]\n",
            "Epoch: 58.4 [D loss: 1.384520, acc: 0.00%] [G loss: 0.327539]\n",
            "Epoch: 58.5 [D loss: 1.359665, acc: 0.00%] [G loss: 0.328094]\n",
            "Epoch: 58.6 [D loss: 1.353890, acc: 0.00%] [G loss: 0.329683]\n",
            "Epoch: 58.7 [D loss: 1.388837, acc: 0.00%] [G loss: 0.327416]\n",
            "Epoch: 58.8 [D loss: 1.366268, acc: 0.00%] [G loss: 0.328212]\n",
            "Epoch: 58.9 [D loss: 1.324419, acc: 0.00%] [G loss: 0.327809]\n",
            "Epoch: 58.10 [D loss: 1.358051, acc: 0.00%] [G loss: 0.327968]\n",
            "Epoch: 58.11 [D loss: 1.310504, acc: 0.00%] [G loss: 0.328422]\n",
            "Epoch: 58.12 [D loss: 1.377240, acc: 0.00%] [G loss: 0.328929]\n",
            "Epoch: 58.13 [D loss: 1.438417, acc: 0.00%] [G loss: 0.328115]\n",
            "Epoch: 58.14 [D loss: 1.372517, acc: 0.00%] [G loss: 0.327038]\n",
            "Epoch: 58.15 [D loss: 1.361582, acc: 0.00%] [G loss: 0.327273]\n",
            "Epoch: 58.16 [D loss: 1.368008, acc: 0.00%] [G loss: 0.327998]\n",
            "Epoch: 58.17 [D loss: 1.322972, acc: 0.00%] [G loss: 0.326645]\n",
            "Epoch: 58.18 [D loss: 1.333213, acc: 0.00%] [G loss: 0.328095]\n",
            "Epoch: 58.19 [D loss: 1.362911, acc: 0.00%] [G loss: 0.330107]\n",
            "Epoch: 58.20 [D loss: 1.301471, acc: 0.00%] [G loss: 0.329656]\n",
            "Epoch: 58.21 [D loss: 1.350531, acc: 0.00%] [G loss: 0.327282]\n",
            "Epoch: 58.22 [D loss: 1.434429, acc: 0.00%] [G loss: 0.331227]\n",
            "Epoch: 58.23 [D loss: 1.343532, acc: 0.00%] [G loss: 0.327804]\n",
            "Epoch: 58.24 [D loss: 1.367534, acc: 0.00%] [G loss: 0.329732]\n",
            "Epoch: 58.25 [D loss: 1.437017, acc: 0.00%] [G loss: 0.329639]\n",
            "Epoch: 58.26 [D loss: 1.387591, acc: 0.00%] [G loss: 0.329374]\n",
            "Epoch: 58.27 [D loss: 1.370885, acc: 0.00%] [G loss: 0.329855]\n",
            "Epoch: 58.28 [D loss: 1.402444, acc: 0.00%] [G loss: 0.328147]\n",
            "Epoch: 58.29 [D loss: 1.391709, acc: 0.00%] [G loss: 0.329141]\n",
            "Epoch: 58.30 [D loss: 1.366310, acc: 0.00%] [G loss: 0.328155]\n",
            "Epoch: 58.31 [D loss: 1.383197, acc: 0.00%] [G loss: 0.327587]\n",
            "Epoch: 58.32 [D loss: 1.398839, acc: 0.00%] [G loss: 0.329563]\n",
            "Epoch: 58.33 [D loss: 1.380444, acc: 0.00%] [G loss: 0.328818]\n",
            "Epoch: 58.34 [D loss: 1.369263, acc: 0.00%] [G loss: 0.328942]\n",
            "Epoch: 58.35 [D loss: 1.440951, acc: 0.00%] [G loss: 0.330438]\n",
            "Epoch: 58.36 [D loss: 1.374068, acc: 0.00%] [G loss: 0.328498]\n",
            "Epoch: 58.37 [D loss: 1.446206, acc: 0.00%] [G loss: 0.329171]\n",
            "Epoch: 58.38 [D loss: 1.445421, acc: 0.00%] [G loss: 0.329171]\n",
            "Epoch: 58.39 [D loss: 1.378748, acc: 0.00%] [G loss: 0.330531]\n",
            "Epoch: 58.40 [D loss: 1.403611, acc: 0.00%] [G loss: 0.329871]\n",
            "Epoch: 58.41 [D loss: 1.391916, acc: 0.00%] [G loss: 0.328671]\n",
            "Epoch: 58.42 [D loss: 1.361098, acc: 0.00%] [G loss: 0.328636]\n",
            "Epoch: 58.43 [D loss: 1.359776, acc: 0.00%] [G loss: 0.327219]\n",
            "Epoch: 58.44 [D loss: 1.352213, acc: 0.00%] [G loss: 0.328648]\n",
            "Epoch: 58.45 [D loss: 1.391762, acc: 0.00%] [G loss: 0.327989]\n",
            "Epoch: 58.46 [D loss: 1.345667, acc: 0.00%] [G loss: 0.330058]\n",
            "Epoch: 58.47 [D loss: 1.387698, acc: 0.00%] [G loss: 0.328366]\n",
            "Epoch: 58.48 [D loss: 1.381810, acc: 0.00%] [G loss: 0.328114]\n",
            "Epoch: 58.49 [D loss: 1.426682, acc: 0.00%] [G loss: 0.329675]\n",
            "Epoch: 58.50 [D loss: 1.330084, acc: 0.00%] [G loss: 0.327477]\n",
            "Epoch: 58.51 [D loss: 1.419056, acc: 0.00%] [G loss: 0.327906]\n",
            "Epoch: 58.52 [D loss: 1.370126, acc: 0.00%] [G loss: 0.330547]\n",
            "Epoch: 58.53 [D loss: 1.341500, acc: 0.00%] [G loss: 0.328369]\n",
            "Epoch: 58.54 [D loss: 1.372567, acc: 0.00%] [G loss: 0.328257]\n",
            "Epoch: 58.55 [D loss: 1.335887, acc: 0.00%] [G loss: 0.329552]\n",
            "Epoch: 58.56 [D loss: 1.312158, acc: 0.00%] [G loss: 0.329723]\n",
            "Epoch: 58.57 [D loss: 1.396179, acc: 0.00%] [G loss: 0.328719]\n",
            "Epoch: 58.58 [D loss: 1.409953, acc: 0.00%] [G loss: 0.327466]\n",
            "Epoch: 58.59 [D loss: 1.419524, acc: 0.00%] [G loss: 0.328901]\n",
            "Epoch: 58.60 [D loss: 1.381569, acc: 0.00%] [G loss: 0.329637]\n",
            "Epoch: 58.61 [D loss: 1.394961, acc: 0.00%] [G loss: 0.329738]\n",
            "Epoch: 58.62 [D loss: 1.378267, acc: 0.00%] [G loss: 0.328693]\n",
            "Epoch: 58.63 [D loss: 1.343385, acc: 0.00%] [G loss: 0.328670]\n",
            "Epoch: 58.64 [D loss: 1.337710, acc: 0.00%] [G loss: 0.326728]\n",
            "Epoch: 58.65 [D loss: 1.339068, acc: 0.00%] [G loss: 0.329810]\n",
            "Epoch: 58.66 [D loss: 1.370372, acc: 0.00%] [G loss: 0.328509]\n",
            "Epoch: 58.67 [D loss: 1.332614, acc: 0.00%] [G loss: 0.329617]\n",
            "Epoch: 58.68 [D loss: 1.464943, acc: 0.00%] [G loss: 0.327015]\n",
            "Epoch: 58.69 [D loss: 1.394207, acc: 0.00%] [G loss: 0.329394]\n",
            "Epoch: 58.70 [D loss: 1.384979, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 58.71 [D loss: 1.388915, acc: 0.00%] [G loss: 0.329327]\n",
            "Epoch: 58.72 [D loss: 1.386260, acc: 0.00%] [G loss: 0.327578]\n",
            "Epoch: 58.73 [D loss: 1.410382, acc: 0.00%] [G loss: 0.328985]\n",
            "Epoch: 58.74 [D loss: 1.405250, acc: 0.00%] [G loss: 0.327642]\n",
            "Epoch: 58.75 [D loss: 1.364465, acc: 0.00%] [G loss: 0.328177]\n",
            "Epoch: 58.76 [D loss: 1.409202, acc: 0.00%] [G loss: 0.329842]\n",
            "Epoch: 58.77 [D loss: 1.373486, acc: 0.00%] [G loss: 0.327321]\n",
            "Epoch: 58.78 [D loss: 1.361310, acc: 0.00%] [G loss: 0.328418]\n",
            "Epoch: 58.79 [D loss: 1.411266, acc: 0.00%] [G loss: 0.327695]\n",
            "Epoch: 58.80 [D loss: 1.366494, acc: 0.00%] [G loss: 0.328060]\n",
            "Epoch: 58.81 [D loss: 1.419576, acc: 0.00%] [G loss: 0.328380]\n",
            "Epoch: 58.82 [D loss: 1.376920, acc: 0.00%] [G loss: 0.330277]\n",
            "Epoch: 58.83 [D loss: 1.369348, acc: 0.00%] [G loss: 0.327650]\n",
            "Epoch: 58.84 [D loss: 1.428087, acc: 0.00%] [G loss: 0.328268]\n",
            "Epoch: 58.85 [D loss: 1.392427, acc: 0.00%] [G loss: 0.328787]\n",
            "Epoch: 58.86 [D loss: 1.397713, acc: 0.00%] [G loss: 0.328836]\n",
            "Epoch: 58.87 [D loss: 1.271170, acc: 0.00%] [G loss: 0.328422]\n",
            "Epoch: 58.88 [D loss: 1.364597, acc: 0.00%] [G loss: 0.327559]\n",
            "Epoch: 58.89 [D loss: 1.371977, acc: 0.00%] [G loss: 0.329042]\n",
            "Epoch: 58.90 [D loss: 1.380134, acc: 0.00%] [G loss: 0.329040]\n",
            "Epoch: 58.91 [D loss: 1.376167, acc: 0.00%] [G loss: 0.327249]\n",
            "Epoch: 58.92 [D loss: 1.390257, acc: 0.00%] [G loss: 0.328517]\n",
            "Epoch: 58.93 [D loss: 1.421430, acc: 0.00%] [G loss: 0.328148]\n",
            "Epoch: 58.94 [D loss: 1.425974, acc: 0.00%] [G loss: 0.328798]\n",
            "Epoch: 58.95 [D loss: 1.335409, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 58.96 [D loss: 1.372069, acc: 0.00%] [G loss: 0.327111]\n",
            "Epoch: 58.97 [D loss: 1.374716, acc: 0.00%] [G loss: 0.328509]\n",
            "Epoch: 58.98 [D loss: 1.403186, acc: 0.00%] [G loss: 0.327752]\n",
            "Epoch: 58.99 [D loss: 1.318109, acc: 0.00%] [G loss: 0.329158]\n",
            "Epoch: 58.100 [D loss: 1.347684, acc: 0.00%] [G loss: 0.327381]\n",
            "Epoch: 58.101 [D loss: 1.360700, acc: 0.00%] [G loss: 0.329391]\n",
            "Epoch: 58.102 [D loss: 1.399941, acc: 0.00%] [G loss: 0.329994]\n",
            "Epoch: 58.103 [D loss: 1.333738, acc: 0.00%] [G loss: 0.328188]\n",
            "Epoch: 58.104 [D loss: 1.431839, acc: 0.00%] [G loss: 0.327901]\n",
            "Epoch: 58.105 [D loss: 1.454167, acc: 0.00%] [G loss: 0.328396]\n",
            "Epoch: 58.106 [D loss: 1.397222, acc: 0.00%] [G loss: 0.329746]\n",
            "Epoch: 58.107 [D loss: 1.391566, acc: 0.00%] [G loss: 0.328953]\n",
            "Epoch: 59.0 [D loss: 1.427667, acc: 0.00%] [G loss: 0.328085]\n",
            "Epoch: 59.1 [D loss: 1.335162, acc: 0.00%] [G loss: 0.329131]\n",
            "Epoch: 59.2 [D loss: 1.321908, acc: 0.00%] [G loss: 0.328167]\n",
            "Epoch: 59.3 [D loss: 1.273938, acc: 0.00%] [G loss: 0.328255]\n",
            "Epoch: 59.4 [D loss: 1.270927, acc: 0.00%] [G loss: 0.331880]\n",
            "Epoch: 59.5 [D loss: 1.330131, acc: 0.00%] [G loss: 0.326473]\n",
            "Epoch: 59.6 [D loss: 1.332786, acc: 0.00%] [G loss: 0.328062]\n",
            "Epoch: 59.7 [D loss: 1.390841, acc: 0.00%] [G loss: 0.328166]\n",
            "Epoch: 59.8 [D loss: 1.356525, acc: 0.00%] [G loss: 0.328404]\n",
            "Epoch: 59.9 [D loss: 1.376740, acc: 0.00%] [G loss: 0.329161]\n",
            "Epoch: 59.10 [D loss: 1.377012, acc: 0.00%] [G loss: 0.328791]\n",
            "Epoch: 59.11 [D loss: 1.394991, acc: 0.00%] [G loss: 0.329925]\n",
            "Epoch: 59.12 [D loss: 1.413703, acc: 0.00%] [G loss: 0.328374]\n",
            "Epoch: 59.13 [D loss: 1.391047, acc: 0.00%] [G loss: 0.329672]\n",
            "Epoch: 59.14 [D loss: 1.385490, acc: 0.00%] [G loss: 0.328021]\n",
            "Epoch: 59.15 [D loss: 1.364066, acc: 0.00%] [G loss: 0.329129]\n",
            "Epoch: 59.16 [D loss: 1.386582, acc: 0.00%] [G loss: 0.328339]\n",
            "Epoch: 59.17 [D loss: 1.368005, acc: 0.00%] [G loss: 0.329461]\n",
            "Epoch: 59.18 [D loss: 1.366211, acc: 0.00%] [G loss: 0.328467]\n",
            "Epoch: 59.19 [D loss: 1.332710, acc: 0.00%] [G loss: 0.329104]\n",
            "Epoch: 59.20 [D loss: 1.372444, acc: 0.00%] [G loss: 0.330914]\n",
            "Epoch: 59.21 [D loss: 1.368621, acc: 0.00%] [G loss: 0.327732]\n",
            "Epoch: 59.22 [D loss: 1.386282, acc: 0.00%] [G loss: 0.329577]\n",
            "Epoch: 59.23 [D loss: 1.442897, acc: 0.00%] [G loss: 0.329334]\n",
            "Epoch: 59.24 [D loss: 1.417851, acc: 0.00%] [G loss: 0.329121]\n",
            "Epoch: 59.25 [D loss: 1.438170, acc: 0.00%] [G loss: 0.329654]\n",
            "Epoch: 59.26 [D loss: 1.382146, acc: 0.00%] [G loss: 0.328619]\n",
            "Epoch: 59.27 [D loss: 1.393169, acc: 0.00%] [G loss: 0.329789]\n",
            "Epoch: 59.28 [D loss: 1.391088, acc: 0.00%] [G loss: 0.328359]\n",
            "Epoch: 59.29 [D loss: 1.315616, acc: 0.00%] [G loss: 0.328891]\n",
            "Epoch: 59.30 [D loss: 1.309435, acc: 0.00%] [G loss: 0.327146]\n",
            "Epoch: 59.31 [D loss: 1.349288, acc: 0.00%] [G loss: 0.329045]\n",
            "Epoch: 59.32 [D loss: 1.364068, acc: 0.00%] [G loss: 0.327539]\n",
            "Epoch: 59.33 [D loss: 1.381531, acc: 0.00%] [G loss: 0.327713]\n",
            "Epoch: 59.34 [D loss: 1.422224, acc: 0.00%] [G loss: 0.328056]\n",
            "Epoch: 59.35 [D loss: 1.389879, acc: 0.00%] [G loss: 0.329275]\n",
            "Epoch: 59.36 [D loss: 1.423761, acc: 0.00%] [G loss: 0.329218]\n",
            "Epoch: 59.37 [D loss: 1.381312, acc: 0.00%] [G loss: 0.330714]\n",
            "Epoch: 59.38 [D loss: 1.379026, acc: 0.00%] [G loss: 0.328456]\n",
            "Epoch: 59.39 [D loss: 1.333522, acc: 0.00%] [G loss: 0.328615]\n",
            "Epoch: 59.40 [D loss: 1.355931, acc: 0.00%] [G loss: 0.330373]\n",
            "Epoch: 59.41 [D loss: 1.374284, acc: 0.00%] [G loss: 0.327074]\n",
            "Epoch: 59.42 [D loss: 1.360794, acc: 0.00%] [G loss: 0.326978]\n",
            "Epoch: 59.43 [D loss: 1.336470, acc: 0.00%] [G loss: 0.327690]\n",
            "Epoch: 59.44 [D loss: 1.415864, acc: 0.00%] [G loss: 0.327841]\n",
            "Epoch: 59.45 [D loss: 1.367005, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 59.46 [D loss: 1.363229, acc: 0.00%] [G loss: 0.329115]\n",
            "Epoch: 59.47 [D loss: 1.346420, acc: 0.00%] [G loss: 0.327878]\n",
            "Epoch: 59.48 [D loss: 1.363314, acc: 0.00%] [G loss: 0.329392]\n",
            "Epoch: 59.49 [D loss: 1.318064, acc: 0.00%] [G loss: 0.327286]\n",
            "Epoch: 59.50 [D loss: 1.400936, acc: 0.00%] [G loss: 0.329775]\n",
            "Epoch: 59.51 [D loss: 1.362415, acc: 0.00%] [G loss: 0.331977]\n",
            "Epoch: 59.52 [D loss: 1.361219, acc: 0.00%] [G loss: 0.328470]\n",
            "Epoch: 59.53 [D loss: 1.454064, acc: 0.00%] [G loss: 0.328387]\n",
            "Epoch: 59.54 [D loss: 1.354105, acc: 0.00%] [G loss: 0.329881]\n",
            "Epoch: 59.55 [D loss: 1.416469, acc: 0.00%] [G loss: 0.327902]\n",
            "Epoch: 59.56 [D loss: 1.447901, acc: 0.00%] [G loss: 0.328800]\n",
            "Epoch: 59.57 [D loss: 1.402125, acc: 0.00%] [G loss: 0.329435]\n",
            "Epoch: 59.58 [D loss: 1.313048, acc: 0.00%] [G loss: 0.328762]\n",
            "Epoch: 59.59 [D loss: 1.347241, acc: 0.00%] [G loss: 0.327496]\n",
            "Epoch: 59.60 [D loss: 1.312815, acc: 0.00%] [G loss: 0.329153]\n",
            "Epoch: 59.61 [D loss: 1.333487, acc: 0.00%] [G loss: 0.328964]\n",
            "Epoch: 59.62 [D loss: 1.365782, acc: 0.00%] [G loss: 0.328704]\n",
            "Epoch: 59.63 [D loss: 1.372332, acc: 0.00%] [G loss: 0.330433]\n",
            "Epoch: 59.64 [D loss: 1.385040, acc: 0.00%] [G loss: 0.327170]\n",
            "Epoch: 59.65 [D loss: 1.370381, acc: 0.00%] [G loss: 0.329281]\n",
            "Epoch: 59.66 [D loss: 1.332545, acc: 0.00%] [G loss: 0.328038]\n",
            "Epoch: 59.67 [D loss: 1.336628, acc: 0.00%] [G loss: 0.330872]\n",
            "Epoch: 59.68 [D loss: 1.298790, acc: 0.00%] [G loss: 0.327629]\n",
            "Epoch: 59.69 [D loss: 1.327087, acc: 0.00%] [G loss: 0.327646]\n",
            "Epoch: 59.70 [D loss: 1.318498, acc: 0.00%] [G loss: 0.327120]\n",
            "Epoch: 59.71 [D loss: 1.398888, acc: 0.00%] [G loss: 0.327701]\n",
            "Epoch: 59.72 [D loss: 1.372073, acc: 0.00%] [G loss: 0.328417]\n",
            "Epoch: 59.73 [D loss: 1.345682, acc: 0.00%] [G loss: 0.329607]\n",
            "Epoch: 59.74 [D loss: 1.410841, acc: 0.00%] [G loss: 0.328686]\n",
            "Epoch: 59.75 [D loss: 1.404784, acc: 0.00%] [G loss: 0.329697]\n",
            "Epoch: 59.76 [D loss: 1.412582, acc: 0.00%] [G loss: 0.328587]\n",
            "Epoch: 59.77 [D loss: 1.392813, acc: 0.00%] [G loss: 0.328859]\n",
            "Epoch: 59.78 [D loss: 1.316843, acc: 0.00%] [G loss: 0.328368]\n",
            "Epoch: 59.79 [D loss: 1.326301, acc: 0.00%] [G loss: 0.327832]\n",
            "Epoch: 59.80 [D loss: 1.281094, acc: 0.00%] [G loss: 0.328728]\n",
            "Epoch: 59.81 [D loss: 1.314976, acc: 0.00%] [G loss: 0.329788]\n",
            "Epoch: 59.82 [D loss: 1.371464, acc: 0.00%] [G loss: 0.328448]\n",
            "Epoch: 59.83 [D loss: 1.378155, acc: 0.00%] [G loss: 0.327476]\n",
            "Epoch: 59.84 [D loss: 1.420520, acc: 0.00%] [G loss: 0.330270]\n",
            "Epoch: 59.85 [D loss: 1.427099, acc: 0.00%] [G loss: 0.328672]\n",
            "Epoch: 59.86 [D loss: 1.369966, acc: 0.00%] [G loss: 0.331056]\n",
            "Epoch: 59.87 [D loss: 1.383734, acc: 0.00%] [G loss: 0.330757]\n",
            "Epoch: 59.88 [D loss: 1.402287, acc: 0.00%] [G loss: 0.327242]\n",
            "Epoch: 59.89 [D loss: 1.330081, acc: 0.00%] [G loss: 0.328299]\n",
            "Epoch: 59.90 [D loss: 1.352124, acc: 0.00%] [G loss: 0.328093]\n",
            "Epoch: 59.91 [D loss: 1.323721, acc: 0.00%] [G loss: 0.329013]\n",
            "Epoch: 59.92 [D loss: 1.286229, acc: 0.00%] [G loss: 0.329362]\n",
            "Epoch: 59.93 [D loss: 1.304781, acc: 0.00%] [G loss: 0.329747]\n",
            "Epoch: 59.94 [D loss: 1.368081, acc: 0.00%] [G loss: 0.328770]\n",
            "Epoch: 59.95 [D loss: 1.371443, acc: 0.00%] [G loss: 0.328772]\n",
            "Epoch: 59.96 [D loss: 1.408660, acc: 0.00%] [G loss: 0.327744]\n",
            "Epoch: 59.97 [D loss: 1.437425, acc: 0.00%] [G loss: 0.328381]\n",
            "Epoch: 59.98 [D loss: 1.464347, acc: 0.00%] [G loss: 0.330008]\n",
            "Epoch: 59.99 [D loss: 1.357519, acc: 0.00%] [G loss: 0.329913]\n",
            "Epoch: 59.100 [D loss: 1.393244, acc: 0.00%] [G loss: 0.327760]\n",
            "Epoch: 59.101 [D loss: 1.377750, acc: 0.00%] [G loss: 0.328541]\n",
            "Epoch: 59.102 [D loss: 1.376524, acc: 0.00%] [G loss: 0.328272]\n",
            "Epoch: 59.103 [D loss: 1.320316, acc: 0.00%] [G loss: 0.330272]\n",
            "Epoch: 59.104 [D loss: 1.320774, acc: 0.00%] [G loss: 0.326803]\n",
            "Epoch: 59.105 [D loss: 1.331922, acc: 0.00%] [G loss: 0.328294]\n",
            "Epoch: 59.106 [D loss: 1.376005, acc: 0.00%] [G loss: 0.327582]\n",
            "Epoch: 59.107 [D loss: 1.346447, acc: 0.00%] [G loss: 0.328407]\n",
            "Epoch: 60.0 [D loss: 1.359033, acc: 0.00%] [G loss: 0.328725]\n",
            "Epoch: 60.1 [D loss: 1.350826, acc: 0.00%] [G loss: 0.327738]\n",
            "Epoch: 60.2 [D loss: 1.371652, acc: 0.00%] [G loss: 0.328527]\n",
            "Epoch: 60.3 [D loss: 1.436779, acc: 0.00%] [G loss: 0.329177]\n",
            "Epoch: 60.4 [D loss: 1.449571, acc: 0.00%] [G loss: 0.328635]\n",
            "Epoch: 60.5 [D loss: 1.344473, acc: 0.00%] [G loss: 0.330490]\n",
            "Epoch: 60.6 [D loss: 1.361794, acc: 0.00%] [G loss: 0.329201]\n",
            "Epoch: 60.7 [D loss: 1.353096, acc: 0.00%] [G loss: 0.329528]\n",
            "Epoch: 60.8 [D loss: 1.347461, acc: 0.00%] [G loss: 0.328760]\n",
            "Epoch: 60.9 [D loss: 1.342280, acc: 0.00%] [G loss: 0.328965]\n",
            "Epoch: 60.10 [D loss: 1.339146, acc: 0.00%] [G loss: 0.328833]\n",
            "Epoch: 60.11 [D loss: 1.373065, acc: 0.00%] [G loss: 0.329514]\n",
            "Epoch: 60.12 [D loss: 1.374916, acc: 0.00%] [G loss: 0.328456]\n",
            "Epoch: 60.13 [D loss: 1.395629, acc: 0.00%] [G loss: 0.328589]\n",
            "Epoch: 60.14 [D loss: 1.505168, acc: 0.00%] [G loss: 0.330969]\n",
            "Epoch: 60.15 [D loss: 1.363256, acc: 0.00%] [G loss: 0.330378]\n",
            "Epoch: 60.16 [D loss: 1.420749, acc: 0.00%] [G loss: 0.327000]\n",
            "Epoch: 60.17 [D loss: 1.340053, acc: 0.00%] [G loss: 0.328712]\n",
            "Epoch: 60.18 [D loss: 1.262397, acc: 0.00%] [G loss: 0.327370]\n",
            "Epoch: 60.19 [D loss: 1.333950, acc: 0.00%] [G loss: 0.329195]\n",
            "Epoch: 60.20 [D loss: 1.313241, acc: 0.00%] [G loss: 0.328389]\n",
            "Epoch: 60.21 [D loss: 1.285804, acc: 0.00%] [G loss: 0.330546]\n",
            "Epoch: 60.22 [D loss: 1.333018, acc: 0.00%] [G loss: 0.329620]\n",
            "Epoch: 60.23 [D loss: 1.314543, acc: 0.00%] [G loss: 0.329087]\n",
            "Epoch: 60.24 [D loss: 1.364446, acc: 0.00%] [G loss: 0.329435]\n",
            "Epoch: 60.25 [D loss: 1.449018, acc: 0.00%] [G loss: 0.328631]\n",
            "Epoch: 60.26 [D loss: 1.400434, acc: 0.00%] [G loss: 0.330539]\n",
            "Epoch: 60.27 [D loss: 1.452255, acc: 0.00%] [G loss: 0.328727]\n",
            "Epoch: 60.28 [D loss: 1.455503, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 60.29 [D loss: 1.384763, acc: 0.00%] [G loss: 0.327987]\n",
            "Epoch: 60.30 [D loss: 1.380554, acc: 0.00%] [G loss: 0.327607]\n",
            "Epoch: 60.31 [D loss: 1.359656, acc: 0.00%] [G loss: 0.328690]\n",
            "Epoch: 60.32 [D loss: 1.310543, acc: 0.00%] [G loss: 0.328271]\n",
            "Epoch: 60.33 [D loss: 1.347076, acc: 0.00%] [G loss: 0.329159]\n",
            "Epoch: 60.34 [D loss: 1.375085, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 60.35 [D loss: 1.391141, acc: 0.00%] [G loss: 0.328249]\n",
            "Epoch: 60.36 [D loss: 1.351260, acc: 0.00%] [G loss: 0.328809]\n",
            "Epoch: 60.37 [D loss: 1.349324, acc: 0.00%] [G loss: 0.328669]\n",
            "Epoch: 60.38 [D loss: 1.377389, acc: 0.00%] [G loss: 0.327600]\n",
            "Epoch: 60.39 [D loss: 1.358172, acc: 0.00%] [G loss: 0.328408]\n",
            "Epoch: 60.40 [D loss: 1.346384, acc: 0.00%] [G loss: 0.327495]\n",
            "Epoch: 60.41 [D loss: 1.359627, acc: 0.00%] [G loss: 0.327455]\n",
            "Epoch: 60.42 [D loss: 1.295980, acc: 0.00%] [G loss: 0.328802]\n",
            "Epoch: 60.43 [D loss: 1.315806, acc: 0.00%] [G loss: 0.328238]\n",
            "Epoch: 60.44 [D loss: 1.346001, acc: 0.00%] [G loss: 0.325790]\n",
            "Epoch: 60.45 [D loss: 1.379043, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 60.46 [D loss: 1.374694, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 60.47 [D loss: 1.324134, acc: 0.00%] [G loss: 0.327586]\n",
            "Epoch: 60.48 [D loss: 1.378726, acc: 0.00%] [G loss: 0.328782]\n",
            "Epoch: 60.49 [D loss: 1.382350, acc: 0.00%] [G loss: 0.327559]\n",
            "Epoch: 60.50 [D loss: 1.366159, acc: 0.00%] [G loss: 0.328731]\n",
            "Epoch: 60.51 [D loss: 1.342816, acc: 0.00%] [G loss: 0.327395]\n",
            "Epoch: 60.52 [D loss: 1.387412, acc: 0.00%] [G loss: 0.328932]\n",
            "Epoch: 60.53 [D loss: 1.341374, acc: 0.00%] [G loss: 0.328430]\n",
            "Epoch: 60.54 [D loss: 1.375156, acc: 0.00%] [G loss: 0.329431]\n",
            "Epoch: 60.55 [D loss: 1.378564, acc: 0.00%] [G loss: 0.330028]\n",
            "Epoch: 60.56 [D loss: 1.394467, acc: 0.00%] [G loss: 0.328181]\n",
            "Epoch: 60.57 [D loss: 1.434401, acc: 0.00%] [G loss: 0.328275]\n",
            "Epoch: 60.58 [D loss: 1.375667, acc: 0.00%] [G loss: 0.331769]\n",
            "Epoch: 60.59 [D loss: 1.370318, acc: 0.00%] [G loss: 0.327129]\n",
            "Epoch: 60.60 [D loss: 1.357077, acc: 0.00%] [G loss: 0.329793]\n",
            "Epoch: 60.61 [D loss: 1.358943, acc: 0.00%] [G loss: 0.331397]\n",
            "Epoch: 60.62 [D loss: 1.360890, acc: 0.00%] [G loss: 0.326736]\n",
            "Epoch: 60.63 [D loss: 1.410467, acc: 0.00%] [G loss: 0.327397]\n",
            "Epoch: 60.64 [D loss: 1.375642, acc: 0.00%] [G loss: 0.328362]\n",
            "Epoch: 60.65 [D loss: 1.352873, acc: 0.00%] [G loss: 0.328355]\n",
            "Epoch: 60.66 [D loss: 1.335299, acc: 0.00%] [G loss: 0.327849]\n",
            "Epoch: 60.67 [D loss: 1.356095, acc: 0.00%] [G loss: 0.327582]\n",
            "Epoch: 60.68 [D loss: 1.423849, acc: 0.00%] [G loss: 0.327954]\n",
            "Epoch: 60.69 [D loss: 1.322662, acc: 0.00%] [G loss: 0.327207]\n",
            "Epoch: 60.70 [D loss: 1.385236, acc: 0.00%] [G loss: 0.329155]\n",
            "Epoch: 60.71 [D loss: 1.397796, acc: 0.00%] [G loss: 0.328735]\n",
            "Epoch: 60.72 [D loss: 1.326590, acc: 0.00%] [G loss: 0.329080]\n",
            "Epoch: 60.73 [D loss: 1.365765, acc: 0.00%] [G loss: 0.327646]\n",
            "Epoch: 60.74 [D loss: 1.352304, acc: 0.00%] [G loss: 0.328965]\n",
            "Epoch: 60.75 [D loss: 1.391398, acc: 0.00%] [G loss: 0.327813]\n",
            "Epoch: 60.76 [D loss: 1.437574, acc: 0.00%] [G loss: 0.326922]\n",
            "Epoch: 60.77 [D loss: 1.368279, acc: 0.00%] [G loss: 0.328198]\n",
            "Epoch: 60.78 [D loss: 1.412471, acc: 0.00%] [G loss: 0.328776]\n",
            "Epoch: 60.79 [D loss: 1.424359, acc: 0.00%] [G loss: 0.327797]\n",
            "Epoch: 60.80 [D loss: 1.341728, acc: 0.00%] [G loss: 0.329152]\n",
            "Epoch: 60.81 [D loss: 1.359690, acc: 0.00%] [G loss: 0.329536]\n",
            "Epoch: 60.82 [D loss: 1.362091, acc: 0.00%] [G loss: 0.327422]\n",
            "Epoch: 60.83 [D loss: 1.358817, acc: 0.00%] [G loss: 0.327804]\n",
            "Epoch: 60.84 [D loss: 1.324146, acc: 0.00%] [G loss: 0.328262]\n",
            "Epoch: 60.85 [D loss: 1.313073, acc: 0.00%] [G loss: 0.328517]\n",
            "Epoch: 60.86 [D loss: 1.273439, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 60.87 [D loss: 1.373895, acc: 0.00%] [G loss: 0.327873]\n",
            "Epoch: 60.88 [D loss: 1.346181, acc: 0.00%] [G loss: 0.327382]\n",
            "Epoch: 60.89 [D loss: 1.321230, acc: 0.00%] [G loss: 0.328768]\n",
            "Epoch: 60.90 [D loss: 1.349549, acc: 0.00%] [G loss: 0.327404]\n",
            "Epoch: 60.91 [D loss: 1.403526, acc: 0.00%] [G loss: 0.328699]\n",
            "Epoch: 60.92 [D loss: 1.411129, acc: 0.00%] [G loss: 0.329182]\n",
            "Epoch: 60.93 [D loss: 1.393812, acc: 0.00%] [G loss: 0.327895]\n",
            "Epoch: 60.94 [D loss: 1.375336, acc: 0.00%] [G loss: 0.330387]\n",
            "Epoch: 60.95 [D loss: 1.369073, acc: 0.00%] [G loss: 0.328513]\n",
            "Epoch: 60.96 [D loss: 1.342819, acc: 0.00%] [G loss: 0.328416]\n",
            "Epoch: 60.97 [D loss: 1.363205, acc: 0.00%] [G loss: 0.329402]\n",
            "Epoch: 60.98 [D loss: 1.357449, acc: 0.00%] [G loss: 0.328185]\n",
            "Epoch: 60.99 [D loss: 1.326573, acc: 0.00%] [G loss: 0.327104]\n",
            "Epoch: 60.100 [D loss: 1.357656, acc: 0.00%] [G loss: 0.328317]\n",
            "Epoch: 60.101 [D loss: 1.365082, acc: 0.00%] [G loss: 0.328633]\n",
            "Epoch: 60.102 [D loss: 1.379440, acc: 0.00%] [G loss: 0.327064]\n",
            "Epoch: 60.103 [D loss: 1.306571, acc: 0.00%] [G loss: 0.328646]\n",
            "Epoch: 60.104 [D loss: 1.348471, acc: 0.00%] [G loss: 0.326836]\n",
            "Epoch: 60.105 [D loss: 1.359859, acc: 0.00%] [G loss: 0.328492]\n",
            "Epoch: 60.106 [D loss: 1.387766, acc: 0.00%] [G loss: 0.329382]\n",
            "Epoch: 60.107 [D loss: 1.389634, acc: 0.00%] [G loss: 0.327214]\n",
            "INFO:tensorflow:Assets written to: GAN_weights/dis_0.00000320_weights/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: GAN_weights/gen_0.00000320_weights/assets\n",
            "Epoch: 61.0 [D loss: 1.452196, acc: 0.00%] [G loss: 0.328155]\n",
            "Epoch: 61.1 [D loss: 1.396414, acc: 0.00%] [G loss: 0.326976]\n",
            "Epoch: 61.2 [D loss: 1.402768, acc: 0.00%] [G loss: 0.329883]\n",
            "Epoch: 61.3 [D loss: 1.431519, acc: 0.00%] [G loss: 0.329278]\n",
            "Epoch: 61.4 [D loss: 1.438926, acc: 0.00%] [G loss: 0.328861]\n",
            "Epoch: 61.5 [D loss: 1.430679, acc: 0.00%] [G loss: 0.328741]\n",
            "Epoch: 61.6 [D loss: 1.389703, acc: 0.00%] [G loss: 0.329572]\n",
            "Epoch: 61.7 [D loss: 1.387430, acc: 0.00%] [G loss: 0.328578]\n",
            "Epoch: 61.8 [D loss: 1.324905, acc: 0.00%] [G loss: 0.327644]\n",
            "Epoch: 61.9 [D loss: 1.366858, acc: 0.00%] [G loss: 0.330054]\n",
            "Epoch: 61.10 [D loss: 1.379339, acc: 0.00%] [G loss: 0.330003]\n",
            "Epoch: 61.11 [D loss: 1.326724, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 61.12 [D loss: 1.343210, acc: 0.00%] [G loss: 0.326828]\n",
            "Epoch: 61.13 [D loss: 1.351614, acc: 0.00%] [G loss: 0.329415]\n",
            "Epoch: 61.14 [D loss: 1.357499, acc: 0.00%] [G loss: 0.328461]\n",
            "Epoch: 61.15 [D loss: 1.374235, acc: 0.00%] [G loss: 0.327563]\n",
            "Epoch: 61.16 [D loss: 1.381872, acc: 0.00%] [G loss: 0.328487]\n",
            "Epoch: 61.17 [D loss: 1.373207, acc: 0.00%] [G loss: 0.328133]\n",
            "Epoch: 61.18 [D loss: 1.374541, acc: 0.00%] [G loss: 0.329263]\n",
            "Epoch: 61.19 [D loss: 1.357707, acc: 0.00%] [G loss: 0.328738]\n",
            "Epoch: 61.20 [D loss: 1.344216, acc: 0.00%] [G loss: 0.328620]\n",
            "Epoch: 61.21 [D loss: 1.314352, acc: 0.00%] [G loss: 0.327315]\n",
            "Epoch: 61.22 [D loss: 1.310899, acc: 0.00%] [G loss: 0.327235]\n",
            "Epoch: 61.23 [D loss: 1.380017, acc: 0.00%] [G loss: 0.328803]\n",
            "Epoch: 61.24 [D loss: 1.369677, acc: 0.00%] [G loss: 0.329724]\n",
            "Epoch: 61.25 [D loss: 1.345249, acc: 0.00%] [G loss: 0.328301]\n",
            "Epoch: 61.26 [D loss: 1.370089, acc: 0.00%] [G loss: 0.327372]\n",
            "Epoch: 61.27 [D loss: 1.422310, acc: 0.00%] [G loss: 0.328797]\n",
            "Epoch: 61.28 [D loss: 1.431813, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 61.29 [D loss: 1.358507, acc: 0.00%] [G loss: 0.327246]\n",
            "Epoch: 61.30 [D loss: 1.390100, acc: 0.00%] [G loss: 0.328037]\n",
            "Epoch: 61.31 [D loss: 1.404624, acc: 0.00%] [G loss: 0.329806]\n",
            "Epoch: 61.32 [D loss: 1.402614, acc: 0.00%] [G loss: 0.328263]\n",
            "Epoch: 61.33 [D loss: 1.357311, acc: 0.00%] [G loss: 0.328818]\n",
            "Epoch: 61.34 [D loss: 1.391027, acc: 0.00%] [G loss: 0.330440]\n",
            "Epoch: 61.35 [D loss: 1.303973, acc: 0.00%] [G loss: 0.330913]\n",
            "Epoch: 61.36 [D loss: 1.323690, acc: 0.00%] [G loss: 0.330167]\n",
            "Epoch: 61.37 [D loss: 1.390849, acc: 0.00%] [G loss: 0.327827]\n",
            "Epoch: 61.38 [D loss: 1.436584, acc: 0.00%] [G loss: 0.329119]\n",
            "Epoch: 61.39 [D loss: 1.364662, acc: 0.00%] [G loss: 0.328511]\n",
            "Epoch: 61.40 [D loss: 1.427447, acc: 0.00%] [G loss: 0.328420]\n",
            "Epoch: 61.41 [D loss: 1.395482, acc: 0.00%] [G loss: 0.329945]\n",
            "Epoch: 61.42 [D loss: 1.377419, acc: 0.00%] [G loss: 0.327882]\n",
            "Epoch: 61.43 [D loss: 1.369458, acc: 0.00%] [G loss: 0.328001]\n",
            "Epoch: 61.44 [D loss: 1.337309, acc: 0.00%] [G loss: 0.332516]\n",
            "Epoch: 61.45 [D loss: 1.423213, acc: 0.00%] [G loss: 0.327509]\n",
            "Epoch: 61.46 [D loss: 1.369303, acc: 0.00%] [G loss: 0.329058]\n",
            "Epoch: 61.47 [D loss: 1.363856, acc: 0.00%] [G loss: 0.330092]\n",
            "Epoch: 61.48 [D loss: 1.328854, acc: 0.00%] [G loss: 0.326923]\n",
            "Epoch: 61.49 [D loss: 1.343388, acc: 0.00%] [G loss: 0.327273]\n",
            "Epoch: 61.50 [D loss: 1.360928, acc: 0.00%] [G loss: 0.328807]\n",
            "Epoch: 61.51 [D loss: 1.296367, acc: 0.00%] [G loss: 0.329347]\n",
            "Epoch: 61.52 [D loss: 1.299223, acc: 0.00%] [G loss: 0.328921]\n",
            "Epoch: 61.53 [D loss: 1.395375, acc: 0.00%] [G loss: 0.328484]\n",
            "Epoch: 61.54 [D loss: 1.393172, acc: 0.00%] [G loss: 0.327213]\n",
            "Epoch: 61.55 [D loss: 1.391810, acc: 0.00%] [G loss: 0.327377]\n",
            "Epoch: 61.56 [D loss: 1.406884, acc: 0.00%] [G loss: 0.328221]\n",
            "Epoch: 61.57 [D loss: 1.384058, acc: 0.00%] [G loss: 0.327041]\n",
            "Epoch: 61.58 [D loss: 1.393579, acc: 0.00%] [G loss: 0.327726]\n",
            "Epoch: 61.59 [D loss: 1.367075, acc: 0.00%] [G loss: 0.327527]\n",
            "Epoch: 61.60 [D loss: 1.345356, acc: 0.00%] [G loss: 0.328352]\n",
            "Epoch: 61.61 [D loss: 1.341163, acc: 0.00%] [G loss: 0.327710]\n",
            "Epoch: 61.62 [D loss: 1.285710, acc: 0.00%] [G loss: 0.328239]\n",
            "Epoch: 61.63 [D loss: 1.382370, acc: 0.00%] [G loss: 0.330121]\n",
            "Epoch: 61.64 [D loss: 1.315223, acc: 0.00%] [G loss: 0.327707]\n",
            "Epoch: 61.65 [D loss: 1.418486, acc: 0.00%] [G loss: 0.329943]\n",
            "Epoch: 61.66 [D loss: 1.328524, acc: 0.00%] [G loss: 0.331077]\n",
            "Epoch: 61.67 [D loss: 1.422364, acc: 0.00%] [G loss: 0.328686]\n",
            "Epoch: 61.68 [D loss: 1.423919, acc: 0.00%] [G loss: 0.327751]\n",
            "Epoch: 61.69 [D loss: 1.393745, acc: 0.00%] [G loss: 0.330912]\n",
            "Epoch: 61.70 [D loss: 1.480695, acc: 0.00%] [G loss: 0.330650]\n",
            "Epoch: 61.71 [D loss: 1.447544, acc: 0.00%] [G loss: 0.328844]\n",
            "Epoch: 61.72 [D loss: 1.377800, acc: 0.00%] [G loss: 0.328697]\n",
            "Epoch: 61.73 [D loss: 1.350953, acc: 0.00%] [G loss: 0.327538]\n",
            "Epoch: 61.74 [D loss: 1.317397, acc: 0.00%] [G loss: 0.328189]\n",
            "Epoch: 61.75 [D loss: 1.296505, acc: 0.00%] [G loss: 0.330611]\n",
            "Epoch: 61.76 [D loss: 1.234897, acc: 0.00%] [G loss: 0.328691]\n",
            "Epoch: 61.77 [D loss: 1.342398, acc: 0.00%] [G loss: 0.330102]\n",
            "Epoch: 61.78 [D loss: 1.336220, acc: 0.00%] [G loss: 0.330448]\n",
            "Epoch: 61.79 [D loss: 1.382971, acc: 0.00%] [G loss: 0.327266]\n",
            "Epoch: 61.80 [D loss: 1.397892, acc: 0.00%] [G loss: 0.329532]\n",
            "Epoch: 61.81 [D loss: 1.404335, acc: 0.00%] [G loss: 0.328494]\n",
            "Epoch: 61.82 [D loss: 1.412625, acc: 0.00%] [G loss: 0.329692]\n",
            "Epoch: 61.83 [D loss: 1.315949, acc: 0.00%] [G loss: 0.327060]\n",
            "Epoch: 61.84 [D loss: 1.320182, acc: 0.00%] [G loss: 0.327737]\n",
            "Epoch: 61.85 [D loss: 1.335335, acc: 0.00%] [G loss: 0.330679]\n",
            "Epoch: 61.86 [D loss: 1.323552, acc: 0.00%] [G loss: 0.327635]\n",
            "Epoch: 61.87 [D loss: 1.311500, acc: 0.00%] [G loss: 0.330751]\n",
            "Epoch: 61.88 [D loss: 1.411621, acc: 0.00%] [G loss: 0.327089]\n",
            "Epoch: 61.89 [D loss: 1.334979, acc: 0.00%] [G loss: 0.329192]\n",
            "Epoch: 61.90 [D loss: 1.366923, acc: 0.00%] [G loss: 0.328068]\n",
            "Epoch: 61.91 [D loss: 1.323402, acc: 0.00%] [G loss: 0.327284]\n",
            "Epoch: 61.92 [D loss: 1.423617, acc: 0.00%] [G loss: 0.327655]\n",
            "Epoch: 61.93 [D loss: 1.348731, acc: 0.00%] [G loss: 0.329086]\n",
            "Epoch: 61.94 [D loss: 1.355388, acc: 0.00%] [G loss: 0.330233]\n",
            "Epoch: 61.95 [D loss: 1.375392, acc: 0.00%] [G loss: 0.327253]\n",
            "Epoch: 61.96 [D loss: 1.352129, acc: 0.00%] [G loss: 0.330445]\n",
            "Epoch: 61.97 [D loss: 1.379679, acc: 0.00%] [G loss: 0.329140]\n",
            "Epoch: 61.98 [D loss: 1.346825, acc: 0.00%] [G loss: 0.329822]\n",
            "Epoch: 61.99 [D loss: 1.397015, acc: 0.00%] [G loss: 0.327674]\n",
            "Epoch: 61.100 [D loss: 1.376415, acc: 0.00%] [G loss: 0.327634]\n",
            "Epoch: 61.101 [D loss: 1.378840, acc: 0.00%] [G loss: 0.328574]\n",
            "Epoch: 61.102 [D loss: 1.419524, acc: 0.00%] [G loss: 0.328445]\n",
            "Epoch: 61.103 [D loss: 1.414040, acc: 0.00%] [G loss: 0.327972]\n",
            "Epoch: 61.104 [D loss: 1.337077, acc: 0.00%] [G loss: 0.327459]\n",
            "Epoch: 61.105 [D loss: 1.307146, acc: 0.00%] [G loss: 0.330621]\n",
            "Epoch: 61.106 [D loss: 1.376882, acc: 0.00%] [G loss: 0.327959]\n",
            "Epoch: 61.107 [D loss: 1.306409, acc: 0.00%] [G loss: 0.327963]\n",
            "Epoch: 62.0 [D loss: 1.322405, acc: 0.00%] [G loss: 0.329118]\n",
            "Epoch: 62.1 [D loss: 1.308249, acc: 0.00%] [G loss: 0.326600]\n",
            "Epoch: 62.2 [D loss: 1.382109, acc: 0.00%] [G loss: 0.327318]\n",
            "Epoch: 62.3 [D loss: 1.423516, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 62.4 [D loss: 1.369201, acc: 0.00%] [G loss: 0.327609]\n",
            "Epoch: 62.5 [D loss: 1.381561, acc: 0.00%] [G loss: 0.329380]\n",
            "Epoch: 62.6 [D loss: 1.461148, acc: 0.00%] [G loss: 0.329865]\n",
            "Epoch: 62.7 [D loss: 1.412943, acc: 0.00%] [G loss: 0.328853]\n",
            "Epoch: 62.8 [D loss: 1.415379, acc: 0.00%] [G loss: 0.329661]\n",
            "Epoch: 62.9 [D loss: 1.403154, acc: 0.00%] [G loss: 0.329075]\n",
            "Epoch: 62.10 [D loss: 1.408157, acc: 0.00%] [G loss: 0.329391]\n",
            "Epoch: 62.11 [D loss: 1.391442, acc: 0.00%] [G loss: 0.328717]\n",
            "Epoch: 62.12 [D loss: 1.312660, acc: 0.00%] [G loss: 0.327745]\n",
            "Epoch: 62.13 [D loss: 1.366695, acc: 0.00%] [G loss: 0.327157]\n",
            "Epoch: 62.14 [D loss: 1.281933, acc: 0.00%] [G loss: 0.330294]\n",
            "Epoch: 62.15 [D loss: 1.274758, acc: 0.00%] [G loss: 0.330050]\n",
            "Epoch: 62.16 [D loss: 1.297947, acc: 0.00%] [G loss: 0.328632]\n",
            "Epoch: 62.17 [D loss: 1.375782, acc: 0.00%] [G loss: 0.329059]\n",
            "Epoch: 62.18 [D loss: 1.376536, acc: 0.00%] [G loss: 0.328915]\n",
            "Epoch: 62.19 [D loss: 1.410912, acc: 0.00%] [G loss: 0.330019]\n",
            "Epoch: 62.20 [D loss: 1.376515, acc: 0.00%] [G loss: 0.328679]\n",
            "Epoch: 62.21 [D loss: 1.454261, acc: 0.00%] [G loss: 0.329535]\n",
            "Epoch: 62.22 [D loss: 1.436714, acc: 0.00%] [G loss: 0.328567]\n",
            "Epoch: 62.23 [D loss: 1.323968, acc: 0.00%] [G loss: 0.327974]\n",
            "Epoch: 62.24 [D loss: 1.409608, acc: 0.00%] [G loss: 0.329998]\n",
            "Epoch: 62.25 [D loss: 1.340335, acc: 0.00%] [G loss: 0.327835]\n",
            "Epoch: 62.26 [D loss: 1.329027, acc: 0.00%] [G loss: 0.326537]\n",
            "Epoch: 62.27 [D loss: 1.297685, acc: 0.00%] [G loss: 0.328121]\n",
            "Epoch: 62.28 [D loss: 1.333543, acc: 0.00%] [G loss: 0.328687]\n",
            "Epoch: 62.29 [D loss: 1.288205, acc: 0.00%] [G loss: 0.328952]\n",
            "Epoch: 62.30 [D loss: 1.335502, acc: 0.00%] [G loss: 0.329113]\n",
            "Epoch: 62.31 [D loss: 1.316349, acc: 0.00%] [G loss: 0.330816]\n",
            "Epoch: 62.32 [D loss: 1.392616, acc: 0.00%] [G loss: 0.329288]\n",
            "Epoch: 62.33 [D loss: 1.414931, acc: 0.00%] [G loss: 0.329509]\n",
            "Epoch: 62.34 [D loss: 1.449391, acc: 0.00%] [G loss: 0.330003]\n",
            "Epoch: 62.35 [D loss: 1.392614, acc: 0.00%] [G loss: 0.329585]\n",
            "Epoch: 62.36 [D loss: 1.437645, acc: 0.00%] [G loss: 0.328473]\n",
            "Epoch: 62.37 [D loss: 1.452289, acc: 0.00%] [G loss: 0.328198]\n",
            "Epoch: 62.38 [D loss: 1.370902, acc: 0.00%] [G loss: 0.329112]\n",
            "Epoch: 62.39 [D loss: 1.356809, acc: 0.00%] [G loss: 0.328434]\n",
            "Epoch: 62.40 [D loss: 1.347395, acc: 0.00%] [G loss: 0.328840]\n",
            "Epoch: 62.41 [D loss: 1.324560, acc: 0.00%] [G loss: 0.327317]\n",
            "Epoch: 62.42 [D loss: 1.333805, acc: 0.00%] [G loss: 0.328797]\n",
            "Epoch: 62.43 [D loss: 1.314445, acc: 0.00%] [G loss: 0.326815]\n",
            "Epoch: 62.44 [D loss: 1.382655, acc: 0.00%] [G loss: 0.328680]\n",
            "Epoch: 62.45 [D loss: 1.347259, acc: 0.00%] [G loss: 0.329506]\n",
            "Epoch: 62.46 [D loss: 1.331089, acc: 0.00%] [G loss: 0.329404]\n",
            "Epoch: 62.47 [D loss: 1.358480, acc: 0.00%] [G loss: 0.330449]\n",
            "Epoch: 62.48 [D loss: 1.335489, acc: 0.00%] [G loss: 0.329663]\n",
            "Epoch: 62.49 [D loss: 1.386331, acc: 0.00%] [G loss: 0.327855]\n",
            "Epoch: 62.50 [D loss: 1.402791, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 62.51 [D loss: 1.399619, acc: 0.00%] [G loss: 0.327343]\n",
            "Epoch: 62.52 [D loss: 1.307123, acc: 0.00%] [G loss: 0.329656]\n",
            "Epoch: 62.53 [D loss: 1.363599, acc: 0.00%] [G loss: 0.329028]\n",
            "Epoch: 62.54 [D loss: 1.441630, acc: 0.00%] [G loss: 0.328412]\n",
            "Epoch: 62.55 [D loss: 1.368940, acc: 0.00%] [G loss: 0.329548]\n",
            "Epoch: 62.56 [D loss: 1.437558, acc: 0.00%] [G loss: 0.327383]\n",
            "Epoch: 62.57 [D loss: 1.368943, acc: 0.00%] [G loss: 0.327647]\n",
            "Epoch: 62.58 [D loss: 1.410412, acc: 0.00%] [G loss: 0.328670]\n",
            "Epoch: 62.59 [D loss: 1.363171, acc: 0.00%] [G loss: 0.328302]\n",
            "Epoch: 62.60 [D loss: 1.367902, acc: 0.00%] [G loss: 0.327947]\n",
            "Epoch: 62.61 [D loss: 1.336441, acc: 0.00%] [G loss: 0.329165]\n",
            "Epoch: 62.62 [D loss: 1.365067, acc: 0.00%] [G loss: 0.329164]\n",
            "Epoch: 62.63 [D loss: 1.258093, acc: 0.00%] [G loss: 0.329272]\n",
            "Epoch: 62.64 [D loss: 1.285402, acc: 0.00%] [G loss: 0.328689]\n",
            "Epoch: 62.65 [D loss: 1.351682, acc: 0.00%] [G loss: 0.327101]\n",
            "Epoch: 62.66 [D loss: 1.309235, acc: 0.00%] [G loss: 0.327724]\n",
            "Epoch: 62.67 [D loss: 1.334283, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 62.68 [D loss: 1.322510, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 62.69 [D loss: 1.360826, acc: 0.00%] [G loss: 0.327340]\n",
            "Epoch: 62.70 [D loss: 1.446647, acc: 0.00%] [G loss: 0.328448]\n",
            "Epoch: 62.71 [D loss: 1.374644, acc: 0.00%] [G loss: 0.327806]\n",
            "Epoch: 62.72 [D loss: 1.383190, acc: 0.00%] [G loss: 0.327173]\n",
            "Epoch: 62.73 [D loss: 1.376685, acc: 0.00%] [G loss: 0.328684]\n",
            "Epoch: 62.74 [D loss: 1.409944, acc: 0.00%] [G loss: 0.328607]\n",
            "Epoch: 62.75 [D loss: 1.410072, acc: 0.00%] [G loss: 0.327969]\n",
            "Epoch: 62.76 [D loss: 1.382797, acc: 0.00%] [G loss: 0.329249]\n",
            "Epoch: 62.77 [D loss: 1.375916, acc: 0.00%] [G loss: 0.327469]\n",
            "Epoch: 62.78 [D loss: 1.361682, acc: 0.00%] [G loss: 0.329473]\n",
            "Epoch: 62.79 [D loss: 1.355891, acc: 0.00%] [G loss: 0.327678]\n",
            "Epoch: 62.80 [D loss: 1.316685, acc: 0.00%] [G loss: 0.328533]\n",
            "Epoch: 62.81 [D loss: 1.396351, acc: 0.00%] [G loss: 0.327504]\n",
            "Epoch: 62.82 [D loss: 1.345005, acc: 0.00%] [G loss: 0.326568]\n",
            "Epoch: 62.83 [D loss: 1.403157, acc: 0.00%] [G loss: 0.329116]\n",
            "Epoch: 62.84 [D loss: 1.346654, acc: 0.00%] [G loss: 0.327177]\n",
            "Epoch: 62.85 [D loss: 1.365576, acc: 0.00%] [G loss: 0.327655]\n",
            "Epoch: 62.86 [D loss: 1.378331, acc: 0.00%] [G loss: 0.326801]\n",
            "Epoch: 62.87 [D loss: 1.334804, acc: 0.00%] [G loss: 0.328464]\n",
            "Epoch: 62.88 [D loss: 1.355246, acc: 0.00%] [G loss: 0.329199]\n",
            "Epoch: 62.89 [D loss: 1.359188, acc: 0.00%] [G loss: 0.327313]\n",
            "Epoch: 62.90 [D loss: 1.360964, acc: 0.00%] [G loss: 0.326238]\n",
            "Epoch: 62.91 [D loss: 1.371885, acc: 0.00%] [G loss: 0.330301]\n",
            "Epoch: 62.92 [D loss: 1.396877, acc: 0.00%] [G loss: 0.330748]\n",
            "Epoch: 62.93 [D loss: 1.425431, acc: 0.00%] [G loss: 0.327780]\n",
            "Epoch: 62.94 [D loss: 1.395405, acc: 0.00%] [G loss: 0.327767]\n",
            "Epoch: 62.95 [D loss: 1.416960, acc: 0.00%] [G loss: 0.328449]\n",
            "Epoch: 62.96 [D loss: 1.370354, acc: 0.00%] [G loss: 0.327626]\n",
            "Epoch: 62.97 [D loss: 1.372512, acc: 0.00%] [G loss: 0.330301]\n",
            "Epoch: 62.98 [D loss: 1.355612, acc: 0.00%] [G loss: 0.327836]\n",
            "Epoch: 62.99 [D loss: 1.354514, acc: 0.00%] [G loss: 0.328313]\n",
            "Epoch: 62.100 [D loss: 1.351104, acc: 0.00%] [G loss: 0.327348]\n",
            "Epoch: 62.101 [D loss: 1.344713, acc: 0.00%] [G loss: 0.327497]\n",
            "Epoch: 62.102 [D loss: 1.389328, acc: 0.00%] [G loss: 0.328041]\n",
            "Epoch: 62.103 [D loss: 1.366385, acc: 0.00%] [G loss: 0.327884]\n",
            "Epoch: 62.104 [D loss: 1.403383, acc: 0.00%] [G loss: 0.328014]\n",
            "Epoch: 62.105 [D loss: 1.476421, acc: 0.00%] [G loss: 0.328612]\n",
            "Epoch: 62.106 [D loss: 1.410389, acc: 0.00%] [G loss: 0.329391]\n",
            "Epoch: 62.107 [D loss: 1.429727, acc: 0.00%] [G loss: 0.329062]\n",
            "Epoch: 63.0 [D loss: 1.436542, acc: 0.00%] [G loss: 0.328053]\n",
            "Epoch: 63.1 [D loss: 1.407356, acc: 0.00%] [G loss: 0.328858]\n",
            "Epoch: 63.2 [D loss: 1.318649, acc: 0.00%] [G loss: 0.328288]\n",
            "Epoch: 63.3 [D loss: 1.350592, acc: 0.00%] [G loss: 0.327255]\n",
            "Epoch: 63.4 [D loss: 1.287739, acc: 0.00%] [G loss: 0.330346]\n",
            "Epoch: 63.5 [D loss: 1.323237, acc: 0.00%] [G loss: 0.331991]\n",
            "Epoch: 63.6 [D loss: 1.366292, acc: 0.00%] [G loss: 0.328167]\n",
            "Epoch: 63.7 [D loss: 1.323585, acc: 0.00%] [G loss: 0.327957]\n",
            "Epoch: 63.8 [D loss: 1.384388, acc: 0.00%] [G loss: 0.329038]\n",
            "Epoch: 63.9 [D loss: 1.367079, acc: 0.00%] [G loss: 0.328868]\n",
            "Epoch: 63.10 [D loss: 1.413739, acc: 0.00%] [G loss: 0.329219]\n",
            "Epoch: 63.11 [D loss: 1.473466, acc: 0.00%] [G loss: 0.330981]\n",
            "Epoch: 63.12 [D loss: 1.370980, acc: 0.00%] [G loss: 0.329504]\n",
            "Epoch: 63.13 [D loss: 1.314817, acc: 0.00%] [G loss: 0.327168]\n",
            "Epoch: 63.14 [D loss: 1.311611, acc: 0.00%] [G loss: 0.327585]\n",
            "Epoch: 63.15 [D loss: 1.370066, acc: 0.00%] [G loss: 0.328757]\n",
            "Epoch: 63.16 [D loss: 1.284864, acc: 0.00%] [G loss: 0.329733]\n",
            "Epoch: 63.17 [D loss: 1.266319, acc: 0.00%] [G loss: 0.329987]\n",
            "Epoch: 63.18 [D loss: 1.372653, acc: 0.00%] [G loss: 0.329141]\n",
            "Epoch: 63.19 [D loss: 1.305266, acc: 0.00%] [G loss: 0.328028]\n",
            "Epoch: 63.20 [D loss: 1.360140, acc: 0.00%] [G loss: 0.327300]\n",
            "Epoch: 63.21 [D loss: 1.408010, acc: 0.00%] [G loss: 0.328656]\n",
            "Epoch: 63.22 [D loss: 1.447444, acc: 0.00%] [G loss: 0.328677]\n",
            "Epoch: 63.23 [D loss: 1.426324, acc: 0.00%] [G loss: 0.329476]\n",
            "Epoch: 63.24 [D loss: 1.386404, acc: 0.00%] [G loss: 0.327601]\n",
            "Epoch: 63.25 [D loss: 1.403572, acc: 0.00%] [G loss: 0.328225]\n",
            "Epoch: 63.26 [D loss: 1.338634, acc: 0.00%] [G loss: 0.330214]\n",
            "Epoch: 63.27 [D loss: 1.328298, acc: 0.00%] [G loss: 0.328168]\n",
            "Epoch: 63.28 [D loss: 1.330905, acc: 0.00%] [G loss: 0.327907]\n",
            "Epoch: 63.29 [D loss: 1.321019, acc: 0.00%] [G loss: 0.328772]\n",
            "Epoch: 63.30 [D loss: 1.433168, acc: 0.00%] [G loss: 0.326816]\n",
            "Epoch: 63.31 [D loss: 1.420483, acc: 0.00%] [G loss: 0.328725]\n",
            "Epoch: 63.32 [D loss: 1.403411, acc: 0.00%] [G loss: 0.329462]\n",
            "Epoch: 63.33 [D loss: 1.442472, acc: 0.00%] [G loss: 0.329555]\n",
            "Epoch: 63.34 [D loss: 1.446588, acc: 0.00%] [G loss: 0.327993]\n",
            "Epoch: 63.35 [D loss: 1.336589, acc: 0.00%] [G loss: 0.328110]\n",
            "Epoch: 63.36 [D loss: 1.358402, acc: 0.00%] [G loss: 0.327816]\n",
            "Epoch: 63.37 [D loss: 1.317289, acc: 0.00%] [G loss: 0.329221]\n",
            "Epoch: 63.38 [D loss: 1.327987, acc: 0.00%] [G loss: 0.329534]\n",
            "Epoch: 63.39 [D loss: 1.344629, acc: 0.00%] [G loss: 0.327331]\n",
            "Epoch: 63.40 [D loss: 1.306872, acc: 0.00%] [G loss: 0.328062]\n",
            "Epoch: 63.41 [D loss: 1.275009, acc: 0.00%] [G loss: 0.328106]\n",
            "Epoch: 63.42 [D loss: 1.319243, acc: 0.00%] [G loss: 0.329438]\n",
            "Epoch: 63.43 [D loss: 1.267832, acc: 0.00%] [G loss: 0.328297]\n",
            "Epoch: 63.44 [D loss: 1.376606, acc: 0.00%] [G loss: 0.329830]\n",
            "Epoch: 63.45 [D loss: 1.400601, acc: 0.00%] [G loss: 0.327286]\n",
            "Epoch: 63.46 [D loss: 1.422366, acc: 0.00%] [G loss: 0.328075]\n",
            "Epoch: 63.47 [D loss: 1.437177, acc: 0.00%] [G loss: 0.327901]\n",
            "Epoch: 63.48 [D loss: 1.370147, acc: 0.00%] [G loss: 0.328409]\n",
            "Epoch: 63.49 [D loss: 1.360659, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 63.50 [D loss: 1.398411, acc: 0.00%] [G loss: 0.327537]\n",
            "Epoch: 63.51 [D loss: 1.391019, acc: 0.00%] [G loss: 0.327973]\n",
            "Epoch: 63.52 [D loss: 1.402611, acc: 0.00%] [G loss: 0.328408]\n",
            "Epoch: 63.53 [D loss: 1.354394, acc: 0.00%] [G loss: 0.329646]\n",
            "Epoch: 63.54 [D loss: 1.350868, acc: 0.00%] [G loss: 0.328046]\n",
            "Epoch: 63.55 [D loss: 1.348977, acc: 0.00%] [G loss: 0.328107]\n",
            "Epoch: 63.56 [D loss: 1.301867, acc: 0.00%] [G loss: 0.329333]\n",
            "Epoch: 63.57 [D loss: 1.305153, acc: 0.00%] [G loss: 0.326940]\n",
            "Epoch: 63.58 [D loss: 1.372561, acc: 0.00%] [G loss: 0.329203]\n",
            "Epoch: 63.59 [D loss: 1.328654, acc: 0.00%] [G loss: 0.328785]\n",
            "Epoch: 63.60 [D loss: 1.407871, acc: 0.00%] [G loss: 0.327210]\n",
            "Epoch: 63.61 [D loss: 1.393003, acc: 0.00%] [G loss: 0.328764]\n",
            "Epoch: 63.62 [D loss: 1.391789, acc: 0.00%] [G loss: 0.328272]\n",
            "Epoch: 63.63 [D loss: 1.333272, acc: 0.00%] [G loss: 0.328583]\n",
            "Epoch: 63.64 [D loss: 1.341869, acc: 0.00%] [G loss: 0.327753]\n",
            "Epoch: 63.65 [D loss: 1.362659, acc: 0.00%] [G loss: 0.328467]\n",
            "Epoch: 63.66 [D loss: 1.372542, acc: 0.00%] [G loss: 0.327850]\n",
            "Epoch: 63.67 [D loss: 1.354125, acc: 0.00%] [G loss: 0.327931]\n",
            "Epoch: 63.68 [D loss: 1.379846, acc: 0.00%] [G loss: 0.326660]\n",
            "Epoch: 63.69 [D loss: 1.357707, acc: 0.00%] [G loss: 0.327479]\n",
            "Epoch: 63.70 [D loss: 1.394751, acc: 0.00%] [G loss: 0.328292]\n",
            "Epoch: 63.71 [D loss: 1.323448, acc: 0.00%] [G loss: 0.329248]\n",
            "Epoch: 63.72 [D loss: 1.371600, acc: 0.00%] [G loss: 0.327669]\n",
            "Epoch: 63.73 [D loss: 1.378679, acc: 0.00%] [G loss: 0.328584]\n",
            "Epoch: 63.74 [D loss: 1.383369, acc: 0.00%] [G loss: 0.327215]\n",
            "Epoch: 63.75 [D loss: 1.363504, acc: 0.00%] [G loss: 0.328969]\n",
            "Epoch: 63.76 [D loss: 1.468641, acc: 0.00%] [G loss: 0.329528]\n",
            "Epoch: 63.77 [D loss: 1.345672, acc: 0.00%] [G loss: 0.328910]\n",
            "Epoch: 63.78 [D loss: 1.313520, acc: 0.00%] [G loss: 0.327949]\n",
            "Epoch: 63.79 [D loss: 1.303670, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 63.80 [D loss: 1.373129, acc: 0.00%] [G loss: 0.326610]\n",
            "Epoch: 63.81 [D loss: 1.319751, acc: 0.00%] [G loss: 0.327112]\n",
            "Epoch: 63.82 [D loss: 1.307424, acc: 0.00%] [G loss: 0.329045]\n",
            "Epoch: 63.83 [D loss: 1.315175, acc: 0.00%] [G loss: 0.327443]\n",
            "Epoch: 63.84 [D loss: 1.371153, acc: 0.00%] [G loss: 0.327960]\n",
            "Epoch: 63.85 [D loss: 1.380303, acc: 0.00%] [G loss: 0.327098]\n",
            "Epoch: 63.86 [D loss: 1.369941, acc: 0.00%] [G loss: 0.327556]\n",
            "Epoch: 63.87 [D loss: 1.337822, acc: 0.00%] [G loss: 0.328531]\n",
            "Epoch: 63.88 [D loss: 1.356907, acc: 0.00%] [G loss: 0.328766]\n",
            "Epoch: 63.89 [D loss: 1.341839, acc: 0.00%] [G loss: 0.327754]\n",
            "Epoch: 63.90 [D loss: 1.401676, acc: 0.00%] [G loss: 0.328184]\n",
            "Epoch: 63.91 [D loss: 1.380599, acc: 0.00%] [G loss: 0.328949]\n",
            "Epoch: 63.92 [D loss: 1.337199, acc: 0.00%] [G loss: 0.327766]\n",
            "Epoch: 63.93 [D loss: 1.378331, acc: 0.00%] [G loss: 0.328393]\n",
            "Epoch: 63.94 [D loss: 1.331787, acc: 0.00%] [G loss: 0.327420]\n",
            "Epoch: 63.95 [D loss: 1.356099, acc: 0.00%] [G loss: 0.329109]\n",
            "Epoch: 63.96 [D loss: 1.331153, acc: 0.00%] [G loss: 0.327967]\n",
            "Epoch: 63.97 [D loss: 1.392746, acc: 0.00%] [G loss: 0.327614]\n",
            "Epoch: 63.98 [D loss: 1.328520, acc: 0.00%] [G loss: 0.331406]\n",
            "Epoch: 63.99 [D loss: 1.376120, acc: 0.00%] [G loss: 0.327886]\n",
            "Epoch: 63.100 [D loss: 1.403940, acc: 0.00%] [G loss: 0.327487]\n",
            "Epoch: 63.101 [D loss: 1.393517, acc: 0.00%] [G loss: 0.329033]\n",
            "Epoch: 63.102 [D loss: 1.323871, acc: 0.00%] [G loss: 0.327245]\n",
            "Epoch: 63.103 [D loss: 1.311176, acc: 0.00%] [G loss: 0.327443]\n",
            "Epoch: 63.104 [D loss: 1.313195, acc: 0.00%] [G loss: 0.327849]\n",
            "Epoch: 63.105 [D loss: 1.302705, acc: 0.00%] [G loss: 0.328905]\n",
            "Epoch: 63.106 [D loss: 1.312302, acc: 0.00%] [G loss: 0.327242]\n",
            "Epoch: 63.107 [D loss: 1.349317, acc: 0.00%] [G loss: 0.328700]\n",
            "Epoch: 64.0 [D loss: 1.382645, acc: 0.00%] [G loss: 0.329131]\n",
            "Epoch: 64.1 [D loss: 1.408693, acc: 0.00%] [G loss: 0.328542]\n",
            "Epoch: 64.2 [D loss: 1.402416, acc: 0.00%] [G loss: 0.327684]\n",
            "Epoch: 64.3 [D loss: 1.401016, acc: 0.00%] [G loss: 0.329780]\n",
            "Epoch: 64.4 [D loss: 1.431503, acc: 0.00%] [G loss: 0.328209]\n",
            "Epoch: 64.5 [D loss: 1.387371, acc: 0.00%] [G loss: 0.327064]\n",
            "Epoch: 64.6 [D loss: 1.397781, acc: 0.00%] [G loss: 0.327191]\n",
            "Epoch: 64.7 [D loss: 1.322936, acc: 0.00%] [G loss: 0.329007]\n",
            "Epoch: 64.8 [D loss: 1.293721, acc: 0.00%] [G loss: 0.328031]\n",
            "Epoch: 64.9 [D loss: 1.351383, acc: 0.00%] [G loss: 0.328096]\n",
            "Epoch: 64.10 [D loss: 1.375085, acc: 0.00%] [G loss: 0.329657]\n",
            "Epoch: 64.11 [D loss: 1.356471, acc: 0.00%] [G loss: 0.329006]\n",
            "Epoch: 64.12 [D loss: 1.319277, acc: 0.00%] [G loss: 0.327195]\n",
            "Epoch: 64.13 [D loss: 1.352342, acc: 0.00%] [G loss: 0.328904]\n",
            "Epoch: 64.14 [D loss: 1.351115, acc: 0.00%] [G loss: 0.327656]\n",
            "Epoch: 64.15 [D loss: 1.361669, acc: 0.00%] [G loss: 0.327558]\n",
            "Epoch: 64.16 [D loss: 1.285133, acc: 0.00%] [G loss: 0.327643]\n",
            "Epoch: 64.17 [D loss: 1.341901, acc: 0.00%] [G loss: 0.327117]\n",
            "Epoch: 64.18 [D loss: 1.392813, acc: 0.00%] [G loss: 0.328351]\n",
            "Epoch: 64.19 [D loss: 1.374555, acc: 0.00%] [G loss: 0.328074]\n",
            "Epoch: 64.20 [D loss: 1.313134, acc: 0.00%] [G loss: 0.328762]\n",
            "Epoch: 64.21 [D loss: 1.344605, acc: 0.00%] [G loss: 0.329117]\n",
            "Epoch: 64.22 [D loss: 1.374853, acc: 0.00%] [G loss: 0.328860]\n",
            "Epoch: 64.23 [D loss: 1.387824, acc: 0.00%] [G loss: 0.328227]\n",
            "Epoch: 64.24 [D loss: 1.420422, acc: 0.00%] [G loss: 0.327460]\n",
            "Epoch: 64.25 [D loss: 1.385379, acc: 0.00%] [G loss: 0.327726]\n",
            "Epoch: 64.26 [D loss: 1.373980, acc: 0.00%] [G loss: 0.328636]\n",
            "Epoch: 64.27 [D loss: 1.394899, acc: 0.00%] [G loss: 0.327695]\n",
            "Epoch: 64.28 [D loss: 1.474219, acc: 0.00%] [G loss: 0.330168]\n",
            "Epoch: 64.29 [D loss: 1.382261, acc: 0.00%] [G loss: 0.328995]\n",
            "Epoch: 64.30 [D loss: 1.339033, acc: 0.00%] [G loss: 0.329386]\n",
            "Epoch: 64.31 [D loss: 1.403378, acc: 0.00%] [G loss: 0.328629]\n",
            "Epoch: 64.32 [D loss: 1.307172, acc: 0.00%] [G loss: 0.330080]\n",
            "Epoch: 64.33 [D loss: 1.321387, acc: 0.00%] [G loss: 0.327008]\n",
            "Epoch: 64.34 [D loss: 1.404689, acc: 0.00%] [G loss: 0.328750]\n",
            "Epoch: 64.35 [D loss: 1.379959, acc: 0.00%] [G loss: 0.328078]\n",
            "Epoch: 64.36 [D loss: 1.354676, acc: 0.00%] [G loss: 0.329352]\n",
            "Epoch: 64.37 [D loss: 1.428053, acc: 0.00%] [G loss: 0.328133]\n",
            "Epoch: 64.38 [D loss: 1.388873, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 64.39 [D loss: 1.408685, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 64.40 [D loss: 1.378816, acc: 0.00%] [G loss: 0.327666]\n",
            "Epoch: 64.41 [D loss: 1.330713, acc: 0.00%] [G loss: 0.328623]\n",
            "Epoch: 64.42 [D loss: 1.321584, acc: 0.00%] [G loss: 0.328072]\n",
            "Epoch: 64.43 [D loss: 1.363315, acc: 0.00%] [G loss: 0.327328]\n",
            "Epoch: 64.44 [D loss: 1.327890, acc: 0.00%] [G loss: 0.328015]\n",
            "Epoch: 64.45 [D loss: 1.320257, acc: 0.00%] [G loss: 0.328641]\n",
            "Epoch: 64.46 [D loss: 1.365935, acc: 0.00%] [G loss: 0.328288]\n",
            "Epoch: 64.47 [D loss: 1.323532, acc: 0.00%] [G loss: 0.327188]\n",
            "Epoch: 64.48 [D loss: 1.394874, acc: 0.00%] [G loss: 0.327824]\n",
            "Epoch: 64.49 [D loss: 1.324721, acc: 0.00%] [G loss: 0.328345]\n",
            "Epoch: 64.50 [D loss: 1.320596, acc: 0.00%] [G loss: 0.328702]\n",
            "Epoch: 64.51 [D loss: 1.311825, acc: 0.00%] [G loss: 0.327275]\n",
            "Epoch: 64.52 [D loss: 1.379174, acc: 0.00%] [G loss: 0.327249]\n",
            "Epoch: 64.53 [D loss: 1.364828, acc: 0.00%] [G loss: 0.327663]\n",
            "Epoch: 64.54 [D loss: 1.316459, acc: 0.00%] [G loss: 0.329051]\n",
            "Epoch: 64.55 [D loss: 1.375443, acc: 0.00%] [G loss: 0.327704]\n",
            "Epoch: 64.56 [D loss: 1.390385, acc: 0.00%] [G loss: 0.327293]\n",
            "Epoch: 64.57 [D loss: 1.415428, acc: 0.00%] [G loss: 0.329654]\n",
            "Epoch: 64.58 [D loss: 1.412654, acc: 0.00%] [G loss: 0.328907]\n",
            "Epoch: 64.59 [D loss: 1.390334, acc: 0.00%] [G loss: 0.328781]\n",
            "Epoch: 64.60 [D loss: 1.389500, acc: 0.00%] [G loss: 0.329894]\n",
            "Epoch: 64.61 [D loss: 1.393353, acc: 0.00%] [G loss: 0.327874]\n",
            "Epoch: 64.62 [D loss: 1.370703, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 64.63 [D loss: 1.378482, acc: 0.00%] [G loss: 0.327596]\n",
            "Epoch: 64.64 [D loss: 1.324378, acc: 0.00%] [G loss: 0.327852]\n",
            "Epoch: 64.65 [D loss: 1.282408, acc: 0.00%] [G loss: 0.328131]\n",
            "Epoch: 64.66 [D loss: 1.296672, acc: 0.00%] [G loss: 0.327559]\n",
            "Epoch: 64.67 [D loss: 1.288754, acc: 0.00%] [G loss: 0.327886]\n",
            "Epoch: 64.68 [D loss: 1.308866, acc: 0.00%] [G loss: 0.328806]\n",
            "Epoch: 64.69 [D loss: 1.285142, acc: 0.00%] [G loss: 0.329933]\n",
            "Epoch: 64.70 [D loss: 1.393689, acc: 0.00%] [G loss: 0.327916]\n",
            "Epoch: 64.71 [D loss: 1.373456, acc: 0.00%] [G loss: 0.327491]\n",
            "Epoch: 64.72 [D loss: 1.406131, acc: 0.00%] [G loss: 0.328058]\n",
            "Epoch: 64.73 [D loss: 1.380518, acc: 0.00%] [G loss: 0.328622]\n",
            "Epoch: 64.74 [D loss: 1.448467, acc: 0.00%] [G loss: 0.328280]\n",
            "Epoch: 64.75 [D loss: 1.450461, acc: 0.00%] [G loss: 0.327262]\n",
            "Epoch: 64.76 [D loss: 1.357611, acc: 0.00%] [G loss: 0.327725]\n",
            "Epoch: 64.77 [D loss: 1.429897, acc: 0.00%] [G loss: 0.328126]\n",
            "Epoch: 64.78 [D loss: 1.343352, acc: 0.00%] [G loss: 0.328508]\n",
            "Epoch: 64.79 [D loss: 1.341943, acc: 0.00%] [G loss: 0.327249]\n",
            "Epoch: 64.80 [D loss: 1.262755, acc: 0.00%] [G loss: 0.328386]\n",
            "Epoch: 64.81 [D loss: 1.278565, acc: 0.00%] [G loss: 0.328917]\n",
            "Epoch: 64.82 [D loss: 1.329192, acc: 0.00%] [G loss: 0.328129]\n",
            "Epoch: 64.83 [D loss: 1.383253, acc: 0.00%] [G loss: 0.327451]\n",
            "Epoch: 64.84 [D loss: 1.384634, acc: 0.00%] [G loss: 0.327022]\n",
            "Epoch: 64.85 [D loss: 1.359484, acc: 0.00%] [G loss: 0.326755]\n",
            "Epoch: 64.86 [D loss: 1.438474, acc: 0.00%] [G loss: 0.330895]\n",
            "Epoch: 64.87 [D loss: 1.408778, acc: 0.00%] [G loss: 0.327532]\n",
            "Epoch: 64.88 [D loss: 1.376349, acc: 0.00%] [G loss: 0.330908]\n",
            "Epoch: 64.89 [D loss: 1.383153, acc: 0.00%] [G loss: 0.328148]\n",
            "Epoch: 64.90 [D loss: 1.367668, acc: 0.00%] [G loss: 0.327422]\n",
            "Epoch: 64.91 [D loss: 1.406231, acc: 0.00%] [G loss: 0.327555]\n",
            "Epoch: 64.92 [D loss: 1.343116, acc: 0.00%] [G loss: 0.328190]\n",
            "Epoch: 64.93 [D loss: 1.376143, acc: 0.00%] [G loss: 0.328025]\n",
            "Epoch: 64.94 [D loss: 1.358778, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 64.95 [D loss: 1.382526, acc: 0.00%] [G loss: 0.328324]\n",
            "Epoch: 64.96 [D loss: 1.397392, acc: 0.00%] [G loss: 0.328089]\n",
            "Epoch: 64.97 [D loss: 1.385331, acc: 0.00%] [G loss: 0.328668]\n",
            "Epoch: 64.98 [D loss: 1.366488, acc: 0.00%] [G loss: 0.328750]\n",
            "Epoch: 64.99 [D loss: 1.339424, acc: 0.00%] [G loss: 0.328014]\n",
            "Epoch: 64.100 [D loss: 1.375081, acc: 0.00%] [G loss: 0.329256]\n",
            "Epoch: 64.101 [D loss: 1.354261, acc: 0.00%] [G loss: 0.327316]\n",
            "Epoch: 64.102 [D loss: 1.351756, acc: 0.00%] [G loss: 0.329091]\n",
            "Epoch: 64.103 [D loss: 1.340872, acc: 0.00%] [G loss: 0.327592]\n",
            "Epoch: 64.104 [D loss: 1.334046, acc: 0.00%] [G loss: 0.328731]\n",
            "Epoch: 64.105 [D loss: 1.369166, acc: 0.00%] [G loss: 0.329161]\n",
            "Epoch: 64.106 [D loss: 1.380084, acc: 0.00%] [G loss: 0.326460]\n",
            "Epoch: 64.107 [D loss: 1.331864, acc: 0.00%] [G loss: 0.328670]\n",
            "Epoch: 65.0 [D loss: 1.327418, acc: 0.00%] [G loss: 0.328266]\n",
            "Epoch: 65.1 [D loss: 1.384727, acc: 0.00%] [G loss: 0.329279]\n",
            "Epoch: 65.2 [D loss: 1.382762, acc: 0.00%] [G loss: 0.327424]\n",
            "Epoch: 65.3 [D loss: 1.444826, acc: 0.00%] [G loss: 0.327945]\n",
            "Epoch: 65.4 [D loss: 1.371354, acc: 0.00%] [G loss: 0.328659]\n",
            "Epoch: 65.5 [D loss: 1.439818, acc: 0.00%] [G loss: 0.327756]\n",
            "Epoch: 65.6 [D loss: 1.362806, acc: 0.00%] [G loss: 0.328560]\n",
            "Epoch: 65.7 [D loss: 1.351580, acc: 0.00%] [G loss: 0.326141]\n",
            "Epoch: 65.8 [D loss: 1.337192, acc: 0.00%] [G loss: 0.326606]\n",
            "Epoch: 65.9 [D loss: 1.382351, acc: 0.00%] [G loss: 0.327756]\n",
            "Epoch: 65.10 [D loss: 1.349441, acc: 0.00%] [G loss: 0.329078]\n",
            "Epoch: 65.11 [D loss: 1.342044, acc: 0.00%] [G loss: 0.327773]\n",
            "Epoch: 65.12 [D loss: 1.344120, acc: 0.00%] [G loss: 0.328913]\n",
            "Epoch: 65.13 [D loss: 1.360062, acc: 0.00%] [G loss: 0.330277]\n",
            "Epoch: 65.14 [D loss: 1.422592, acc: 0.00%] [G loss: 0.328220]\n",
            "Epoch: 65.15 [D loss: 1.453166, acc: 0.00%] [G loss: 0.327437]\n",
            "Epoch: 65.16 [D loss: 1.387283, acc: 0.00%] [G loss: 0.328726]\n",
            "Epoch: 65.17 [D loss: 1.444815, acc: 0.00%] [G loss: 0.329485]\n",
            "Epoch: 65.18 [D loss: 1.401333, acc: 0.00%] [G loss: 0.328311]\n",
            "Epoch: 65.19 [D loss: 1.352072, acc: 0.00%] [G loss: 0.327653]\n",
            "Epoch: 65.20 [D loss: 1.364093, acc: 0.00%] [G loss: 0.328091]\n",
            "Epoch: 65.21 [D loss: 1.276409, acc: 0.00%] [G loss: 0.327679]\n",
            "Epoch: 65.22 [D loss: 1.363188, acc: 0.00%] [G loss: 0.328448]\n",
            "Epoch: 65.23 [D loss: 1.312505, acc: 0.00%] [G loss: 0.328241]\n",
            "Epoch: 65.24 [D loss: 1.313595, acc: 0.00%] [G loss: 0.328600]\n",
            "Epoch: 65.25 [D loss: 1.366850, acc: 0.00%] [G loss: 0.329431]\n",
            "Epoch: 65.26 [D loss: 1.333281, acc: 0.00%] [G loss: 0.327314]\n",
            "Epoch: 65.27 [D loss: 1.401336, acc: 0.00%] [G loss: 0.327478]\n",
            "Epoch: 65.28 [D loss: 1.358280, acc: 0.00%] [G loss: 0.329457]\n",
            "Epoch: 65.29 [D loss: 1.362619, acc: 0.00%] [G loss: 0.327638]\n",
            "Epoch: 65.30 [D loss: 1.412551, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 65.31 [D loss: 1.345740, acc: 0.00%] [G loss: 0.329130]\n",
            "Epoch: 65.32 [D loss: 1.371119, acc: 0.00%] [G loss: 0.327998]\n",
            "Epoch: 65.33 [D loss: 1.353344, acc: 0.00%] [G loss: 0.329098]\n",
            "Epoch: 65.34 [D loss: 1.350104, acc: 0.00%] [G loss: 0.327822]\n",
            "Epoch: 65.35 [D loss: 1.331245, acc: 0.00%] [G loss: 0.327610]\n",
            "Epoch: 65.36 [D loss: 1.262103, acc: 0.00%] [G loss: 0.328564]\n",
            "Epoch: 65.37 [D loss: 1.313151, acc: 0.00%] [G loss: 0.327659]\n",
            "Epoch: 65.38 [D loss: 1.317199, acc: 0.00%] [G loss: 0.329027]\n",
            "Epoch: 65.39 [D loss: 1.348701, acc: 0.00%] [G loss: 0.327596]\n",
            "Epoch: 65.40 [D loss: 1.339584, acc: 0.00%] [G loss: 0.328135]\n",
            "Epoch: 65.41 [D loss: 1.431097, acc: 0.00%] [G loss: 0.329217]\n",
            "Epoch: 65.42 [D loss: 1.361524, acc: 0.00%] [G loss: 0.328143]\n",
            "Epoch: 65.43 [D loss: 1.343013, acc: 0.00%] [G loss: 0.328785]\n",
            "Epoch: 65.44 [D loss: 1.415833, acc: 0.00%] [G loss: 0.329832]\n",
            "Epoch: 65.45 [D loss: 1.357185, acc: 0.00%] [G loss: 0.328782]\n",
            "Epoch: 65.46 [D loss: 1.382866, acc: 0.00%] [G loss: 0.329360]\n",
            "Epoch: 65.47 [D loss: 1.338181, acc: 0.00%] [G loss: 0.326858]\n",
            "Epoch: 65.48 [D loss: 1.402911, acc: 0.00%] [G loss: 0.328128]\n",
            "Epoch: 65.49 [D loss: 1.371081, acc: 0.00%] [G loss: 0.328003]\n",
            "Epoch: 65.50 [D loss: 1.305008, acc: 0.00%] [G loss: 0.326660]\n",
            "Epoch: 65.51 [D loss: 1.305759, acc: 0.00%] [G loss: 0.327753]\n",
            "Epoch: 65.52 [D loss: 1.347655, acc: 0.00%] [G loss: 0.328953]\n",
            "Epoch: 65.53 [D loss: 1.356861, acc: 0.00%] [G loss: 0.327624]\n",
            "Epoch: 65.54 [D loss: 1.281387, acc: 0.00%] [G loss: 0.327727]\n",
            "Epoch: 65.55 [D loss: 1.366685, acc: 0.00%] [G loss: 0.328214]\n",
            "Epoch: 65.56 [D loss: 1.374615, acc: 0.00%] [G loss: 0.327816]\n",
            "Epoch: 65.57 [D loss: 1.412308, acc: 0.00%] [G loss: 0.328123]\n",
            "Epoch: 65.58 [D loss: 1.385455, acc: 0.00%] [G loss: 0.327952]\n",
            "Epoch: 65.59 [D loss: 1.408160, acc: 0.00%] [G loss: 0.326452]\n",
            "Epoch: 65.60 [D loss: 1.371979, acc: 0.00%] [G loss: 0.330295]\n",
            "Epoch: 65.61 [D loss: 1.346110, acc: 0.00%] [G loss: 0.329114]\n",
            "Epoch: 65.62 [D loss: 1.432385, acc: 0.00%] [G loss: 0.328213]\n",
            "Epoch: 65.63 [D loss: 1.393773, acc: 0.00%] [G loss: 0.328515]\n",
            "Epoch: 65.64 [D loss: 1.348994, acc: 0.00%] [G loss: 0.328236]\n",
            "Epoch: 65.65 [D loss: 1.366740, acc: 0.00%] [G loss: 0.327259]\n",
            "Epoch: 65.66 [D loss: 1.358327, acc: 0.00%] [G loss: 0.328541]\n",
            "Epoch: 65.67 [D loss: 1.307991, acc: 0.00%] [G loss: 0.328514]\n",
            "Epoch: 65.68 [D loss: 1.363312, acc: 0.00%] [G loss: 0.328655]\n",
            "Epoch: 65.69 [D loss: 1.311327, acc: 0.00%] [G loss: 0.328850]\n",
            "Epoch: 65.70 [D loss: 1.334307, acc: 0.00%] [G loss: 0.328181]\n",
            "Epoch: 65.71 [D loss: 1.341271, acc: 0.00%] [G loss: 0.328873]\n",
            "Epoch: 65.72 [D loss: 1.407467, acc: 0.00%] [G loss: 0.327073]\n",
            "Epoch: 65.73 [D loss: 1.407934, acc: 0.00%] [G loss: 0.329221]\n",
            "Epoch: 65.74 [D loss: 1.413517, acc: 0.00%] [G loss: 0.327812]\n",
            "Epoch: 65.75 [D loss: 1.308196, acc: 0.00%] [G loss: 0.327695]\n",
            "Epoch: 65.76 [D loss: 1.289122, acc: 0.00%] [G loss: 0.327080]\n",
            "Epoch: 65.77 [D loss: 1.308920, acc: 0.00%] [G loss: 0.329078]\n",
            "Epoch: 65.78 [D loss: 1.395189, acc: 0.00%] [G loss: 0.328185]\n",
            "Epoch: 65.79 [D loss: 1.392848, acc: 0.00%] [G loss: 0.327149]\n",
            "Epoch: 65.80 [D loss: 1.302287, acc: 0.00%] [G loss: 0.328841]\n",
            "Epoch: 65.81 [D loss: 1.374082, acc: 0.00%] [G loss: 0.328373]\n",
            "Epoch: 65.82 [D loss: 1.335181, acc: 0.00%] [G loss: 0.327481]\n",
            "Epoch: 65.83 [D loss: 1.400296, acc: 0.00%] [G loss: 0.327987]\n",
            "Epoch: 65.84 [D loss: 1.389758, acc: 0.00%] [G loss: 0.328660]\n",
            "Epoch: 65.85 [D loss: 1.389185, acc: 0.00%] [G loss: 0.327344]\n",
            "Epoch: 65.86 [D loss: 1.361660, acc: 0.00%] [G loss: 0.327886]\n",
            "Epoch: 65.87 [D loss: 1.365066, acc: 0.00%] [G loss: 0.326718]\n",
            "Epoch: 65.88 [D loss: 1.369691, acc: 0.00%] [G loss: 0.329588]\n",
            "Epoch: 65.89 [D loss: 1.354915, acc: 0.00%] [G loss: 0.327791]\n",
            "Epoch: 65.90 [D loss: 1.396857, acc: 0.00%] [G loss: 0.329344]\n",
            "Epoch: 65.91 [D loss: 1.387701, acc: 0.00%] [G loss: 0.326775]\n",
            "Epoch: 65.92 [D loss: 1.352041, acc: 0.00%] [G loss: 0.328279]\n",
            "Epoch: 65.93 [D loss: 1.343313, acc: 0.00%] [G loss: 0.328289]\n",
            "Epoch: 65.94 [D loss: 1.308666, acc: 0.00%] [G loss: 0.327487]\n",
            "Epoch: 65.95 [D loss: 1.348814, acc: 0.00%] [G loss: 0.328189]\n",
            "Epoch: 65.96 [D loss: 1.395540, acc: 0.00%] [G loss: 0.327488]\n",
            "Epoch: 65.97 [D loss: 1.301092, acc: 0.00%] [G loss: 0.328534]\n",
            "Epoch: 65.98 [D loss: 1.341015, acc: 0.00%] [G loss: 0.327540]\n",
            "Epoch: 65.99 [D loss: 1.433224, acc: 0.00%] [G loss: 0.328743]\n",
            "Epoch: 65.100 [D loss: 1.374814, acc: 0.00%] [G loss: 0.329769]\n",
            "Epoch: 65.101 [D loss: 1.449986, acc: 0.00%] [G loss: 0.328497]\n",
            "Epoch: 65.102 [D loss: 1.398871, acc: 0.00%] [G loss: 0.329113]\n",
            "Epoch: 65.103 [D loss: 1.361504, acc: 0.00%] [G loss: 0.326458]\n",
            "Epoch: 65.104 [D loss: 1.334697, acc: 0.00%] [G loss: 0.329957]\n",
            "Epoch: 65.105 [D loss: 1.276422, acc: 0.00%] [G loss: 0.328422]\n",
            "Epoch: 65.106 [D loss: 1.273182, acc: 0.00%] [G loss: 0.326509]\n",
            "Epoch: 65.107 [D loss: 1.365786, acc: 0.00%] [G loss: 0.329212]\n",
            "Epoch: 66.0 [D loss: 1.323128, acc: 0.00%] [G loss: 0.328740]\n",
            "Epoch: 66.1 [D loss: 1.348285, acc: 0.00%] [G loss: 0.327666]\n",
            "Epoch: 66.2 [D loss: 1.383598, acc: 0.00%] [G loss: 0.328143]\n",
            "Epoch: 66.3 [D loss: 1.392497, acc: 0.00%] [G loss: 0.327145]\n",
            "Epoch: 66.4 [D loss: 1.353755, acc: 0.00%] [G loss: 0.328465]\n",
            "Epoch: 66.5 [D loss: 1.415797, acc: 0.00%] [G loss: 0.327959]\n",
            "Epoch: 66.6 [D loss: 1.354939, acc: 0.00%] [G loss: 0.327258]\n",
            "Epoch: 66.7 [D loss: 1.363879, acc: 0.00%] [G loss: 0.327492]\n",
            "Epoch: 66.8 [D loss: 1.368650, acc: 0.00%] [G loss: 0.330202]\n",
            "Epoch: 66.9 [D loss: 1.404855, acc: 0.00%] [G loss: 0.327678]\n",
            "Epoch: 66.10 [D loss: 1.320527, acc: 0.00%] [G loss: 0.327173]\n",
            "Epoch: 66.11 [D loss: 1.383600, acc: 0.00%] [G loss: 0.328421]\n",
            "Epoch: 66.12 [D loss: 1.388632, acc: 0.00%] [G loss: 0.327426]\n",
            "Epoch: 66.13 [D loss: 1.349062, acc: 0.00%] [G loss: 0.327536]\n",
            "Epoch: 66.14 [D loss: 1.357412, acc: 0.00%] [G loss: 0.327562]\n",
            "Epoch: 66.15 [D loss: 1.355298, acc: 0.00%] [G loss: 0.329006]\n",
            "Epoch: 66.16 [D loss: 1.370805, acc: 0.00%] [G loss: 0.328484]\n",
            "Epoch: 66.17 [D loss: 1.322242, acc: 0.00%] [G loss: 0.328280]\n",
            "Epoch: 66.18 [D loss: 1.394122, acc: 0.00%] [G loss: 0.327995]\n",
            "Epoch: 66.19 [D loss: 1.366650, acc: 0.00%] [G loss: 0.328487]\n",
            "Epoch: 66.20 [D loss: 1.419611, acc: 0.00%] [G loss: 0.328492]\n",
            "Epoch: 66.21 [D loss: 1.422035, acc: 0.00%] [G loss: 0.327208]\n",
            "Epoch: 66.22 [D loss: 1.389722, acc: 0.00%] [G loss: 0.327973]\n",
            "Epoch: 66.23 [D loss: 1.363726, acc: 0.00%] [G loss: 0.327633]\n",
            "Epoch: 66.24 [D loss: 1.362620, acc: 0.00%] [G loss: 0.327309]\n",
            "Epoch: 66.25 [D loss: 1.401504, acc: 0.00%] [G loss: 0.330768]\n",
            "Epoch: 66.26 [D loss: 1.367653, acc: 0.00%] [G loss: 0.327471]\n",
            "Epoch: 66.27 [D loss: 1.313972, acc: 0.00%] [G loss: 0.329095]\n",
            "Epoch: 66.28 [D loss: 1.345663, acc: 0.00%] [G loss: 0.327552]\n",
            "Epoch: 66.29 [D loss: 1.378670, acc: 0.00%] [G loss: 0.329213]\n",
            "Epoch: 66.30 [D loss: 1.314580, acc: 0.00%] [G loss: 0.327944]\n",
            "Epoch: 66.31 [D loss: 1.364618, acc: 0.00%] [G loss: 0.329790]\n",
            "Epoch: 66.32 [D loss: 1.342116, acc: 0.00%] [G loss: 0.328473]\n",
            "Epoch: 66.33 [D loss: 1.355986, acc: 0.00%] [G loss: 0.326942]\n",
            "Epoch: 66.34 [D loss: 1.384291, acc: 0.00%] [G loss: 0.328192]\n",
            "Epoch: 66.35 [D loss: 1.327516, acc: 0.00%] [G loss: 0.328566]\n",
            "Epoch: 66.36 [D loss: 1.374976, acc: 0.00%] [G loss: 0.327411]\n",
            "Epoch: 66.37 [D loss: 1.297955, acc: 0.00%] [G loss: 0.329979]\n",
            "Epoch: 66.38 [D loss: 1.346861, acc: 0.00%] [G loss: 0.328298]\n",
            "Epoch: 66.39 [D loss: 1.350259, acc: 0.00%] [G loss: 0.328728]\n",
            "Epoch: 66.40 [D loss: 1.357911, acc: 0.00%] [G loss: 0.329002]\n",
            "Epoch: 66.41 [D loss: 1.390158, acc: 0.00%] [G loss: 0.328506]\n",
            "Epoch: 66.42 [D loss: 1.357814, acc: 0.00%] [G loss: 0.329502]\n",
            "Epoch: 66.43 [D loss: 1.421298, acc: 0.00%] [G loss: 0.328151]\n",
            "Epoch: 66.44 [D loss: 1.484761, acc: 0.00%] [G loss: 0.327429]\n",
            "Epoch: 66.45 [D loss: 1.396737, acc: 0.00%] [G loss: 0.329057]\n",
            "Epoch: 66.46 [D loss: 1.386454, acc: 0.00%] [G loss: 0.327784]\n",
            "Epoch: 66.47 [D loss: 1.391585, acc: 0.00%] [G loss: 0.327304]\n",
            "Epoch: 66.48 [D loss: 1.413707, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 66.49 [D loss: 1.343542, acc: 0.00%] [G loss: 0.327198]\n",
            "Epoch: 66.50 [D loss: 1.325990, acc: 0.00%] [G loss: 0.327075]\n",
            "Epoch: 66.51 [D loss: 1.343503, acc: 0.00%] [G loss: 0.327512]\n",
            "Epoch: 66.52 [D loss: 1.349132, acc: 0.00%] [G loss: 0.328193]\n",
            "Epoch: 66.53 [D loss: 1.330157, acc: 0.00%] [G loss: 0.328261]\n",
            "Epoch: 66.54 [D loss: 1.349949, acc: 0.00%] [G loss: 0.328105]\n",
            "Epoch: 66.55 [D loss: 1.418534, acc: 0.00%] [G loss: 0.328255]\n",
            "Epoch: 66.56 [D loss: 1.422044, acc: 0.00%] [G loss: 0.327649]\n",
            "Epoch: 66.57 [D loss: 1.404305, acc: 0.00%] [G loss: 0.326616]\n",
            "Epoch: 66.58 [D loss: 1.380646, acc: 0.00%] [G loss: 0.327069]\n",
            "Epoch: 66.59 [D loss: 1.357535, acc: 0.00%] [G loss: 0.327746]\n",
            "Epoch: 66.60 [D loss: 1.329962, acc: 0.00%] [G loss: 0.328264]\n",
            "Epoch: 66.61 [D loss: 1.351389, acc: 0.00%] [G loss: 0.330383]\n",
            "Epoch: 66.62 [D loss: 1.394524, acc: 0.00%] [G loss: 0.328059]\n",
            "Epoch: 66.63 [D loss: 1.407728, acc: 0.00%] [G loss: 0.330092]\n",
            "Epoch: 66.64 [D loss: 1.371248, acc: 0.00%] [G loss: 0.328642]\n",
            "Epoch: 66.65 [D loss: 1.425259, acc: 0.00%] [G loss: 0.327322]\n",
            "Epoch: 66.66 [D loss: 1.396642, acc: 0.00%] [G loss: 0.329299]\n",
            "Epoch: 66.67 [D loss: 1.377375, acc: 0.00%] [G loss: 0.328507]\n",
            "Epoch: 66.68 [D loss: 1.362213, acc: 0.00%] [G loss: 0.326925]\n",
            "Epoch: 66.69 [D loss: 1.330265, acc: 0.00%] [G loss: 0.328908]\n",
            "Epoch: 66.70 [D loss: 1.355146, acc: 0.00%] [G loss: 0.328699]\n",
            "Epoch: 66.71 [D loss: 1.353137, acc: 0.00%] [G loss: 0.328058]\n",
            "Epoch: 66.72 [D loss: 1.382767, acc: 0.00%] [G loss: 0.328108]\n",
            "Epoch: 66.73 [D loss: 1.371208, acc: 0.00%] [G loss: 0.328963]\n",
            "Epoch: 66.74 [D loss: 1.388846, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 66.75 [D loss: 1.333867, acc: 0.00%] [G loss: 0.327629]\n",
            "Epoch: 66.76 [D loss: 1.374753, acc: 0.00%] [G loss: 0.328944]\n",
            "Epoch: 66.77 [D loss: 1.393084, acc: 0.00%] [G loss: 0.327489]\n",
            "Epoch: 66.78 [D loss: 1.403269, acc: 0.00%] [G loss: 0.328380]\n",
            "Epoch: 66.79 [D loss: 1.360152, acc: 0.00%] [G loss: 0.327221]\n",
            "Epoch: 66.80 [D loss: 1.400656, acc: 0.00%] [G loss: 0.327485]\n",
            "Epoch: 66.81 [D loss: 1.345474, acc: 0.00%] [G loss: 0.329035]\n",
            "Epoch: 66.82 [D loss: 1.361737, acc: 0.00%] [G loss: 0.326772]\n",
            "Epoch: 66.83 [D loss: 1.415920, acc: 0.00%] [G loss: 0.328960]\n",
            "Epoch: 66.84 [D loss: 1.395770, acc: 0.00%] [G loss: 0.327316]\n",
            "Epoch: 66.85 [D loss: 1.366244, acc: 0.00%] [G loss: 0.330149]\n",
            "Epoch: 66.86 [D loss: 1.375116, acc: 0.00%] [G loss: 0.327384]\n",
            "Epoch: 66.87 [D loss: 1.424725, acc: 0.00%] [G loss: 0.329437]\n",
            "Epoch: 66.88 [D loss: 1.365769, acc: 0.00%] [G loss: 0.327792]\n",
            "Epoch: 66.89 [D loss: 1.343783, acc: 0.00%] [G loss: 0.326790]\n",
            "Epoch: 66.90 [D loss: 1.330603, acc: 0.00%] [G loss: 0.328754]\n",
            "Epoch: 66.91 [D loss: 1.275432, acc: 0.00%] [G loss: 0.329191]\n",
            "Epoch: 66.92 [D loss: 1.330894, acc: 0.00%] [G loss: 0.329107]\n",
            "Epoch: 66.93 [D loss: 1.323849, acc: 0.00%] [G loss: 0.328458]\n",
            "Epoch: 66.94 [D loss: 1.281136, acc: 0.00%] [G loss: 0.330765]\n",
            "Epoch: 66.95 [D loss: 1.387341, acc: 0.00%] [G loss: 0.328153]\n",
            "Epoch: 66.96 [D loss: 1.375225, acc: 0.00%] [G loss: 0.329797]\n",
            "Epoch: 66.97 [D loss: 1.418912, acc: 0.00%] [G loss: 0.329880]\n",
            "Epoch: 66.98 [D loss: 1.439179, acc: 0.00%] [G loss: 0.328451]\n",
            "Epoch: 66.99 [D loss: 1.454359, acc: 0.00%] [G loss: 0.328331]\n",
            "Epoch: 66.100 [D loss: 1.365880, acc: 0.00%] [G loss: 0.328358]\n",
            "Epoch: 66.101 [D loss: 1.402627, acc: 0.00%] [G loss: 0.327983]\n",
            "Epoch: 66.102 [D loss: 1.309462, acc: 0.00%] [G loss: 0.328093]\n",
            "Epoch: 66.103 [D loss: 1.314285, acc: 0.00%] [G loss: 0.328879]\n",
            "Epoch: 66.104 [D loss: 1.335268, acc: 0.00%] [G loss: 0.329345]\n",
            "Epoch: 66.105 [D loss: 1.353714, acc: 0.00%] [G loss: 0.327495]\n",
            "Epoch: 66.106 [D loss: 1.413182, acc: 0.00%] [G loss: 0.326367]\n",
            "Epoch: 66.107 [D loss: 1.438593, acc: 0.00%] [G loss: 0.329405]\n",
            "Epoch: 67.0 [D loss: 1.392944, acc: 0.00%] [G loss: 0.327517]\n",
            "Epoch: 67.1 [D loss: 1.364439, acc: 0.00%] [G loss: 0.328146]\n",
            "Epoch: 67.2 [D loss: 1.459791, acc: 0.00%] [G loss: 0.327796]\n",
            "Epoch: 67.3 [D loss: 1.406548, acc: 0.00%] [G loss: 0.329342]\n",
            "Epoch: 67.4 [D loss: 1.406170, acc: 0.00%] [G loss: 0.327572]\n",
            "Epoch: 67.5 [D loss: 1.374837, acc: 0.00%] [G loss: 0.331695]\n",
            "Epoch: 67.6 [D loss: 1.386681, acc: 0.00%] [G loss: 0.327478]\n",
            "Epoch: 67.7 [D loss: 1.347317, acc: 0.00%] [G loss: 0.328290]\n",
            "Epoch: 67.8 [D loss: 1.353074, acc: 0.00%] [G loss: 0.329467]\n",
            "Epoch: 67.9 [D loss: 1.351081, acc: 0.00%] [G loss: 0.329279]\n",
            "Epoch: 67.10 [D loss: 1.353761, acc: 0.00%] [G loss: 0.328896]\n",
            "Epoch: 67.11 [D loss: 1.357585, acc: 0.00%] [G loss: 0.328635]\n",
            "Epoch: 67.12 [D loss: 1.367120, acc: 0.00%] [G loss: 0.328366]\n",
            "Epoch: 67.13 [D loss: 1.394096, acc: 0.00%] [G loss: 0.328031]\n",
            "Epoch: 67.14 [D loss: 1.394970, acc: 0.00%] [G loss: 0.328488]\n",
            "Epoch: 67.15 [D loss: 1.392321, acc: 0.00%] [G loss: 0.328406]\n",
            "Epoch: 67.16 [D loss: 1.404453, acc: 0.00%] [G loss: 0.328521]\n",
            "Epoch: 67.17 [D loss: 1.399562, acc: 0.00%] [G loss: 0.328694]\n",
            "Epoch: 67.18 [D loss: 1.452258, acc: 0.00%] [G loss: 0.328073]\n",
            "Epoch: 67.19 [D loss: 1.382201, acc: 0.00%] [G loss: 0.327237]\n",
            "Epoch: 67.20 [D loss: 1.390240, acc: 0.00%] [G loss: 0.327441]\n",
            "Epoch: 67.21 [D loss: 1.373224, acc: 0.00%] [G loss: 0.327459]\n",
            "Epoch: 67.22 [D loss: 1.357597, acc: 0.00%] [G loss: 0.328073]\n",
            "Epoch: 67.23 [D loss: 1.319372, acc: 0.00%] [G loss: 0.328467]\n",
            "Epoch: 67.24 [D loss: 1.363454, acc: 0.00%] [G loss: 0.327350]\n",
            "Epoch: 67.25 [D loss: 1.335292, acc: 0.00%] [G loss: 0.327380]\n",
            "Epoch: 67.26 [D loss: 1.381442, acc: 0.00%] [G loss: 0.327445]\n",
            "Epoch: 67.27 [D loss: 1.356431, acc: 0.00%] [G loss: 0.326698]\n",
            "Epoch: 67.28 [D loss: 1.391746, acc: 0.00%] [G loss: 0.328162]\n",
            "Epoch: 67.29 [D loss: 1.336342, acc: 0.00%] [G loss: 0.327559]\n",
            "Epoch: 67.30 [D loss: 1.423381, acc: 0.00%] [G loss: 0.327134]\n",
            "Epoch: 67.31 [D loss: 1.370811, acc: 0.00%] [G loss: 0.327545]\n",
            "Epoch: 67.32 [D loss: 1.382186, acc: 0.00%] [G loss: 0.327592]\n",
            "Epoch: 67.33 [D loss: 1.416922, acc: 0.00%] [G loss: 0.329206]\n",
            "Epoch: 67.34 [D loss: 1.420447, acc: 0.00%] [G loss: 0.328603]\n",
            "Epoch: 67.35 [D loss: 1.326000, acc: 0.00%] [G loss: 0.328352]\n",
            "Epoch: 67.36 [D loss: 1.408302, acc: 0.00%] [G loss: 0.327488]\n",
            "Epoch: 67.37 [D loss: 1.390165, acc: 0.00%] [G loss: 0.328021]\n",
            "Epoch: 67.38 [D loss: 1.347782, acc: 0.00%] [G loss: 0.328392]\n",
            "Epoch: 67.39 [D loss: 1.333333, acc: 0.00%] [G loss: 0.327609]\n",
            "Epoch: 67.40 [D loss: 1.330731, acc: 0.00%] [G loss: 0.327666]\n",
            "Epoch: 67.41 [D loss: 1.425074, acc: 0.00%] [G loss: 0.331284]\n",
            "Epoch: 67.42 [D loss: 1.389206, acc: 0.00%] [G loss: 0.326926]\n",
            "Epoch: 67.43 [D loss: 1.406409, acc: 0.00%] [G loss: 0.327719]\n",
            "Epoch: 67.44 [D loss: 1.410003, acc: 0.00%] [G loss: 0.328799]\n",
            "Epoch: 67.45 [D loss: 1.454021, acc: 0.00%] [G loss: 0.329605]\n",
            "Epoch: 67.46 [D loss: 1.404406, acc: 0.00%] [G loss: 0.328251]\n",
            "Epoch: 67.47 [D loss: 1.372257, acc: 0.00%] [G loss: 0.327210]\n",
            "Epoch: 67.48 [D loss: 1.337584, acc: 0.00%] [G loss: 0.328133]\n",
            "Epoch: 67.49 [D loss: 1.394707, acc: 0.00%] [G loss: 0.327855]\n",
            "Epoch: 67.50 [D loss: 1.394120, acc: 0.00%] [G loss: 0.328495]\n",
            "Epoch: 67.51 [D loss: 1.462878, acc: 0.00%] [G loss: 0.328063]\n",
            "Epoch: 67.52 [D loss: 1.393774, acc: 0.00%] [G loss: 0.328269]\n",
            "Epoch: 67.53 [D loss: 1.323134, acc: 0.00%] [G loss: 0.327796]\n",
            "Epoch: 67.54 [D loss: 1.343359, acc: 0.00%] [G loss: 0.327982]\n",
            "Epoch: 67.55 [D loss: 1.377722, acc: 0.00%] [G loss: 0.327146]\n",
            "Epoch: 67.56 [D loss: 1.391395, acc: 0.00%] [G loss: 0.328370]\n",
            "Epoch: 67.57 [D loss: 1.405926, acc: 0.00%] [G loss: 0.327050]\n",
            "Epoch: 67.58 [D loss: 1.354323, acc: 0.00%] [G loss: 0.328033]\n",
            "Epoch: 67.59 [D loss: 1.356133, acc: 0.00%] [G loss: 0.327998]\n",
            "Epoch: 67.60 [D loss: 1.343804, acc: 0.00%] [G loss: 0.328607]\n",
            "Epoch: 67.61 [D loss: 1.384475, acc: 0.00%] [G loss: 0.327679]\n",
            "Epoch: 67.62 [D loss: 1.343211, acc: 0.00%] [G loss: 0.327708]\n",
            "Epoch: 67.63 [D loss: 1.367760, acc: 0.00%] [G loss: 0.327889]\n",
            "Epoch: 67.64 [D loss: 1.368684, acc: 0.00%] [G loss: 0.327954]\n",
            "Epoch: 67.65 [D loss: 1.336793, acc: 0.00%] [G loss: 0.328249]\n",
            "Epoch: 67.66 [D loss: 1.380003, acc: 0.00%] [G loss: 0.327988]\n",
            "Epoch: 67.67 [D loss: 1.433256, acc: 0.00%] [G loss: 0.327824]\n",
            "Epoch: 67.68 [D loss: 1.358279, acc: 0.00%] [G loss: 0.328151]\n",
            "Epoch: 67.69 [D loss: 1.378431, acc: 0.00%] [G loss: 0.327111]\n",
            "Epoch: 67.70 [D loss: 1.390063, acc: 0.00%] [G loss: 0.327292]\n",
            "Epoch: 67.71 [D loss: 1.382683, acc: 0.00%] [G loss: 0.328300]\n",
            "Epoch: 67.72 [D loss: 1.345767, acc: 0.00%] [G loss: 0.328327]\n",
            "Epoch: 67.73 [D loss: 1.346652, acc: 0.00%] [G loss: 0.328086]\n",
            "Epoch: 67.74 [D loss: 1.329455, acc: 0.00%] [G loss: 0.328918]\n",
            "Epoch: 67.75 [D loss: 1.325706, acc: 0.00%] [G loss: 0.328817]\n",
            "Epoch: 67.76 [D loss: 1.379961, acc: 0.00%] [G loss: 0.326740]\n",
            "Epoch: 67.77 [D loss: 1.336880, acc: 0.00%] [G loss: 0.328081]\n",
            "Epoch: 67.78 [D loss: 1.337012, acc: 0.00%] [G loss: 0.327007]\n",
            "Epoch: 67.79 [D loss: 1.401754, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 67.80 [D loss: 1.374914, acc: 0.00%] [G loss: 0.327881]\n",
            "Epoch: 67.81 [D loss: 1.389236, acc: 0.00%] [G loss: 0.328703]\n",
            "Epoch: 67.82 [D loss: 1.386044, acc: 0.00%] [G loss: 0.327029]\n",
            "Epoch: 67.83 [D loss: 1.375646, acc: 0.00%] [G loss: 0.328717]\n",
            "Epoch: 67.84 [D loss: 1.388061, acc: 0.00%] [G loss: 0.327737]\n",
            "Epoch: 67.85 [D loss: 1.390343, acc: 0.00%] [G loss: 0.329877]\n",
            "Epoch: 67.86 [D loss: 1.405418, acc: 0.00%] [G loss: 0.327515]\n",
            "Epoch: 67.87 [D loss: 1.391776, acc: 0.00%] [G loss: 0.327896]\n",
            "Epoch: 67.88 [D loss: 1.356635, acc: 0.00%] [G loss: 0.327518]\n",
            "Epoch: 67.89 [D loss: 1.380011, acc: 0.00%] [G loss: 0.328713]\n",
            "Epoch: 67.90 [D loss: 1.363856, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 67.91 [D loss: 1.334351, acc: 0.00%] [G loss: 0.326463]\n",
            "Epoch: 67.92 [D loss: 1.355855, acc: 0.00%] [G loss: 0.327597]\n",
            "Epoch: 67.93 [D loss: 1.318878, acc: 0.00%] [G loss: 0.328494]\n",
            "Epoch: 67.94 [D loss: 1.282751, acc: 0.00%] [G loss: 0.328380]\n",
            "Epoch: 67.95 [D loss: 1.378020, acc: 0.00%] [G loss: 0.328599]\n",
            "Epoch: 67.96 [D loss: 1.334225, acc: 0.00%] [G loss: 0.328043]\n",
            "Epoch: 67.97 [D loss: 1.357160, acc: 0.00%] [G loss: 0.327763]\n",
            "Epoch: 67.98 [D loss: 1.322200, acc: 0.00%] [G loss: 0.329536]\n",
            "Epoch: 67.99 [D loss: 1.392162, acc: 0.00%] [G loss: 0.329889]\n",
            "Epoch: 67.100 [D loss: 1.336277, acc: 0.00%] [G loss: 0.326381]\n",
            "Epoch: 67.101 [D loss: 1.417027, acc: 0.00%] [G loss: 0.329045]\n",
            "Epoch: 67.102 [D loss: 1.368667, acc: 0.00%] [G loss: 0.329551]\n",
            "Epoch: 67.103 [D loss: 1.414805, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 67.104 [D loss: 1.377166, acc: 0.00%] [G loss: 0.327570]\n",
            "Epoch: 67.105 [D loss: 1.474242, acc: 0.00%] [G loss: 0.328922]\n",
            "Epoch: 67.106 [D loss: 1.409848, acc: 0.00%] [G loss: 0.329627]\n",
            "Epoch: 67.107 [D loss: 1.386759, acc: 0.00%] [G loss: 0.326815]\n",
            "Epoch: 68.0 [D loss: 1.335212, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 68.1 [D loss: 1.347375, acc: 0.00%] [G loss: 0.325860]\n",
            "Epoch: 68.2 [D loss: 1.344504, acc: 0.00%] [G loss: 0.328471]\n",
            "Epoch: 68.3 [D loss: 1.355733, acc: 0.00%] [G loss: 0.327144]\n",
            "Epoch: 68.4 [D loss: 1.311928, acc: 0.00%] [G loss: 0.327498]\n",
            "Epoch: 68.5 [D loss: 1.283662, acc: 0.00%] [G loss: 0.329596]\n",
            "Epoch: 68.6 [D loss: 1.366842, acc: 0.00%] [G loss: 0.326840]\n",
            "Epoch: 68.7 [D loss: 1.353558, acc: 0.00%] [G loss: 0.329051]\n",
            "Epoch: 68.8 [D loss: 1.335770, acc: 0.00%] [G loss: 0.327341]\n",
            "Epoch: 68.9 [D loss: 1.411786, acc: 0.00%] [G loss: 0.327434]\n",
            "Epoch: 68.10 [D loss: 1.408859, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 68.11 [D loss: 1.345501, acc: 0.00%] [G loss: 0.328271]\n",
            "Epoch: 68.12 [D loss: 1.341558, acc: 0.00%] [G loss: 0.326915]\n",
            "Epoch: 68.13 [D loss: 1.387266, acc: 0.00%] [G loss: 0.327904]\n",
            "Epoch: 68.14 [D loss: 1.302509, acc: 0.00%] [G loss: 0.328530]\n",
            "Epoch: 68.15 [D loss: 1.310814, acc: 0.00%] [G loss: 0.327693]\n",
            "Epoch: 68.16 [D loss: 1.336606, acc: 0.00%] [G loss: 0.327994]\n",
            "Epoch: 68.17 [D loss: 1.346970, acc: 0.00%] [G loss: 0.328558]\n",
            "Epoch: 68.18 [D loss: 1.360748, acc: 0.00%] [G loss: 0.329403]\n",
            "Epoch: 68.19 [D loss: 1.403617, acc: 0.00%] [G loss: 0.328015]\n",
            "Epoch: 68.20 [D loss: 1.387706, acc: 0.00%] [G loss: 0.328083]\n",
            "Epoch: 68.21 [D loss: 1.395600, acc: 0.00%] [G loss: 0.328854]\n",
            "Epoch: 68.22 [D loss: 1.374840, acc: 0.00%] [G loss: 0.327021]\n",
            "Epoch: 68.23 [D loss: 1.350964, acc: 0.00%] [G loss: 0.330376]\n",
            "Epoch: 68.24 [D loss: 1.347420, acc: 0.00%] [G loss: 0.327510]\n",
            "Epoch: 68.25 [D loss: 1.356075, acc: 0.00%] [G loss: 0.329768]\n",
            "Epoch: 68.26 [D loss: 1.393851, acc: 0.00%] [G loss: 0.329737]\n",
            "Epoch: 68.27 [D loss: 1.349270, acc: 0.00%] [G loss: 0.328198]\n",
            "Epoch: 68.28 [D loss: 1.370619, acc: 0.00%] [G loss: 0.327834]\n",
            "Epoch: 68.29 [D loss: 1.366582, acc: 0.00%] [G loss: 0.326928]\n",
            "Epoch: 68.30 [D loss: 1.380415, acc: 0.00%] [G loss: 0.327931]\n",
            "Epoch: 68.31 [D loss: 1.370604, acc: 0.00%] [G loss: 0.327006]\n",
            "Epoch: 68.32 [D loss: 1.428572, acc: 0.00%] [G loss: 0.329069]\n",
            "Epoch: 68.33 [D loss: 1.404969, acc: 0.00%] [G loss: 0.327994]\n",
            "Epoch: 68.34 [D loss: 1.373267, acc: 0.00%] [G loss: 0.327950]\n",
            "Epoch: 68.35 [D loss: 1.390095, acc: 0.00%] [G loss: 0.327435]\n",
            "Epoch: 68.36 [D loss: 1.383302, acc: 0.00%] [G loss: 0.327571]\n",
            "Epoch: 68.37 [D loss: 1.361504, acc: 0.00%] [G loss: 0.327957]\n",
            "Epoch: 68.38 [D loss: 1.365286, acc: 0.00%] [G loss: 0.327999]\n",
            "Epoch: 68.39 [D loss: 1.351956, acc: 0.00%] [G loss: 0.327340]\n",
            "Epoch: 68.40 [D loss: 1.306758, acc: 0.00%] [G loss: 0.327053]\n",
            "Epoch: 68.41 [D loss: 1.269163, acc: 0.00%] [G loss: 0.329341]\n",
            "Epoch: 68.42 [D loss: 1.333842, acc: 0.00%] [G loss: 0.327929]\n",
            "Epoch: 68.43 [D loss: 1.388773, acc: 0.00%] [G loss: 0.327213]\n",
            "Epoch: 68.44 [D loss: 1.356661, acc: 0.00%] [G loss: 0.327090]\n",
            "Epoch: 68.45 [D loss: 1.415845, acc: 0.00%] [G loss: 0.327395]\n",
            "Epoch: 68.46 [D loss: 1.398704, acc: 0.00%] [G loss: 0.328881]\n",
            "Epoch: 68.47 [D loss: 1.428082, acc: 0.00%] [G loss: 0.326430]\n",
            "Epoch: 68.48 [D loss: 1.340843, acc: 0.00%] [G loss: 0.326390]\n",
            "Epoch: 68.49 [D loss: 1.363997, acc: 0.00%] [G loss: 0.326948]\n",
            "Epoch: 68.50 [D loss: 1.344241, acc: 0.00%] [G loss: 0.326573]\n",
            "Epoch: 68.51 [D loss: 1.379172, acc: 0.00%] [G loss: 0.327556]\n",
            "Epoch: 68.52 [D loss: 1.338079, acc: 0.00%] [G loss: 0.328315]\n",
            "Epoch: 68.53 [D loss: 1.348474, acc: 0.00%] [G loss: 0.328795]\n",
            "Epoch: 68.54 [D loss: 1.371698, acc: 0.00%] [G loss: 0.327780]\n",
            "Epoch: 68.55 [D loss: 1.347917, acc: 0.00%] [G loss: 0.327398]\n",
            "Epoch: 68.56 [D loss: 1.354361, acc: 0.00%] [G loss: 0.326938]\n",
            "Epoch: 68.57 [D loss: 1.331440, acc: 0.00%] [G loss: 0.326539]\n",
            "Epoch: 68.58 [D loss: 1.325904, acc: 0.00%] [G loss: 0.327141]\n",
            "Epoch: 68.59 [D loss: 1.362726, acc: 0.00%] [G loss: 0.327880]\n",
            "Epoch: 68.60 [D loss: 1.325755, acc: 0.00%] [G loss: 0.327067]\n",
            "Epoch: 68.61 [D loss: 1.362668, acc: 0.00%] [G loss: 0.327938]\n",
            "Epoch: 68.62 [D loss: 1.338186, acc: 0.00%] [G loss: 0.328120]\n",
            "Epoch: 68.63 [D loss: 1.337428, acc: 0.00%] [G loss: 0.327822]\n",
            "Epoch: 68.64 [D loss: 1.326969, acc: 0.00%] [G loss: 0.328103]\n",
            "Epoch: 68.65 [D loss: 1.355029, acc: 0.00%] [G loss: 0.328448]\n",
            "Epoch: 68.66 [D loss: 1.375873, acc: 0.00%] [G loss: 0.328552]\n",
            "Epoch: 68.67 [D loss: 1.427772, acc: 0.00%] [G loss: 0.328598]\n",
            "Epoch: 68.68 [D loss: 1.387343, acc: 0.00%] [G loss: 0.328058]\n",
            "Epoch: 68.69 [D loss: 1.414738, acc: 0.00%] [G loss: 0.327225]\n",
            "Epoch: 68.70 [D loss: 1.428350, acc: 0.00%] [G loss: 0.328057]\n",
            "Epoch: 68.71 [D loss: 1.385942, acc: 0.00%] [G loss: 0.327892]\n",
            "Epoch: 68.72 [D loss: 1.378451, acc: 0.00%] [G loss: 0.327985]\n",
            "Epoch: 68.73 [D loss: 1.386940, acc: 0.00%] [G loss: 0.327673]\n",
            "Epoch: 68.74 [D loss: 1.322874, acc: 0.00%] [G loss: 0.327040]\n",
            "Epoch: 68.75 [D loss: 1.343347, acc: 0.00%] [G loss: 0.326606]\n",
            "Epoch: 68.76 [D loss: 1.329921, acc: 0.00%] [G loss: 0.327169]\n",
            "Epoch: 68.77 [D loss: 1.371401, acc: 0.00%] [G loss: 0.329136]\n",
            "Epoch: 68.78 [D loss: 1.354138, acc: 0.00%] [G loss: 0.327875]\n",
            "Epoch: 68.79 [D loss: 1.369733, acc: 0.00%] [G loss: 0.327853]\n",
            "Epoch: 68.80 [D loss: 1.391082, acc: 0.00%] [G loss: 0.327247]\n",
            "Epoch: 68.81 [D loss: 1.388914, acc: 0.00%] [G loss: 0.326418]\n",
            "Epoch: 68.82 [D loss: 1.399866, acc: 0.00%] [G loss: 0.327713]\n",
            "Epoch: 68.83 [D loss: 1.429927, acc: 0.00%] [G loss: 0.328970]\n",
            "Epoch: 68.84 [D loss: 1.416105, acc: 0.00%] [G loss: 0.326926]\n",
            "Epoch: 68.85 [D loss: 1.369745, acc: 0.00%] [G loss: 0.327541]\n",
            "Epoch: 68.86 [D loss: 1.347994, acc: 0.00%] [G loss: 0.328278]\n",
            "Epoch: 68.87 [D loss: 1.362743, acc: 0.00%] [G loss: 0.326920]\n",
            "Epoch: 68.88 [D loss: 1.364093, acc: 0.00%] [G loss: 0.327854]\n",
            "Epoch: 68.89 [D loss: 1.364165, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 68.90 [D loss: 1.360511, acc: 0.00%] [G loss: 0.326592]\n",
            "Epoch: 68.91 [D loss: 1.359424, acc: 0.00%] [G loss: 0.328581]\n",
            "Epoch: 68.92 [D loss: 1.406976, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 68.93 [D loss: 1.366616, acc: 0.00%] [G loss: 0.327152]\n",
            "Epoch: 68.94 [D loss: 1.397781, acc: 0.00%] [G loss: 0.329272]\n",
            "Epoch: 68.95 [D loss: 1.341945, acc: 0.00%] [G loss: 0.328158]\n",
            "Epoch: 68.96 [D loss: 1.384009, acc: 0.00%] [G loss: 0.328583]\n",
            "Epoch: 68.97 [D loss: 1.375801, acc: 0.00%] [G loss: 0.328061]\n",
            "Epoch: 68.98 [D loss: 1.375123, acc: 0.00%] [G loss: 0.328443]\n",
            "Epoch: 68.99 [D loss: 1.425069, acc: 0.00%] [G loss: 0.329131]\n",
            "Epoch: 68.100 [D loss: 1.396136, acc: 0.00%] [G loss: 0.329203]\n",
            "Epoch: 68.101 [D loss: 1.418032, acc: 0.00%] [G loss: 0.327112]\n",
            "Epoch: 68.102 [D loss: 1.329102, acc: 0.00%] [G loss: 0.326933]\n",
            "Epoch: 68.103 [D loss: 1.391132, acc: 0.00%] [G loss: 0.327982]\n",
            "Epoch: 68.104 [D loss: 1.349828, acc: 0.00%] [G loss: 0.327563]\n",
            "Epoch: 68.105 [D loss: 1.320649, acc: 0.00%] [G loss: 0.329191]\n",
            "Epoch: 68.106 [D loss: 1.334363, acc: 0.00%] [G loss: 0.328479]\n",
            "Epoch: 68.107 [D loss: 1.332378, acc: 0.00%] [G loss: 0.328568]\n",
            "Epoch: 69.0 [D loss: 1.346021, acc: 0.00%] [G loss: 0.327325]\n",
            "Epoch: 69.1 [D loss: 1.399399, acc: 0.00%] [G loss: 0.327648]\n",
            "Epoch: 69.2 [D loss: 1.468840, acc: 0.00%] [G loss: 0.327211]\n",
            "Epoch: 69.3 [D loss: 1.379841, acc: 0.00%] [G loss: 0.327360]\n",
            "Epoch: 69.4 [D loss: 1.380967, acc: 0.00%] [G loss: 0.326447]\n",
            "Epoch: 69.5 [D loss: 1.357904, acc: 0.00%] [G loss: 0.328405]\n",
            "Epoch: 69.6 [D loss: 1.368611, acc: 0.00%] [G loss: 0.327198]\n",
            "Epoch: 69.7 [D loss: 1.409567, acc: 0.00%] [G loss: 0.329177]\n",
            "Epoch: 69.8 [D loss: 1.352412, acc: 0.00%] [G loss: 0.329050]\n",
            "Epoch: 69.9 [D loss: 1.375434, acc: 0.00%] [G loss: 0.328673]\n",
            "Epoch: 69.10 [D loss: 1.435167, acc: 0.00%] [G loss: 0.328513]\n",
            "Epoch: 69.11 [D loss: 1.425835, acc: 0.00%] [G loss: 0.327140]\n",
            "Epoch: 69.12 [D loss: 1.394941, acc: 0.00%] [G loss: 0.327464]\n",
            "Epoch: 69.13 [D loss: 1.411344, acc: 0.00%] [G loss: 0.327820]\n",
            "Epoch: 69.14 [D loss: 1.382718, acc: 0.00%] [G loss: 0.329276]\n",
            "Epoch: 69.15 [D loss: 1.363812, acc: 0.00%] [G loss: 0.327501]\n",
            "Epoch: 69.16 [D loss: 1.344453, acc: 0.00%] [G loss: 0.327845]\n",
            "Epoch: 69.17 [D loss: 1.329164, acc: 0.00%] [G loss: 0.329274]\n",
            "Epoch: 69.18 [D loss: 1.384910, acc: 0.00%] [G loss: 0.327116]\n",
            "Epoch: 69.19 [D loss: 1.342920, acc: 0.00%] [G loss: 0.326605]\n",
            "Epoch: 69.20 [D loss: 1.384046, acc: 0.00%] [G loss: 0.327819]\n",
            "Epoch: 69.21 [D loss: 1.371973, acc: 0.00%] [G loss: 0.329647]\n",
            "Epoch: 69.22 [D loss: 1.349292, acc: 0.00%] [G loss: 0.329778]\n",
            "Epoch: 69.23 [D loss: 1.363492, acc: 0.00%] [G loss: 0.329015]\n",
            "Epoch: 69.24 [D loss: 1.378267, acc: 0.00%] [G loss: 0.328544]\n",
            "Epoch: 69.25 [D loss: 1.409384, acc: 0.00%] [G loss: 0.327882]\n",
            "Epoch: 69.26 [D loss: 1.365160, acc: 0.00%] [G loss: 0.328347]\n",
            "Epoch: 69.27 [D loss: 1.373122, acc: 0.00%] [G loss: 0.328268]\n",
            "Epoch: 69.28 [D loss: 1.396584, acc: 0.00%] [G loss: 0.329394]\n",
            "Epoch: 69.29 [D loss: 1.365897, acc: 0.00%] [G loss: 0.327713]\n",
            "Epoch: 69.30 [D loss: 1.296439, acc: 0.00%] [G loss: 0.327107]\n",
            "Epoch: 69.31 [D loss: 1.293183, acc: 0.00%] [G loss: 0.327230]\n",
            "Epoch: 69.32 [D loss: 1.297742, acc: 0.00%] [G loss: 0.327976]\n",
            "Epoch: 69.33 [D loss: 1.297859, acc: 0.00%] [G loss: 0.327196]\n",
            "Epoch: 69.34 [D loss: 1.366158, acc: 0.00%] [G loss: 0.328775]\n",
            "Epoch: 69.35 [D loss: 1.357253, acc: 0.00%] [G loss: 0.330497]\n",
            "Epoch: 69.36 [D loss: 1.409609, acc: 0.00%] [G loss: 0.327331]\n",
            "Epoch: 69.37 [D loss: 1.368169, acc: 0.00%] [G loss: 0.327754]\n",
            "Epoch: 69.38 [D loss: 1.419673, acc: 0.00%] [G loss: 0.328512]\n",
            "Epoch: 69.39 [D loss: 1.450088, acc: 0.00%] [G loss: 0.329036]\n",
            "Epoch: 69.40 [D loss: 1.460941, acc: 0.00%] [G loss: 0.327807]\n",
            "Epoch: 69.41 [D loss: 1.421576, acc: 0.00%] [G loss: 0.327661]\n",
            "Epoch: 69.42 [D loss: 1.352877, acc: 0.00%] [G loss: 0.327164]\n",
            "Epoch: 69.43 [D loss: 1.385691, acc: 0.00%] [G loss: 0.327351]\n",
            "Epoch: 69.44 [D loss: 1.361003, acc: 0.00%] [G loss: 0.330349]\n",
            "Epoch: 69.45 [D loss: 1.300962, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 69.46 [D loss: 1.397159, acc: 0.00%] [G loss: 0.328171]\n",
            "Epoch: 69.47 [D loss: 1.342985, acc: 0.00%] [G loss: 0.327722]\n",
            "Epoch: 69.48 [D loss: 1.391329, acc: 0.00%] [G loss: 0.327955]\n",
            "Epoch: 69.49 [D loss: 1.373038, acc: 0.00%] [G loss: 0.327617]\n",
            "Epoch: 69.50 [D loss: 1.330108, acc: 0.00%] [G loss: 0.327326]\n",
            "Epoch: 69.51 [D loss: 1.385034, acc: 0.00%] [G loss: 0.328613]\n",
            "Epoch: 69.52 [D loss: 1.403774, acc: 0.00%] [G loss: 0.328686]\n",
            "Epoch: 69.53 [D loss: 1.355527, acc: 0.00%] [G loss: 0.326494]\n",
            "Epoch: 69.54 [D loss: 1.402438, acc: 0.00%] [G loss: 0.326692]\n",
            "Epoch: 69.55 [D loss: 1.351790, acc: 0.00%] [G loss: 0.328141]\n",
            "Epoch: 69.56 [D loss: 1.417485, acc: 0.00%] [G loss: 0.328892]\n",
            "Epoch: 69.57 [D loss: 1.393981, acc: 0.00%] [G loss: 0.328730]\n",
            "Epoch: 69.58 [D loss: 1.388994, acc: 0.00%] [G loss: 0.329153]\n",
            "Epoch: 69.59 [D loss: 1.359508, acc: 0.00%] [G loss: 0.327886]\n",
            "Epoch: 69.60 [D loss: 1.368273, acc: 0.00%] [G loss: 0.326702]\n",
            "Epoch: 69.61 [D loss: 1.309573, acc: 0.00%] [G loss: 0.328041]\n",
            "Epoch: 69.62 [D loss: 1.344314, acc: 0.00%] [G loss: 0.328206]\n",
            "Epoch: 69.63 [D loss: 1.354941, acc: 0.00%] [G loss: 0.329004]\n",
            "Epoch: 69.64 [D loss: 1.338070, acc: 0.00%] [G loss: 0.328781]\n",
            "Epoch: 69.65 [D loss: 1.357790, acc: 0.00%] [G loss: 0.326752]\n",
            "Epoch: 69.66 [D loss: 1.371301, acc: 0.00%] [G loss: 0.327304]\n",
            "Epoch: 69.67 [D loss: 1.377702, acc: 0.00%] [G loss: 0.330337]\n",
            "Epoch: 69.68 [D loss: 1.377243, acc: 0.00%] [G loss: 0.327365]\n",
            "Epoch: 69.69 [D loss: 1.398013, acc: 0.00%] [G loss: 0.327531]\n",
            "Epoch: 69.70 [D loss: 1.382958, acc: 0.00%] [G loss: 0.328644]\n",
            "Epoch: 69.71 [D loss: 1.338658, acc: 0.00%] [G loss: 0.329750]\n",
            "Epoch: 69.72 [D loss: 1.350644, acc: 0.00%] [G loss: 0.328472]\n",
            "Epoch: 69.73 [D loss: 1.301696, acc: 0.00%] [G loss: 0.328106]\n",
            "Epoch: 69.74 [D loss: 1.362733, acc: 0.00%] [G loss: 0.329990]\n",
            "Epoch: 69.75 [D loss: 1.332418, acc: 0.00%] [G loss: 0.328795]\n",
            "Epoch: 69.76 [D loss: 1.335496, acc: 0.00%] [G loss: 0.326775]\n",
            "Epoch: 69.77 [D loss: 1.351618, acc: 0.00%] [G loss: 0.328292]\n",
            "Epoch: 69.78 [D loss: 1.365455, acc: 0.00%] [G loss: 0.327561]\n",
            "Epoch: 69.79 [D loss: 1.397818, acc: 0.00%] [G loss: 0.327199]\n",
            "Epoch: 69.80 [D loss: 1.424142, acc: 0.00%] [G loss: 0.328332]\n",
            "Epoch: 69.81 [D loss: 1.385762, acc: 0.00%] [G loss: 0.327628]\n",
            "Epoch: 69.82 [D loss: 1.376578, acc: 0.00%] [G loss: 0.329010]\n",
            "Epoch: 69.83 [D loss: 1.394217, acc: 0.00%] [G loss: 0.326944]\n",
            "Epoch: 69.84 [D loss: 1.374528, acc: 0.00%] [G loss: 0.329096]\n",
            "Epoch: 69.85 [D loss: 1.340528, acc: 0.00%] [G loss: 0.327382]\n",
            "Epoch: 69.86 [D loss: 1.332297, acc: 0.00%] [G loss: 0.327829]\n",
            "Epoch: 69.87 [D loss: 1.426693, acc: 0.00%] [G loss: 0.328298]\n",
            "Epoch: 69.88 [D loss: 1.375119, acc: 0.00%] [G loss: 0.327459]\n",
            "Epoch: 69.89 [D loss: 1.369032, acc: 0.00%] [G loss: 0.328498]\n",
            "Epoch: 69.90 [D loss: 1.427338, acc: 0.00%] [G loss: 0.327417]\n",
            "Epoch: 69.91 [D loss: 1.351063, acc: 0.00%] [G loss: 0.328635]\n",
            "Epoch: 69.92 [D loss: 1.406586, acc: 0.00%] [G loss: 0.329016]\n",
            "Epoch: 69.93 [D loss: 1.370466, acc: 0.00%] [G loss: 0.328156]\n",
            "Epoch: 69.94 [D loss: 1.382458, acc: 0.00%] [G loss: 0.327093]\n",
            "Epoch: 69.95 [D loss: 1.354481, acc: 0.00%] [G loss: 0.327139]\n",
            "Epoch: 69.96 [D loss: 1.374636, acc: 0.00%] [G loss: 0.327123]\n",
            "Epoch: 69.97 [D loss: 1.443544, acc: 0.00%] [G loss: 0.328172]\n",
            "Epoch: 69.98 [D loss: 1.429488, acc: 0.00%] [G loss: 0.327561]\n",
            "Epoch: 69.99 [D loss: 1.395951, acc: 0.00%] [G loss: 0.327977]\n",
            "Epoch: 69.100 [D loss: 1.326826, acc: 0.00%] [G loss: 0.329105]\n",
            "Epoch: 69.101 [D loss: 1.352823, acc: 0.00%] [G loss: 0.327927]\n",
            "Epoch: 69.102 [D loss: 1.290057, acc: 0.00%] [G loss: 0.327207]\n",
            "Epoch: 69.103 [D loss: 1.379845, acc: 0.00%] [G loss: 0.326587]\n",
            "Epoch: 69.104 [D loss: 1.388393, acc: 0.00%] [G loss: 0.328723]\n",
            "Epoch: 69.105 [D loss: 1.357063, acc: 0.00%] [G loss: 0.326543]\n",
            "Epoch: 69.106 [D loss: 1.366565, acc: 0.00%] [G loss: 0.327574]\n",
            "Epoch: 69.107 [D loss: 1.374613, acc: 0.00%] [G loss: 0.326676]\n",
            "Epoch: 70.0 [D loss: 1.344640, acc: 0.00%] [G loss: 0.327276]\n",
            "Epoch: 70.1 [D loss: 1.284120, acc: 0.00%] [G loss: 0.331425]\n",
            "Epoch: 70.2 [D loss: 1.345503, acc: 0.00%] [G loss: 0.328352]\n",
            "Epoch: 70.3 [D loss: 1.387002, acc: 0.00%] [G loss: 0.327149]\n",
            "Epoch: 70.4 [D loss: 1.398662, acc: 0.00%] [G loss: 0.327716]\n",
            "Epoch: 70.5 [D loss: 1.385061, acc: 0.00%] [G loss: 0.330527]\n",
            "Epoch: 70.6 [D loss: 1.394568, acc: 0.00%] [G loss: 0.327352]\n",
            "Epoch: 70.7 [D loss: 1.381771, acc: 0.00%] [G loss: 0.331162]\n",
            "Epoch: 70.8 [D loss: 1.333102, acc: 0.00%] [G loss: 0.327625]\n",
            "Epoch: 70.9 [D loss: 1.348839, acc: 0.00%] [G loss: 0.326764]\n",
            "Epoch: 70.10 [D loss: 1.334488, acc: 0.00%] [G loss: 0.329158]\n",
            "Epoch: 70.11 [D loss: 1.314079, acc: 0.00%] [G loss: 0.327375]\n",
            "Epoch: 70.12 [D loss: 1.363903, acc: 0.00%] [G loss: 0.329836]\n",
            "Epoch: 70.13 [D loss: 1.379491, acc: 0.00%] [G loss: 0.326781]\n",
            "Epoch: 70.14 [D loss: 1.400945, acc: 0.00%] [G loss: 0.327252]\n",
            "Epoch: 70.15 [D loss: 1.412858, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 70.16 [D loss: 1.418374, acc: 0.00%] [G loss: 0.328014]\n",
            "Epoch: 70.17 [D loss: 1.379237, acc: 0.00%] [G loss: 0.327309]\n",
            "Epoch: 70.18 [D loss: 1.371216, acc: 0.00%] [G loss: 0.328665]\n",
            "Epoch: 70.19 [D loss: 1.377664, acc: 0.00%] [G loss: 0.327151]\n",
            "Epoch: 70.20 [D loss: 1.407348, acc: 0.00%] [G loss: 0.327083]\n",
            "Epoch: 70.21 [D loss: 1.323463, acc: 0.00%] [G loss: 0.328462]\n",
            "Epoch: 70.22 [D loss: 1.328573, acc: 0.00%] [G loss: 0.327964]\n",
            "Epoch: 70.23 [D loss: 1.361632, acc: 0.00%] [G loss: 0.328705]\n",
            "Epoch: 70.24 [D loss: 1.342971, acc: 0.00%] [G loss: 0.328525]\n",
            "Epoch: 70.25 [D loss: 1.392108, acc: 0.00%] [G loss: 0.327614]\n",
            "Epoch: 70.26 [D loss: 1.352683, acc: 0.00%] [G loss: 0.327866]\n",
            "Epoch: 70.27 [D loss: 1.475509, acc: 0.00%] [G loss: 0.329568]\n",
            "Epoch: 70.28 [D loss: 1.501919, acc: 0.00%] [G loss: 0.330781]\n",
            "Epoch: 70.29 [D loss: 1.453943, acc: 0.00%] [G loss: 0.330087]\n",
            "Epoch: 70.30 [D loss: 1.373732, acc: 0.00%] [G loss: 0.327787]\n",
            "Epoch: 70.31 [D loss: 1.300477, acc: 0.00%] [G loss: 0.327637]\n",
            "Epoch: 70.32 [D loss: 1.294979, acc: 0.00%] [G loss: 0.328215]\n",
            "Epoch: 70.33 [D loss: 1.231627, acc: 0.00%] [G loss: 0.327200]\n",
            "Epoch: 70.34 [D loss: 1.297166, acc: 0.00%] [G loss: 0.332151]\n",
            "Epoch: 70.35 [D loss: 1.328667, acc: 0.00%] [G loss: 0.330361]\n",
            "Epoch: 70.36 [D loss: 1.402691, acc: 0.00%] [G loss: 0.327856]\n",
            "Epoch: 70.37 [D loss: 1.409076, acc: 0.00%] [G loss: 0.327468]\n",
            "Epoch: 70.38 [D loss: 1.462657, acc: 0.00%] [G loss: 0.330590]\n",
            "Epoch: 70.39 [D loss: 1.423548, acc: 0.00%] [G loss: 0.329471]\n",
            "Epoch: 70.40 [D loss: 1.417329, acc: 0.00%] [G loss: 0.328151]\n",
            "Epoch: 70.41 [D loss: 1.413405, acc: 0.00%] [G loss: 0.328121]\n",
            "Epoch: 70.42 [D loss: 1.363735, acc: 0.00%] [G loss: 0.327668]\n",
            "Epoch: 70.43 [D loss: 1.346715, acc: 0.00%] [G loss: 0.328035]\n",
            "Epoch: 70.44 [D loss: 1.355998, acc: 0.00%] [G loss: 0.329163]\n",
            "Epoch: 70.45 [D loss: 1.330992, acc: 0.00%] [G loss: 0.328001]\n",
            "Epoch: 70.46 [D loss: 1.321982, acc: 0.00%] [G loss: 0.327779]\n",
            "Epoch: 70.47 [D loss: 1.369251, acc: 0.00%] [G loss: 0.329520]\n",
            "Epoch: 70.48 [D loss: 1.356106, acc: 0.00%] [G loss: 0.330099]\n",
            "Epoch: 70.49 [D loss: 1.407703, acc: 0.00%] [G loss: 0.328065]\n",
            "Epoch: 70.50 [D loss: 1.426149, acc: 0.00%] [G loss: 0.328565]\n",
            "Epoch: 70.51 [D loss: 1.522206, acc: 0.00%] [G loss: 0.329349]\n",
            "Epoch: 70.52 [D loss: 1.454371, acc: 0.00%] [G loss: 0.329796]\n",
            "Epoch: 70.53 [D loss: 1.474022, acc: 0.00%] [G loss: 0.328261]\n",
            "Epoch: 70.54 [D loss: 1.367779, acc: 0.00%] [G loss: 0.329410]\n",
            "Epoch: 70.55 [D loss: 1.368909, acc: 0.00%] [G loss: 0.327208]\n",
            "Epoch: 70.56 [D loss: 1.326330, acc: 0.00%] [G loss: 0.327131]\n",
            "Epoch: 70.57 [D loss: 1.299894, acc: 0.00%] [G loss: 0.327671]\n",
            "Epoch: 70.58 [D loss: 1.268941, acc: 0.00%] [G loss: 0.329761]\n",
            "Epoch: 70.59 [D loss: 1.323485, acc: 0.00%] [G loss: 0.327736]\n",
            "Epoch: 70.60 [D loss: 1.314907, acc: 0.00%] [G loss: 0.327040]\n",
            "Epoch: 70.61 [D loss: 1.320789, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 70.62 [D loss: 1.365301, acc: 0.00%] [G loss: 0.327790]\n",
            "Epoch: 70.63 [D loss: 1.428102, acc: 0.00%] [G loss: 0.327687]\n",
            "Epoch: 70.64 [D loss: 1.424674, acc: 0.00%] [G loss: 0.329126]\n",
            "Epoch: 70.65 [D loss: 1.423855, acc: 0.00%] [G loss: 0.328936]\n",
            "Epoch: 70.66 [D loss: 1.433654, acc: 0.00%] [G loss: 0.330100]\n",
            "Epoch: 70.67 [D loss: 1.353211, acc: 0.00%] [G loss: 0.326984]\n",
            "Epoch: 70.68 [D loss: 1.371843, acc: 0.00%] [G loss: 0.327362]\n",
            "Epoch: 70.69 [D loss: 1.357227, acc: 0.00%] [G loss: 0.327685]\n",
            "Epoch: 70.70 [D loss: 1.303224, acc: 0.00%] [G loss: 0.326964]\n",
            "Epoch: 70.71 [D loss: 1.328965, acc: 0.00%] [G loss: 0.328269]\n",
            "Epoch: 70.72 [D loss: 1.346885, acc: 0.00%] [G loss: 0.327106]\n",
            "Epoch: 70.73 [D loss: 1.352748, acc: 0.00%] [G loss: 0.328376]\n",
            "Epoch: 70.74 [D loss: 1.405195, acc: 0.00%] [G loss: 0.328757]\n",
            "Epoch: 70.75 [D loss: 1.364043, acc: 0.00%] [G loss: 0.327889]\n",
            "Epoch: 70.76 [D loss: 1.391055, acc: 0.00%] [G loss: 0.329385]\n",
            "Epoch: 70.77 [D loss: 1.348600, acc: 0.00%] [G loss: 0.328186]\n",
            "Epoch: 70.78 [D loss: 1.343191, acc: 0.00%] [G loss: 0.327801]\n",
            "Epoch: 70.79 [D loss: 1.327633, acc: 0.00%] [G loss: 0.327060]\n",
            "Epoch: 70.80 [D loss: 1.323883, acc: 0.00%] [G loss: 0.327087]\n",
            "Epoch: 70.81 [D loss: 1.411039, acc: 0.00%] [G loss: 0.327130]\n",
            "Epoch: 70.82 [D loss: 1.348782, acc: 0.00%] [G loss: 0.327794]\n",
            "Epoch: 70.83 [D loss: 1.361807, acc: 0.00%] [G loss: 0.327125]\n",
            "Epoch: 70.84 [D loss: 1.358075, acc: 0.00%] [G loss: 0.329206]\n",
            "Epoch: 70.85 [D loss: 1.281605, acc: 0.00%] [G loss: 0.328552]\n",
            "Epoch: 70.86 [D loss: 1.325814, acc: 0.00%] [G loss: 0.326821]\n",
            "Epoch: 70.87 [D loss: 1.330913, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 70.88 [D loss: 1.372658, acc: 0.00%] [G loss: 0.329007]\n",
            "Epoch: 70.89 [D loss: 1.324973, acc: 0.00%] [G loss: 0.328208]\n",
            "Epoch: 70.90 [D loss: 1.336901, acc: 0.00%] [G loss: 0.327494]\n",
            "Epoch: 70.91 [D loss: 1.411167, acc: 0.00%] [G loss: 0.328791]\n",
            "Epoch: 70.92 [D loss: 1.380684, acc: 0.00%] [G loss: 0.328048]\n",
            "Epoch: 70.93 [D loss: 1.414289, acc: 0.00%] [G loss: 0.328196]\n",
            "Epoch: 70.94 [D loss: 1.415891, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 70.95 [D loss: 1.304003, acc: 0.00%] [G loss: 0.328905]\n",
            "Epoch: 70.96 [D loss: 1.312110, acc: 0.00%] [G loss: 0.328241]\n",
            "Epoch: 70.97 [D loss: 1.334393, acc: 0.00%] [G loss: 0.328894]\n",
            "Epoch: 70.98 [D loss: 1.302798, acc: 0.00%] [G loss: 0.327455]\n",
            "Epoch: 70.99 [D loss: 1.383959, acc: 0.00%] [G loss: 0.329040]\n",
            "Epoch: 70.100 [D loss: 1.367632, acc: 0.00%] [G loss: 0.328105]\n",
            "Epoch: 70.101 [D loss: 1.351988, acc: 0.00%] [G loss: 0.326505]\n",
            "Epoch: 70.102 [D loss: 1.399834, acc: 0.00%] [G loss: 0.327947]\n",
            "Epoch: 70.103 [D loss: 1.409867, acc: 0.00%] [G loss: 0.328696]\n",
            "Epoch: 70.104 [D loss: 1.399824, acc: 0.00%] [G loss: 0.328003]\n",
            "Epoch: 70.105 [D loss: 1.364305, acc: 0.00%] [G loss: 0.327751]\n",
            "Epoch: 70.106 [D loss: 1.364684, acc: 0.00%] [G loss: 0.328789]\n",
            "Epoch: 70.107 [D loss: 1.375597, acc: 0.00%] [G loss: 0.327766]\n",
            "INFO:tensorflow:Assets written to: GAN_weights/dis_0.00000330_weights/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: GAN_weights/gen_0.00000330_weights/assets\n",
            "Epoch: 71.0 [D loss: 1.371590, acc: 0.00%] [G loss: 0.327928]\n",
            "Epoch: 71.1 [D loss: 1.328906, acc: 0.00%] [G loss: 0.326129]\n",
            "Epoch: 71.2 [D loss: 1.345044, acc: 0.00%] [G loss: 0.327969]\n",
            "Epoch: 71.3 [D loss: 1.372976, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 71.4 [D loss: 1.397406, acc: 0.00%] [G loss: 0.328304]\n",
            "Epoch: 71.5 [D loss: 1.381790, acc: 0.00%] [G loss: 0.327923]\n",
            "Epoch: 71.6 [D loss: 1.364999, acc: 0.00%] [G loss: 0.327919]\n",
            "Epoch: 71.7 [D loss: 1.355542, acc: 0.00%] [G loss: 0.327698]\n",
            "Epoch: 71.8 [D loss: 1.347402, acc: 0.00%] [G loss: 0.327222]\n",
            "Epoch: 71.9 [D loss: 1.377037, acc: 0.00%] [G loss: 0.327224]\n",
            "Epoch: 71.10 [D loss: 1.418876, acc: 0.00%] [G loss: 0.327707]\n",
            "Epoch: 71.11 [D loss: 1.334832, acc: 0.00%] [G loss: 0.326283]\n",
            "Epoch: 71.12 [D loss: 1.358185, acc: 0.00%] [G loss: 0.326140]\n",
            "Epoch: 71.13 [D loss: 1.383500, acc: 0.00%] [G loss: 0.327370]\n",
            "Epoch: 71.14 [D loss: 1.414228, acc: 0.00%] [G loss: 0.328666]\n",
            "Epoch: 71.15 [D loss: 1.381831, acc: 0.00%] [G loss: 0.326088]\n",
            "Epoch: 71.16 [D loss: 1.401400, acc: 0.00%] [G loss: 0.327448]\n",
            "Epoch: 71.17 [D loss: 1.352334, acc: 0.00%] [G loss: 0.328401]\n",
            "Epoch: 71.18 [D loss: 1.393974, acc: 0.00%] [G loss: 0.327265]\n",
            "Epoch: 71.19 [D loss: 1.407289, acc: 0.00%] [G loss: 0.329609]\n",
            "Epoch: 71.20 [D loss: 1.355531, acc: 0.00%] [G loss: 0.327736]\n",
            "Epoch: 71.21 [D loss: 1.360506, acc: 0.00%] [G loss: 0.328100]\n",
            "Epoch: 71.22 [D loss: 1.346284, acc: 0.00%] [G loss: 0.327177]\n",
            "Epoch: 71.23 [D loss: 1.310643, acc: 0.00%] [G loss: 0.328349]\n",
            "Epoch: 71.24 [D loss: 1.347691, acc: 0.00%] [G loss: 0.327950]\n",
            "Epoch: 71.25 [D loss: 1.344026, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 71.26 [D loss: 1.345594, acc: 0.00%] [G loss: 0.327094]\n",
            "Epoch: 71.27 [D loss: 1.363743, acc: 0.00%] [G loss: 0.326655]\n",
            "Epoch: 71.28 [D loss: 1.359896, acc: 0.00%] [G loss: 0.327809]\n",
            "Epoch: 71.29 [D loss: 1.363399, acc: 0.00%] [G loss: 0.327747]\n",
            "Epoch: 71.30 [D loss: 1.353384, acc: 0.00%] [G loss: 0.327391]\n",
            "Epoch: 71.31 [D loss: 1.327890, acc: 0.00%] [G loss: 0.327309]\n",
            "Epoch: 71.32 [D loss: 1.335229, acc: 0.00%] [G loss: 0.327280]\n",
            "Epoch: 71.33 [D loss: 1.367971, acc: 0.00%] [G loss: 0.327951]\n",
            "Epoch: 71.34 [D loss: 1.339962, acc: 0.00%] [G loss: 0.327810]\n",
            "Epoch: 71.35 [D loss: 1.362242, acc: 0.00%] [G loss: 0.327841]\n",
            "Epoch: 71.36 [D loss: 1.383190, acc: 0.00%] [G loss: 0.326466]\n",
            "Epoch: 71.37 [D loss: 1.373683, acc: 0.00%] [G loss: 0.327932]\n",
            "Epoch: 71.38 [D loss: 1.427719, acc: 0.00%] [G loss: 0.328137]\n",
            "Epoch: 71.39 [D loss: 1.374340, acc: 0.00%] [G loss: 0.329205]\n",
            "Epoch: 71.40 [D loss: 1.355666, acc: 0.00%] [G loss: 0.328928]\n",
            "Epoch: 71.41 [D loss: 1.388515, acc: 0.00%] [G loss: 0.328856]\n",
            "Epoch: 71.42 [D loss: 1.354612, acc: 0.00%] [G loss: 0.328991]\n",
            "Epoch: 71.43 [D loss: 1.362017, acc: 0.00%] [G loss: 0.327774]\n",
            "Epoch: 71.44 [D loss: 1.356521, acc: 0.00%] [G loss: 0.327866]\n",
            "Epoch: 71.45 [D loss: 1.428081, acc: 0.00%] [G loss: 0.327121]\n",
            "Epoch: 71.46 [D loss: 1.455171, acc: 0.00%] [G loss: 0.328691]\n",
            "Epoch: 71.47 [D loss: 1.428492, acc: 0.00%] [G loss: 0.330669]\n",
            "Epoch: 71.48 [D loss: 1.357609, acc: 0.00%] [G loss: 0.328669]\n",
            "Epoch: 71.49 [D loss: 1.395348, acc: 0.00%] [G loss: 0.327752]\n",
            "Epoch: 71.50 [D loss: 1.330509, acc: 0.00%] [G loss: 0.329615]\n",
            "Epoch: 71.51 [D loss: 1.365227, acc: 0.00%] [G loss: 0.327370]\n",
            "Epoch: 71.52 [D loss: 1.331000, acc: 0.00%] [G loss: 0.328377]\n",
            "Epoch: 71.53 [D loss: 1.371143, acc: 0.00%] [G loss: 0.327706]\n",
            "Epoch: 71.54 [D loss: 1.381702, acc: 0.00%] [G loss: 0.327820]\n",
            "Epoch: 71.55 [D loss: 1.374738, acc: 0.00%] [G loss: 0.327722]\n",
            "Epoch: 71.56 [D loss: 1.350566, acc: 0.00%] [G loss: 0.327540]\n",
            "Epoch: 71.57 [D loss: 1.370239, acc: 0.00%] [G loss: 0.326804]\n",
            "Epoch: 71.58 [D loss: 1.407861, acc: 0.00%] [G loss: 0.327321]\n",
            "Epoch: 71.59 [D loss: 1.319866, acc: 0.00%] [G loss: 0.326735]\n",
            "Epoch: 71.60 [D loss: 1.383685, acc: 0.00%] [G loss: 0.328112]\n",
            "Epoch: 71.61 [D loss: 1.337232, acc: 0.00%] [G loss: 0.328528]\n",
            "Epoch: 71.62 [D loss: 1.317055, acc: 0.00%] [G loss: 0.328148]\n",
            "Epoch: 71.63 [D loss: 1.372269, acc: 0.00%] [G loss: 0.327280]\n",
            "Epoch: 71.64 [D loss: 1.345871, acc: 0.00%] [G loss: 0.326474]\n",
            "Epoch: 71.65 [D loss: 1.388796, acc: 0.00%] [G loss: 0.329292]\n",
            "Epoch: 71.66 [D loss: 1.390544, acc: 0.00%] [G loss: 0.328580]\n",
            "Epoch: 71.67 [D loss: 1.353503, acc: 0.00%] [G loss: 0.326790]\n",
            "Epoch: 71.68 [D loss: 1.384173, acc: 0.00%] [G loss: 0.327508]\n",
            "Epoch: 71.69 [D loss: 1.327910, acc: 0.00%] [G loss: 0.327322]\n",
            "Epoch: 71.70 [D loss: 1.342416, acc: 0.00%] [G loss: 0.327159]\n",
            "Epoch: 71.71 [D loss: 1.354383, acc: 0.00%] [G loss: 0.328957]\n",
            "Epoch: 71.72 [D loss: 1.301245, acc: 0.00%] [G loss: 0.327758]\n",
            "Epoch: 71.73 [D loss: 1.406497, acc: 0.00%] [G loss: 0.328485]\n",
            "Epoch: 71.74 [D loss: 1.399807, acc: 0.00%] [G loss: 0.327412]\n",
            "Epoch: 71.75 [D loss: 1.354915, acc: 0.00%] [G loss: 0.326915]\n",
            "Epoch: 71.76 [D loss: 1.432023, acc: 0.00%] [G loss: 0.327611]\n",
            "Epoch: 71.77 [D loss: 1.413474, acc: 0.00%] [G loss: 0.326985]\n",
            "Epoch: 71.78 [D loss: 1.408554, acc: 0.00%] [G loss: 0.328107]\n",
            "Epoch: 71.79 [D loss: 1.355437, acc: 0.00%] [G loss: 0.326455]\n",
            "Epoch: 71.80 [D loss: 1.386271, acc: 0.00%] [G loss: 0.326554]\n",
            "Epoch: 71.81 [D loss: 1.337775, acc: 0.00%] [G loss: 0.328890]\n",
            "Epoch: 71.82 [D loss: 1.308060, acc: 0.00%] [G loss: 0.328110]\n",
            "Epoch: 71.83 [D loss: 1.365455, acc: 0.00%] [G loss: 0.329290]\n",
            "Epoch: 71.84 [D loss: 1.357130, acc: 0.00%] [G loss: 0.327932]\n",
            "Epoch: 71.85 [D loss: 1.343747, acc: 0.00%] [G loss: 0.327130]\n",
            "Epoch: 71.86 [D loss: 1.422092, acc: 0.00%] [G loss: 0.327797]\n",
            "Epoch: 71.87 [D loss: 1.411343, acc: 0.00%] [G loss: 0.327693]\n",
            "Epoch: 71.88 [D loss: 1.330597, acc: 0.00%] [G loss: 0.327253]\n",
            "Epoch: 71.89 [D loss: 1.388384, acc: 0.00%] [G loss: 0.326974]\n",
            "Epoch: 71.90 [D loss: 1.341024, acc: 0.00%] [G loss: 0.329318]\n",
            "Epoch: 71.91 [D loss: 1.367841, acc: 0.00%] [G loss: 0.328672]\n",
            "Epoch: 71.92 [D loss: 1.356467, acc: 0.00%] [G loss: 0.327798]\n",
            "Epoch: 71.93 [D loss: 1.312202, acc: 0.00%] [G loss: 0.328354]\n",
            "Epoch: 71.94 [D loss: 1.378111, acc: 0.00%] [G loss: 0.327466]\n",
            "Epoch: 71.95 [D loss: 1.341090, acc: 0.00%] [G loss: 0.326739]\n",
            "Epoch: 71.96 [D loss: 1.350838, acc: 0.00%] [G loss: 0.328141]\n",
            "Epoch: 71.97 [D loss: 1.378671, acc: 0.00%] [G loss: 0.328042]\n",
            "Epoch: 71.98 [D loss: 1.326040, acc: 0.00%] [G loss: 0.326838]\n",
            "Epoch: 71.99 [D loss: 1.392530, acc: 0.00%] [G loss: 0.326509]\n",
            "Epoch: 71.100 [D loss: 1.392131, acc: 0.00%] [G loss: 0.327799]\n",
            "Epoch: 71.101 [D loss: 1.305098, acc: 0.00%] [G loss: 0.328371]\n",
            "Epoch: 71.102 [D loss: 1.349470, acc: 0.00%] [G loss: 0.326416]\n",
            "Epoch: 71.103 [D loss: 1.348842, acc: 0.00%] [G loss: 0.327070]\n",
            "Epoch: 71.104 [D loss: 1.382490, acc: 0.00%] [G loss: 0.327441]\n",
            "Epoch: 71.105 [D loss: 1.395229, acc: 0.00%] [G loss: 0.327810]\n",
            "Epoch: 71.106 [D loss: 1.402367, acc: 0.00%] [G loss: 0.326723]\n",
            "Epoch: 71.107 [D loss: 1.381206, acc: 0.00%] [G loss: 0.329564]\n",
            "Epoch: 72.0 [D loss: 1.421178, acc: 0.00%] [G loss: 0.328799]\n",
            "Epoch: 72.1 [D loss: 1.337525, acc: 0.00%] [G loss: 0.327788]\n",
            "Epoch: 72.2 [D loss: 1.368732, acc: 0.00%] [G loss: 0.328163]\n",
            "Epoch: 72.3 [D loss: 1.328074, acc: 0.00%] [G loss: 0.327927]\n",
            "Epoch: 72.4 [D loss: 1.262219, acc: 0.00%] [G loss: 0.328785]\n",
            "Epoch: 72.5 [D loss: 1.355496, acc: 0.00%] [G loss: 0.327972]\n",
            "Epoch: 72.6 [D loss: 1.341060, acc: 0.00%] [G loss: 0.328088]\n",
            "Epoch: 72.7 [D loss: 1.360271, acc: 0.00%] [G loss: 0.327369]\n",
            "Epoch: 72.8 [D loss: 1.370199, acc: 0.00%] [G loss: 0.327673]\n",
            "Epoch: 72.9 [D loss: 1.414450, acc: 0.00%] [G loss: 0.329077]\n",
            "Epoch: 72.10 [D loss: 1.388586, acc: 0.00%] [G loss: 0.328317]\n",
            "Epoch: 72.11 [D loss: 1.387383, acc: 0.00%] [G loss: 0.328070]\n",
            "Epoch: 72.12 [D loss: 1.353430, acc: 0.00%] [G loss: 0.328045]\n",
            "Epoch: 72.13 [D loss: 1.352404, acc: 0.00%] [G loss: 0.327290]\n",
            "Epoch: 72.14 [D loss: 1.317970, acc: 0.00%] [G loss: 0.328913]\n",
            "Epoch: 72.15 [D loss: 1.344569, acc: 0.00%] [G loss: 0.327668]\n",
            "Epoch: 72.16 [D loss: 1.354024, acc: 0.00%] [G loss: 0.328132]\n",
            "Epoch: 72.17 [D loss: 1.357078, acc: 0.00%] [G loss: 0.327456]\n",
            "Epoch: 72.18 [D loss: 1.409564, acc: 0.00%] [G loss: 0.328040]\n",
            "Epoch: 72.19 [D loss: 1.351435, acc: 0.00%] [G loss: 0.327411]\n",
            "Epoch: 72.20 [D loss: 1.461851, acc: 0.00%] [G loss: 0.329218]\n",
            "Epoch: 72.21 [D loss: 1.435994, acc: 0.00%] [G loss: 0.329423]\n",
            "Epoch: 72.22 [D loss: 1.451661, acc: 0.00%] [G loss: 0.327878]\n",
            "Epoch: 72.23 [D loss: 1.448203, acc: 0.00%] [G loss: 0.327876]\n",
            "Epoch: 72.24 [D loss: 1.462075, acc: 0.00%] [G loss: 0.329189]\n",
            "Epoch: 72.25 [D loss: 1.379011, acc: 0.00%] [G loss: 0.328333]\n",
            "Epoch: 72.26 [D loss: 1.354782, acc: 0.00%] [G loss: 0.327494]\n",
            "Epoch: 72.27 [D loss: 1.350806, acc: 0.00%] [G loss: 0.327716]\n",
            "Epoch: 72.28 [D loss: 1.341081, acc: 0.00%] [G loss: 0.326217]\n",
            "Epoch: 72.29 [D loss: 1.357346, acc: 0.00%] [G loss: 0.329777]\n",
            "Epoch: 72.30 [D loss: 1.379048, acc: 0.00%] [G loss: 0.328324]\n",
            "Epoch: 72.31 [D loss: 1.338384, acc: 0.00%] [G loss: 0.331376]\n",
            "Epoch: 72.32 [D loss: 1.380095, acc: 0.00%] [G loss: 0.327466]\n",
            "Epoch: 72.33 [D loss: 1.420330, acc: 0.00%] [G loss: 0.327175]\n",
            "Epoch: 72.34 [D loss: 1.415204, acc: 0.00%] [G loss: 0.328115]\n",
            "Epoch: 72.35 [D loss: 1.422072, acc: 0.00%] [G loss: 0.327611]\n",
            "Epoch: 72.36 [D loss: 1.327387, acc: 0.00%] [G loss: 0.327464]\n",
            "Epoch: 72.37 [D loss: 1.367217, acc: 0.00%] [G loss: 0.326458]\n",
            "Epoch: 72.38 [D loss: 1.359204, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 72.39 [D loss: 1.335210, acc: 0.00%] [G loss: 0.327871]\n",
            "Epoch: 72.40 [D loss: 1.312315, acc: 0.00%] [G loss: 0.329490]\n",
            "Epoch: 72.41 [D loss: 1.325596, acc: 0.00%] [G loss: 0.329283]\n",
            "Epoch: 72.42 [D loss: 1.348158, acc: 0.00%] [G loss: 0.328968]\n",
            "Epoch: 72.43 [D loss: 1.390022, acc: 0.00%] [G loss: 0.327747]\n",
            "Epoch: 72.44 [D loss: 1.424308, acc: 0.00%] [G loss: 0.328743]\n",
            "Epoch: 72.45 [D loss: 1.403669, acc: 0.00%] [G loss: 0.329917]\n",
            "Epoch: 72.46 [D loss: 1.407703, acc: 0.00%] [G loss: 0.326718]\n",
            "Epoch: 72.47 [D loss: 1.422547, acc: 0.00%] [G loss: 0.329037]\n",
            "Epoch: 72.48 [D loss: 1.394816, acc: 0.00%] [G loss: 0.327013]\n",
            "Epoch: 72.49 [D loss: 1.400734, acc: 0.00%] [G loss: 0.327611]\n",
            "Epoch: 72.50 [D loss: 1.344358, acc: 0.00%] [G loss: 0.328064]\n",
            "Epoch: 72.51 [D loss: 1.365292, acc: 0.00%] [G loss: 0.326484]\n",
            "Epoch: 72.52 [D loss: 1.287305, acc: 0.00%] [G loss: 0.327433]\n",
            "Epoch: 72.53 [D loss: 1.277479, acc: 0.00%] [G loss: 0.327924]\n",
            "Epoch: 72.54 [D loss: 1.300256, acc: 0.00%] [G loss: 0.327114]\n",
            "Epoch: 72.55 [D loss: 1.358775, acc: 0.00%] [G loss: 0.327177]\n",
            "Epoch: 72.56 [D loss: 1.320756, acc: 0.00%] [G loss: 0.327490]\n",
            "Epoch: 72.57 [D loss: 1.353837, acc: 0.00%] [G loss: 0.327447]\n",
            "Epoch: 72.58 [D loss: 1.400250, acc: 0.00%] [G loss: 0.327335]\n",
            "Epoch: 72.59 [D loss: 1.384645, acc: 0.00%] [G loss: 0.328665]\n",
            "Epoch: 72.60 [D loss: 1.358162, acc: 0.00%] [G loss: 0.327204]\n",
            "Epoch: 72.61 [D loss: 1.388240, acc: 0.00%] [G loss: 0.328244]\n",
            "Epoch: 72.62 [D loss: 1.311659, acc: 0.00%] [G loss: 0.327735]\n",
            "Epoch: 72.63 [D loss: 1.354357, acc: 0.00%] [G loss: 0.326715]\n",
            "Epoch: 72.64 [D loss: 1.330582, acc: 0.00%] [G loss: 0.328465]\n",
            "Epoch: 72.65 [D loss: 1.324285, acc: 0.00%] [G loss: 0.327553]\n",
            "Epoch: 72.66 [D loss: 1.315842, acc: 0.00%] [G loss: 0.328738]\n",
            "Epoch: 72.67 [D loss: 1.390286, acc: 0.00%] [G loss: 0.327859]\n",
            "Epoch: 72.68 [D loss: 1.327000, acc: 0.00%] [G loss: 0.328578]\n",
            "Epoch: 72.69 [D loss: 1.336009, acc: 0.00%] [G loss: 0.328111]\n",
            "Epoch: 72.70 [D loss: 1.354941, acc: 0.00%] [G loss: 0.328066]\n",
            "Epoch: 72.71 [D loss: 1.387615, acc: 0.00%] [G loss: 0.329623]\n",
            "Epoch: 72.72 [D loss: 1.405297, acc: 0.00%] [G loss: 0.328097]\n",
            "Epoch: 72.73 [D loss: 1.409645, acc: 0.00%] [G loss: 0.329139]\n",
            "Epoch: 72.74 [D loss: 1.387166, acc: 0.00%] [G loss: 0.328406]\n",
            "Epoch: 72.75 [D loss: 1.402312, acc: 0.00%] [G loss: 0.326675]\n",
            "Epoch: 72.76 [D loss: 1.388077, acc: 0.00%] [G loss: 0.327024]\n",
            "Epoch: 72.77 [D loss: 1.321959, acc: 0.00%] [G loss: 0.327152]\n",
            "Epoch: 72.78 [D loss: 1.342029, acc: 0.00%] [G loss: 0.326772]\n",
            "Epoch: 72.79 [D loss: 1.332645, acc: 0.00%] [G loss: 0.327709]\n",
            "Epoch: 72.80 [D loss: 1.316284, acc: 0.00%] [G loss: 0.327040]\n",
            "Epoch: 72.81 [D loss: 1.364250, acc: 0.00%] [G loss: 0.327840]\n",
            "Epoch: 72.82 [D loss: 1.385715, acc: 0.00%] [G loss: 0.327723]\n",
            "Epoch: 72.83 [D loss: 1.356786, acc: 0.00%] [G loss: 0.327587]\n",
            "Epoch: 72.84 [D loss: 1.394183, acc: 0.00%] [G loss: 0.326379]\n",
            "Epoch: 72.85 [D loss: 1.366553, acc: 0.00%] [G loss: 0.328292]\n",
            "Epoch: 72.86 [D loss: 1.403641, acc: 0.00%] [G loss: 0.329234]\n",
            "Epoch: 72.87 [D loss: 1.328178, acc: 0.00%] [G loss: 0.327204]\n",
            "Epoch: 72.88 [D loss: 1.283751, acc: 0.00%] [G loss: 0.327859]\n",
            "Epoch: 72.89 [D loss: 1.384366, acc: 0.00%] [G loss: 0.328088]\n",
            "Epoch: 72.90 [D loss: 1.327495, acc: 0.00%] [G loss: 0.326695]\n",
            "Epoch: 72.91 [D loss: 1.406112, acc: 0.00%] [G loss: 0.327463]\n",
            "Epoch: 72.92 [D loss: 1.357999, acc: 0.00%] [G loss: 0.326972]\n",
            "Epoch: 72.93 [D loss: 1.350836, acc: 0.00%] [G loss: 0.328131]\n",
            "Epoch: 72.94 [D loss: 1.390895, acc: 0.00%] [G loss: 0.326586]\n",
            "Epoch: 72.95 [D loss: 1.345365, acc: 0.00%] [G loss: 0.327793]\n",
            "Epoch: 72.96 [D loss: 1.335734, acc: 0.00%] [G loss: 0.327473]\n",
            "Epoch: 72.97 [D loss: 1.350378, acc: 0.00%] [G loss: 0.327803]\n",
            "Epoch: 72.98 [D loss: 1.320323, acc: 0.00%] [G loss: 0.328556]\n",
            "Epoch: 72.99 [D loss: 1.353553, acc: 0.00%] [G loss: 0.327482]\n",
            "Epoch: 72.100 [D loss: 1.371318, acc: 0.00%] [G loss: 0.328398]\n",
            "Epoch: 72.101 [D loss: 1.329447, acc: 0.00%] [G loss: 0.328531]\n",
            "Epoch: 72.102 [D loss: 1.392635, acc: 0.00%] [G loss: 0.328809]\n",
            "Epoch: 72.103 [D loss: 1.421155, acc: 0.00%] [G loss: 0.326842]\n",
            "Epoch: 72.104 [D loss: 1.353025, acc: 0.00%] [G loss: 0.327805]\n",
            "Epoch: 72.105 [D loss: 1.384373, acc: 0.00%] [G loss: 0.327674]\n",
            "Epoch: 72.106 [D loss: 1.376053, acc: 0.00%] [G loss: 0.328330]\n",
            "Epoch: 72.107 [D loss: 1.379514, acc: 0.00%] [G loss: 0.326619]\n",
            "Epoch: 73.0 [D loss: 1.355614, acc: 0.00%] [G loss: 0.326902]\n",
            "Epoch: 73.1 [D loss: 1.291731, acc: 0.00%] [G loss: 0.328091]\n",
            "Epoch: 73.2 [D loss: 1.290155, acc: 0.00%] [G loss: 0.327006]\n",
            "Epoch: 73.3 [D loss: 1.278343, acc: 0.00%] [G loss: 0.328354]\n",
            "Epoch: 73.4 [D loss: 1.351347, acc: 0.00%] [G loss: 0.327031]\n",
            "Epoch: 73.5 [D loss: 1.369696, acc: 0.00%] [G loss: 0.328711]\n",
            "Epoch: 73.6 [D loss: 1.351107, acc: 0.00%] [G loss: 0.327152]\n",
            "Epoch: 73.7 [D loss: 1.455774, acc: 0.00%] [G loss: 0.328197]\n",
            "Epoch: 73.8 [D loss: 1.419100, acc: 0.00%] [G loss: 0.327399]\n",
            "Epoch: 73.9 [D loss: 1.409819, acc: 0.00%] [G loss: 0.329118]\n",
            "Epoch: 73.10 [D loss: 1.445800, acc: 0.00%] [G loss: 0.328936]\n",
            "Epoch: 73.11 [D loss: 1.416267, acc: 0.00%] [G loss: 0.328196]\n",
            "Epoch: 73.12 [D loss: 1.390167, acc: 0.00%] [G loss: 0.328696]\n",
            "Epoch: 73.13 [D loss: 1.403297, acc: 0.00%] [G loss: 0.328210]\n",
            "Epoch: 73.14 [D loss: 1.274080, acc: 0.00%] [G loss: 0.326261]\n",
            "Epoch: 73.15 [D loss: 1.308175, acc: 0.00%] [G loss: 0.327533]\n",
            "Epoch: 73.16 [D loss: 1.331045, acc: 0.00%] [G loss: 0.328331]\n",
            "Epoch: 73.17 [D loss: 1.298478, acc: 0.00%] [G loss: 0.327809]\n",
            "Epoch: 73.18 [D loss: 1.323933, acc: 0.00%] [G loss: 0.327195]\n",
            "Epoch: 73.19 [D loss: 1.340958, acc: 0.00%] [G loss: 0.329405]\n",
            "Epoch: 73.20 [D loss: 1.365728, acc: 0.00%] [G loss: 0.327276]\n",
            "Epoch: 73.21 [D loss: 1.384659, acc: 0.00%] [G loss: 0.328831]\n",
            "Epoch: 73.22 [D loss: 1.439611, acc: 0.00%] [G loss: 0.327721]\n",
            "Epoch: 73.23 [D loss: 1.429805, acc: 0.00%] [G loss: 0.328168]\n",
            "Epoch: 73.24 [D loss: 1.396678, acc: 0.00%] [G loss: 0.327852]\n",
            "Epoch: 73.25 [D loss: 1.370864, acc: 0.00%] [G loss: 0.327737]\n",
            "Epoch: 73.26 [D loss: 1.376101, acc: 0.00%] [G loss: 0.328455]\n",
            "Epoch: 73.27 [D loss: 1.306961, acc: 0.00%] [G loss: 0.327861]\n",
            "Epoch: 73.28 [D loss: 1.345790, acc: 0.00%] [G loss: 0.328508]\n",
            "Epoch: 73.29 [D loss: 1.338388, acc: 0.00%] [G loss: 0.328388]\n",
            "Epoch: 73.30 [D loss: 1.332180, acc: 0.00%] [G loss: 0.327750]\n",
            "Epoch: 73.31 [D loss: 1.326038, acc: 0.00%] [G loss: 0.327532]\n",
            "Epoch: 73.32 [D loss: 1.366196, acc: 0.00%] [G loss: 0.327526]\n",
            "Epoch: 73.33 [D loss: 1.425559, acc: 0.00%] [G loss: 0.327994]\n",
            "Epoch: 73.34 [D loss: 1.456350, acc: 0.00%] [G loss: 0.329262]\n",
            "Epoch: 73.35 [D loss: 1.350817, acc: 0.00%] [G loss: 0.328854]\n",
            "Epoch: 73.36 [D loss: 1.389749, acc: 0.00%] [G loss: 0.327041]\n",
            "Epoch: 73.37 [D loss: 1.360149, acc: 0.00%] [G loss: 0.328064]\n",
            "Epoch: 73.38 [D loss: 1.378068, acc: 0.00%] [G loss: 0.327245]\n",
            "Epoch: 73.39 [D loss: 1.328186, acc: 0.00%] [G loss: 0.326778]\n",
            "Epoch: 73.40 [D loss: 1.352136, acc: 0.00%] [G loss: 0.326722]\n",
            "Epoch: 73.41 [D loss: 1.295490, acc: 0.00%] [G loss: 0.327199]\n",
            "Epoch: 73.42 [D loss: 1.313427, acc: 0.00%] [G loss: 0.327294]\n",
            "Epoch: 73.43 [D loss: 1.336668, acc: 0.00%] [G loss: 0.329972]\n",
            "Epoch: 73.44 [D loss: 1.320492, acc: 0.00%] [G loss: 0.326967]\n",
            "Epoch: 73.45 [D loss: 1.420379, acc: 0.00%] [G loss: 0.326865]\n",
            "Epoch: 73.46 [D loss: 1.378238, acc: 0.00%] [G loss: 0.327653]\n",
            "Epoch: 73.47 [D loss: 1.417143, acc: 0.00%] [G loss: 0.329665]\n",
            "Epoch: 73.48 [D loss: 1.373178, acc: 0.00%] [G loss: 0.328569]\n",
            "Epoch: 73.49 [D loss: 1.375654, acc: 0.00%] [G loss: 0.326839]\n",
            "Epoch: 73.50 [D loss: 1.320921, acc: 0.00%] [G loss: 0.327628]\n",
            "Epoch: 73.51 [D loss: 1.327605, acc: 0.00%] [G loss: 0.330080]\n",
            "Epoch: 73.52 [D loss: 1.403022, acc: 0.00%] [G loss: 0.327697]\n",
            "Epoch: 73.53 [D loss: 1.371231, acc: 0.00%] [G loss: 0.327881]\n",
            "Epoch: 73.54 [D loss: 1.375126, acc: 0.00%] [G loss: 0.328107]\n",
            "Epoch: 73.55 [D loss: 1.424782, acc: 0.00%] [G loss: 0.327988]\n",
            "Epoch: 73.56 [D loss: 1.418330, acc: 0.00%] [G loss: 0.327330]\n",
            "Epoch: 73.57 [D loss: 1.402492, acc: 0.00%] [G loss: 0.327972]\n",
            "Epoch: 73.58 [D loss: 1.435154, acc: 0.00%] [G loss: 0.327302]\n",
            "Epoch: 73.59 [D loss: 1.358732, acc: 0.00%] [G loss: 0.327585]\n",
            "Epoch: 73.60 [D loss: 1.347342, acc: 0.00%] [G loss: 0.327233]\n",
            "Epoch: 73.61 [D loss: 1.339734, acc: 0.00%] [G loss: 0.328451]\n",
            "Epoch: 73.62 [D loss: 1.377710, acc: 0.00%] [G loss: 0.328234]\n",
            "Epoch: 73.63 [D loss: 1.347493, acc: 0.00%] [G loss: 0.328346]\n",
            "Epoch: 73.64 [D loss: 1.350648, acc: 0.00%] [G loss: 0.328274]\n",
            "Epoch: 73.65 [D loss: 1.356704, acc: 0.00%] [G loss: 0.326437]\n",
            "Epoch: 73.66 [D loss: 1.398854, acc: 0.00%] [G loss: 0.329269]\n",
            "Epoch: 73.67 [D loss: 1.470310, acc: 0.00%] [G loss: 0.329574]\n",
            "Epoch: 73.68 [D loss: 1.418147, acc: 0.00%] [G loss: 0.329436]\n",
            "Epoch: 73.69 [D loss: 1.378628, acc: 0.00%] [G loss: 0.327440]\n",
            "Epoch: 73.70 [D loss: 1.446640, acc: 0.00%] [G loss: 0.327407]\n",
            "Epoch: 73.71 [D loss: 1.399457, acc: 0.00%] [G loss: 0.326514]\n",
            "Epoch: 73.72 [D loss: 1.374450, acc: 0.00%] [G loss: 0.327029]\n",
            "Epoch: 73.73 [D loss: 1.335547, acc: 0.00%] [G loss: 0.327248]\n",
            "Epoch: 73.74 [D loss: 1.338093, acc: 0.00%] [G loss: 0.328280]\n",
            "Epoch: 73.75 [D loss: 1.298454, acc: 0.00%] [G loss: 0.327283]\n",
            "Epoch: 73.76 [D loss: 1.346641, acc: 0.00%] [G loss: 0.327052]\n",
            "Epoch: 73.77 [D loss: 1.316514, acc: 0.00%] [G loss: 0.328020]\n",
            "Epoch: 73.78 [D loss: 1.332949, acc: 0.00%] [G loss: 0.327500]\n",
            "Epoch: 73.79 [D loss: 1.396254, acc: 0.00%] [G loss: 0.328404]\n",
            "Epoch: 73.80 [D loss: 1.411860, acc: 0.00%] [G loss: 0.328716]\n",
            "Epoch: 73.81 [D loss: 1.439924, acc: 0.00%] [G loss: 0.328425]\n",
            "Epoch: 73.82 [D loss: 1.440252, acc: 0.00%] [G loss: 0.327409]\n",
            "Epoch: 73.83 [D loss: 1.410375, acc: 0.00%] [G loss: 0.327402]\n",
            "Epoch: 73.84 [D loss: 1.433335, acc: 0.00%] [G loss: 0.327534]\n",
            "Epoch: 73.85 [D loss: 1.361545, acc: 0.00%] [G loss: 0.326815]\n",
            "Epoch: 73.86 [D loss: 1.338710, acc: 0.00%] [G loss: 0.327629]\n",
            "Epoch: 73.87 [D loss: 1.348938, acc: 0.00%] [G loss: 0.328430]\n",
            "Epoch: 73.88 [D loss: 1.327818, acc: 0.00%] [G loss: 0.329436]\n",
            "Epoch: 73.89 [D loss: 1.363412, acc: 0.00%] [G loss: 0.329357]\n",
            "Epoch: 73.90 [D loss: 1.351325, acc: 0.00%] [G loss: 0.328029]\n",
            "Epoch: 73.91 [D loss: 1.443510, acc: 0.00%] [G loss: 0.327937]\n",
            "Epoch: 73.92 [D loss: 1.409173, acc: 0.00%] [G loss: 0.328081]\n",
            "Epoch: 73.93 [D loss: 1.411644, acc: 0.00%] [G loss: 0.329330]\n",
            "Epoch: 73.94 [D loss: 1.404759, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 73.95 [D loss: 1.433461, acc: 0.00%] [G loss: 0.329454]\n",
            "Epoch: 73.96 [D loss: 1.424566, acc: 0.00%] [G loss: 0.328030]\n",
            "Epoch: 73.97 [D loss: 1.337562, acc: 0.00%] [G loss: 0.327899]\n",
            "Epoch: 73.98 [D loss: 1.312995, acc: 0.00%] [G loss: 0.327835]\n",
            "Epoch: 73.99 [D loss: 1.310039, acc: 0.00%] [G loss: 0.327893]\n",
            "Epoch: 73.100 [D loss: 1.317024, acc: 0.00%] [G loss: 0.329153]\n",
            "Epoch: 73.101 [D loss: 1.373513, acc: 0.00%] [G loss: 0.328502]\n",
            "Epoch: 73.102 [D loss: 1.360676, acc: 0.00%] [G loss: 0.327448]\n",
            "Epoch: 73.103 [D loss: 1.334949, acc: 0.00%] [G loss: 0.327587]\n",
            "Epoch: 73.104 [D loss: 1.417439, acc: 0.00%] [G loss: 0.327832]\n",
            "Epoch: 73.105 [D loss: 1.403763, acc: 0.00%] [G loss: 0.327751]\n",
            "Epoch: 73.106 [D loss: 1.421428, acc: 0.00%] [G loss: 0.326610]\n",
            "Epoch: 73.107 [D loss: 1.445639, acc: 0.00%] [G loss: 0.328265]\n",
            "Epoch: 74.0 [D loss: 1.402710, acc: 0.00%] [G loss: 0.327266]\n",
            "Epoch: 74.1 [D loss: 1.347996, acc: 0.00%] [G loss: 0.328306]\n",
            "Epoch: 74.2 [D loss: 1.361940, acc: 0.00%] [G loss: 0.329020]\n",
            "Epoch: 74.3 [D loss: 1.359381, acc: 0.00%] [G loss: 0.328362]\n",
            "Epoch: 74.4 [D loss: 1.322328, acc: 0.00%] [G loss: 0.327686]\n",
            "Epoch: 74.5 [D loss: 1.329982, acc: 0.00%] [G loss: 0.326920]\n",
            "Epoch: 74.6 [D loss: 1.340253, acc: 0.00%] [G loss: 0.330301]\n",
            "Epoch: 74.7 [D loss: 1.368298, acc: 0.00%] [G loss: 0.329029]\n",
            "Epoch: 74.8 [D loss: 1.361809, acc: 0.00%] [G loss: 0.327796]\n",
            "Epoch: 74.9 [D loss: 1.432769, acc: 0.00%] [G loss: 0.328089]\n",
            "Epoch: 74.10 [D loss: 1.442435, acc: 0.00%] [G loss: 0.329043]\n",
            "Epoch: 74.11 [D loss: 1.461567, acc: 0.00%] [G loss: 0.327363]\n",
            "Epoch: 74.12 [D loss: 1.434227, acc: 0.00%] [G loss: 0.330112]\n",
            "Epoch: 74.13 [D loss: 1.438619, acc: 0.00%] [G loss: 0.328816]\n",
            "Epoch: 74.14 [D loss: 1.411305, acc: 0.00%] [G loss: 0.329194]\n",
            "Epoch: 74.15 [D loss: 1.402157, acc: 0.00%] [G loss: 0.327965]\n",
            "Epoch: 74.16 [D loss: 1.371287, acc: 0.00%] [G loss: 0.329107]\n",
            "Epoch: 74.17 [D loss: 1.335523, acc: 0.00%] [G loss: 0.328796]\n",
            "Epoch: 74.18 [D loss: 1.429598, acc: 0.00%] [G loss: 0.328559]\n",
            "Epoch: 74.19 [D loss: 1.355906, acc: 0.00%] [G loss: 0.327036]\n",
            "Epoch: 74.20 [D loss: 1.395953, acc: 0.00%] [G loss: 0.327039]\n",
            "Epoch: 74.21 [D loss: 1.379241, acc: 0.00%] [G loss: 0.327062]\n",
            "Epoch: 74.22 [D loss: 1.365636, acc: 0.00%] [G loss: 0.328187]\n",
            "Epoch: 74.23 [D loss: 1.361057, acc: 0.00%] [G loss: 0.326629]\n",
            "Epoch: 74.24 [D loss: 1.374293, acc: 0.00%] [G loss: 0.329060]\n",
            "Epoch: 74.25 [D loss: 1.350989, acc: 0.00%] [G loss: 0.327816]\n",
            "Epoch: 74.26 [D loss: 1.291582, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 74.27 [D loss: 1.323624, acc: 0.00%] [G loss: 0.329339]\n",
            "Epoch: 74.28 [D loss: 1.340477, acc: 0.00%] [G loss: 0.326856]\n",
            "Epoch: 74.29 [D loss: 1.383292, acc: 0.00%] [G loss: 0.328461]\n",
            "Epoch: 74.30 [D loss: 1.343849, acc: 0.00%] [G loss: 0.327589]\n",
            "Epoch: 74.31 [D loss: 1.345980, acc: 0.00%] [G loss: 0.326859]\n",
            "Epoch: 74.32 [D loss: 1.359963, acc: 0.00%] [G loss: 0.328614]\n",
            "Epoch: 74.33 [D loss: 1.326764, acc: 0.00%] [G loss: 0.326559]\n",
            "Epoch: 74.34 [D loss: 1.358636, acc: 0.00%] [G loss: 0.326486]\n",
            "Epoch: 74.35 [D loss: 1.359044, acc: 0.00%] [G loss: 0.326703]\n",
            "Epoch: 74.36 [D loss: 1.346752, acc: 0.00%] [G loss: 0.327672]\n",
            "Epoch: 74.37 [D loss: 1.328027, acc: 0.00%] [G loss: 0.328061]\n",
            "Epoch: 74.38 [D loss: 1.367281, acc: 0.00%] [G loss: 0.327658]\n",
            "Epoch: 74.39 [D loss: 1.329037, acc: 0.00%] [G loss: 0.327279]\n",
            "Epoch: 74.40 [D loss: 1.346471, acc: 0.00%] [G loss: 0.328653]\n",
            "Epoch: 74.41 [D loss: 1.337963, acc: 0.00%] [G loss: 0.326621]\n",
            "Epoch: 74.42 [D loss: 1.322255, acc: 0.00%] [G loss: 0.327032]\n",
            "Epoch: 74.43 [D loss: 1.385221, acc: 0.00%] [G loss: 0.327451]\n",
            "Epoch: 74.44 [D loss: 1.421307, acc: 0.00%] [G loss: 0.329225]\n",
            "Epoch: 74.45 [D loss: 1.321350, acc: 0.00%] [G loss: 0.327094]\n",
            "Epoch: 74.46 [D loss: 1.337032, acc: 0.00%] [G loss: 0.327831]\n",
            "Epoch: 74.47 [D loss: 1.398814, acc: 0.00%] [G loss: 0.327180]\n",
            "Epoch: 74.48 [D loss: 1.361740, acc: 0.00%] [G loss: 0.329116]\n",
            "Epoch: 74.49 [D loss: 1.391402, acc: 0.00%] [G loss: 0.329493]\n",
            "Epoch: 74.50 [D loss: 1.408942, acc: 0.00%] [G loss: 0.326849]\n",
            "Epoch: 74.51 [D loss: 1.354655, acc: 0.00%] [G loss: 0.329231]\n",
            "Epoch: 74.52 [D loss: 1.367979, acc: 0.00%] [G loss: 0.327284]\n",
            "Epoch: 74.53 [D loss: 1.360279, acc: 0.00%] [G loss: 0.326464]\n",
            "Epoch: 74.54 [D loss: 1.371313, acc: 0.00%] [G loss: 0.327127]\n",
            "Epoch: 74.55 [D loss: 1.380929, acc: 0.00%] [G loss: 0.327668]\n",
            "Epoch: 74.56 [D loss: 1.360761, acc: 0.00%] [G loss: 0.327022]\n",
            "Epoch: 74.57 [D loss: 1.317789, acc: 0.00%] [G loss: 0.327700]\n",
            "Epoch: 74.58 [D loss: 1.353685, acc: 0.00%] [G loss: 0.328048]\n",
            "Epoch: 74.59 [D loss: 1.306764, acc: 0.00%] [G loss: 0.327549]\n",
            "Epoch: 74.60 [D loss: 1.339020, acc: 0.00%] [G loss: 0.329004]\n",
            "Epoch: 74.61 [D loss: 1.378652, acc: 0.00%] [G loss: 0.327022]\n",
            "Epoch: 74.62 [D loss: 1.364452, acc: 0.00%] [G loss: 0.326350]\n",
            "Epoch: 74.63 [D loss: 1.401365, acc: 0.00%] [G loss: 0.327366]\n",
            "Epoch: 74.64 [D loss: 1.373206, acc: 0.00%] [G loss: 0.328866]\n",
            "Epoch: 74.65 [D loss: 1.358942, acc: 0.00%] [G loss: 0.327138]\n",
            "Epoch: 74.66 [D loss: 1.361138, acc: 0.00%] [G loss: 0.327753]\n",
            "Epoch: 74.67 [D loss: 1.344278, acc: 0.00%] [G loss: 0.327181]\n",
            "Epoch: 74.68 [D loss: 1.326486, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 74.69 [D loss: 1.297719, acc: 0.00%] [G loss: 0.328518]\n",
            "Epoch: 74.70 [D loss: 1.325448, acc: 0.00%] [G loss: 0.328883]\n",
            "Epoch: 74.71 [D loss: 1.417251, acc: 0.00%] [G loss: 0.327102]\n",
            "Epoch: 74.72 [D loss: 1.344056, acc: 0.00%] [G loss: 0.327657]\n",
            "Epoch: 74.73 [D loss: 1.425406, acc: 0.00%] [G loss: 0.329114]\n",
            "Epoch: 74.74 [D loss: 1.371664, acc: 0.00%] [G loss: 0.328050]\n",
            "Epoch: 74.75 [D loss: 1.366916, acc: 0.00%] [G loss: 0.327305]\n",
            "Epoch: 74.76 [D loss: 1.316459, acc: 0.00%] [G loss: 0.326381]\n",
            "Epoch: 74.77 [D loss: 1.358512, acc: 0.00%] [G loss: 0.328084]\n",
            "Epoch: 74.78 [D loss: 1.360446, acc: 0.00%] [G loss: 0.327305]\n",
            "Epoch: 74.79 [D loss: 1.393019, acc: 0.00%] [G loss: 0.327496]\n",
            "Epoch: 74.80 [D loss: 1.382014, acc: 0.00%] [G loss: 0.327630]\n",
            "Epoch: 74.81 [D loss: 1.352979, acc: 0.00%] [G loss: 0.328509]\n",
            "Epoch: 74.82 [D loss: 1.331033, acc: 0.00%] [G loss: 0.327114]\n",
            "Epoch: 74.83 [D loss: 1.348628, acc: 0.00%] [G loss: 0.327349]\n",
            "Epoch: 74.84 [D loss: 1.336493, acc: 0.00%] [G loss: 0.328663]\n",
            "Epoch: 74.85 [D loss: 1.330446, acc: 0.00%] [G loss: 0.328996]\n",
            "Epoch: 74.86 [D loss: 1.366943, acc: 0.00%] [G loss: 0.331588]\n",
            "Epoch: 74.87 [D loss: 1.356198, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 74.88 [D loss: 1.386113, acc: 0.00%] [G loss: 0.327683]\n",
            "Epoch: 74.89 [D loss: 1.379693, acc: 0.00%] [G loss: 0.329832]\n",
            "Epoch: 74.90 [D loss: 1.417806, acc: 0.00%] [G loss: 0.327375]\n",
            "Epoch: 74.91 [D loss: 1.415204, acc: 0.00%] [G loss: 0.327501]\n",
            "Epoch: 74.92 [D loss: 1.374823, acc: 0.00%] [G loss: 0.326863]\n",
            "Epoch: 74.93 [D loss: 1.333203, acc: 0.00%] [G loss: 0.328027]\n",
            "Epoch: 74.94 [D loss: 1.300910, acc: 0.00%] [G loss: 0.328324]\n",
            "Epoch: 74.95 [D loss: 1.325630, acc: 0.00%] [G loss: 0.328765]\n",
            "Epoch: 74.96 [D loss: 1.342690, acc: 0.00%] [G loss: 0.328004]\n",
            "Epoch: 74.97 [D loss: 1.398510, acc: 0.00%] [G loss: 0.327695]\n",
            "Epoch: 74.98 [D loss: 1.380578, acc: 0.00%] [G loss: 0.329850]\n",
            "Epoch: 74.99 [D loss: 1.385462, acc: 0.00%] [G loss: 0.328088]\n",
            "Epoch: 74.100 [D loss: 1.334061, acc: 0.00%] [G loss: 0.327988]\n",
            "Epoch: 74.101 [D loss: 1.428951, acc: 0.00%] [G loss: 0.326979]\n",
            "Epoch: 74.102 [D loss: 1.409874, acc: 0.00%] [G loss: 0.327213]\n",
            "Epoch: 74.103 [D loss: 1.325423, acc: 0.00%] [G loss: 0.327844]\n",
            "Epoch: 74.104 [D loss: 1.383056, acc: 0.00%] [G loss: 0.327389]\n",
            "Epoch: 74.105 [D loss: 1.349366, acc: 0.00%] [G loss: 0.327863]\n",
            "Epoch: 74.106 [D loss: 1.350266, acc: 0.00%] [G loss: 0.328428]\n",
            "Epoch: 74.107 [D loss: 1.346143, acc: 0.00%] [G loss: 0.327510]\n",
            "Epoch: 75.0 [D loss: 1.346533, acc: 0.00%] [G loss: 0.327358]\n",
            "Epoch: 75.1 [D loss: 1.359448, acc: 0.00%] [G loss: 0.329872]\n",
            "Epoch: 75.2 [D loss: 1.384682, acc: 0.00%] [G loss: 0.328941]\n",
            "Epoch: 75.3 [D loss: 1.344446, acc: 0.00%] [G loss: 0.327398]\n",
            "Epoch: 75.4 [D loss: 1.394380, acc: 0.00%] [G loss: 0.328137]\n",
            "Epoch: 75.5 [D loss: 1.320256, acc: 0.00%] [G loss: 0.327599]\n",
            "Epoch: 75.6 [D loss: 1.329409, acc: 0.00%] [G loss: 0.327013]\n",
            "Epoch: 75.7 [D loss: 1.370107, acc: 0.00%] [G loss: 0.329814]\n",
            "Epoch: 75.8 [D loss: 1.340378, acc: 0.00%] [G loss: 0.327048]\n",
            "Epoch: 75.9 [D loss: 1.326618, acc: 0.00%] [G loss: 0.327104]\n",
            "Epoch: 75.10 [D loss: 1.351067, acc: 0.00%] [G loss: 0.327226]\n",
            "Epoch: 75.11 [D loss: 1.357812, acc: 0.00%] [G loss: 0.326756]\n",
            "Epoch: 75.12 [D loss: 1.358166, acc: 0.00%] [G loss: 0.328442]\n",
            "Epoch: 75.13 [D loss: 1.379466, acc: 0.00%] [G loss: 0.327488]\n",
            "Epoch: 75.14 [D loss: 1.349907, acc: 0.00%] [G loss: 0.326912]\n",
            "Epoch: 75.15 [D loss: 1.354894, acc: 0.00%] [G loss: 0.326714]\n",
            "Epoch: 75.16 [D loss: 1.363398, acc: 0.00%] [G loss: 0.327371]\n",
            "Epoch: 75.17 [D loss: 1.360582, acc: 0.00%] [G loss: 0.327512]\n",
            "Epoch: 75.18 [D loss: 1.310455, acc: 0.00%] [G loss: 0.327117]\n",
            "Epoch: 75.19 [D loss: 1.326856, acc: 0.00%] [G loss: 0.326098]\n",
            "Epoch: 75.20 [D loss: 1.362729, acc: 0.00%] [G loss: 0.327159]\n",
            "Epoch: 75.21 [D loss: 1.354594, acc: 0.00%] [G loss: 0.327409]\n",
            "Epoch: 75.22 [D loss: 1.339423, acc: 0.00%] [G loss: 0.328486]\n",
            "Epoch: 75.23 [D loss: 1.364399, acc: 0.00%] [G loss: 0.328530]\n",
            "Epoch: 75.24 [D loss: 1.396164, acc: 0.00%] [G loss: 0.328076]\n",
            "Epoch: 75.25 [D loss: 1.368689, acc: 0.00%] [G loss: 0.327995]\n",
            "Epoch: 75.26 [D loss: 1.394171, acc: 0.00%] [G loss: 0.328026]\n",
            "Epoch: 75.27 [D loss: 1.390027, acc: 0.00%] [G loss: 0.328358]\n",
            "Epoch: 75.28 [D loss: 1.362141, acc: 0.00%] [G loss: 0.326746]\n",
            "Epoch: 75.29 [D loss: 1.324334, acc: 0.00%] [G loss: 0.327518]\n",
            "Epoch: 75.30 [D loss: 1.363376, acc: 0.00%] [G loss: 0.326903]\n",
            "Epoch: 75.31 [D loss: 1.326361, acc: 0.00%] [G loss: 0.327669]\n",
            "Epoch: 75.32 [D loss: 1.334525, acc: 0.00%] [G loss: 0.327189]\n",
            "Epoch: 75.33 [D loss: 1.372878, acc: 0.00%] [G loss: 0.327039]\n",
            "Epoch: 75.34 [D loss: 1.347569, acc: 0.00%] [G loss: 0.326259]\n",
            "Epoch: 75.35 [D loss: 1.275613, acc: 0.00%] [G loss: 0.327707]\n",
            "Epoch: 75.36 [D loss: 1.357238, acc: 0.00%] [G loss: 0.327198]\n",
            "Epoch: 75.37 [D loss: 1.372389, acc: 0.00%] [G loss: 0.328406]\n",
            "Epoch: 75.38 [D loss: 1.394433, acc: 0.00%] [G loss: 0.327424]\n",
            "Epoch: 75.39 [D loss: 1.387793, acc: 0.00%] [G loss: 0.328707]\n",
            "Epoch: 75.40 [D loss: 1.382237, acc: 0.00%] [G loss: 0.327753]\n",
            "Epoch: 75.41 [D loss: 1.366869, acc: 0.00%] [G loss: 0.327420]\n",
            "Epoch: 75.42 [D loss: 1.328178, acc: 0.00%] [G loss: 0.326871]\n",
            "Epoch: 75.43 [D loss: 1.341186, acc: 0.00%] [G loss: 0.326864]\n",
            "Epoch: 75.44 [D loss: 1.372761, acc: 0.00%] [G loss: 0.327823]\n",
            "Epoch: 75.45 [D loss: 1.346343, acc: 0.00%] [G loss: 0.327123]\n",
            "Epoch: 75.46 [D loss: 1.325699, acc: 0.00%] [G loss: 0.326777]\n",
            "Epoch: 75.47 [D loss: 1.342171, acc: 0.00%] [G loss: 0.327537]\n",
            "Epoch: 75.48 [D loss: 1.340259, acc: 0.00%] [G loss: 0.327716]\n",
            "Epoch: 75.49 [D loss: 1.340712, acc: 0.00%] [G loss: 0.328451]\n",
            "Epoch: 75.50 [D loss: 1.362165, acc: 0.00%] [G loss: 0.327058]\n",
            "Epoch: 75.51 [D loss: 1.322050, acc: 0.00%] [G loss: 0.327373]\n",
            "Epoch: 75.52 [D loss: 1.376260, acc: 0.00%] [G loss: 0.329371]\n",
            "Epoch: 75.53 [D loss: 1.395328, acc: 0.00%] [G loss: 0.329260]\n",
            "Epoch: 75.54 [D loss: 1.403207, acc: 0.00%] [G loss: 0.328073]\n",
            "Epoch: 75.55 [D loss: 1.376024, acc: 0.00%] [G loss: 0.327643]\n",
            "Epoch: 75.56 [D loss: 1.357975, acc: 0.00%] [G loss: 0.327542]\n",
            "Epoch: 75.57 [D loss: 1.341368, acc: 0.00%] [G loss: 0.326844]\n",
            "Epoch: 75.58 [D loss: 1.329844, acc: 0.00%] [G loss: 0.327280]\n",
            "Epoch: 75.59 [D loss: 1.320898, acc: 0.00%] [G loss: 0.326584]\n",
            "Epoch: 75.60 [D loss: 1.328368, acc: 0.00%] [G loss: 0.330852]\n",
            "Epoch: 75.61 [D loss: 1.302632, acc: 0.00%] [G loss: 0.327552]\n",
            "Epoch: 75.62 [D loss: 1.328678, acc: 0.00%] [G loss: 0.328114]\n",
            "Epoch: 75.63 [D loss: 1.346159, acc: 0.00%] [G loss: 0.327969]\n",
            "Epoch: 75.64 [D loss: 1.408933, acc: 0.00%] [G loss: 0.326149]\n",
            "Epoch: 75.65 [D loss: 1.365369, acc: 0.00%] [G loss: 0.327489]\n",
            "Epoch: 75.66 [D loss: 1.376493, acc: 0.00%] [G loss: 0.326992]\n",
            "Epoch: 75.67 [D loss: 1.381622, acc: 0.00%] [G loss: 0.328754]\n",
            "Epoch: 75.68 [D loss: 1.346780, acc: 0.00%] [G loss: 0.328923]\n",
            "Epoch: 75.69 [D loss: 1.342186, acc: 0.00%] [G loss: 0.327686]\n",
            "Epoch: 75.70 [D loss: 1.299020, acc: 0.00%] [G loss: 0.329902]\n",
            "Epoch: 75.71 [D loss: 1.346219, acc: 0.00%] [G loss: 0.328086]\n",
            "Epoch: 75.72 [D loss: 1.381360, acc: 0.00%] [G loss: 0.328835]\n",
            "Epoch: 75.73 [D loss: 1.319649, acc: 0.00%] [G loss: 0.327683]\n",
            "Epoch: 75.74 [D loss: 1.379306, acc: 0.00%] [G loss: 0.327941]\n",
            "Epoch: 75.75 [D loss: 1.403042, acc: 0.00%] [G loss: 0.327434]\n",
            "Epoch: 75.76 [D loss: 1.423085, acc: 0.00%] [G loss: 0.329802]\n",
            "Epoch: 75.77 [D loss: 1.426824, acc: 0.00%] [G loss: 0.327427]\n",
            "Epoch: 75.78 [D loss: 1.382895, acc: 0.00%] [G loss: 0.326678]\n",
            "Epoch: 75.79 [D loss: 1.385046, acc: 0.00%] [G loss: 0.328586]\n",
            "Epoch: 75.80 [D loss: 1.350592, acc: 0.00%] [G loss: 0.327339]\n",
            "Epoch: 75.81 [D loss: 1.291329, acc: 0.00%] [G loss: 0.327263]\n",
            "Epoch: 75.82 [D loss: 1.308490, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 75.83 [D loss: 1.334026, acc: 0.00%] [G loss: 0.329085]\n",
            "Epoch: 75.84 [D loss: 1.351597, acc: 0.00%] [G loss: 0.330467]\n",
            "Epoch: 75.85 [D loss: 1.273609, acc: 0.00%] [G loss: 0.328348]\n",
            "Epoch: 75.86 [D loss: 1.352977, acc: 0.00%] [G loss: 0.327381]\n",
            "Epoch: 75.87 [D loss: 1.390952, acc: 0.00%] [G loss: 0.328874]\n",
            "Epoch: 75.88 [D loss: 1.432369, acc: 0.00%] [G loss: 0.328049]\n",
            "Epoch: 75.89 [D loss: 1.405363, acc: 0.00%] [G loss: 0.328778]\n",
            "Epoch: 75.90 [D loss: 1.428867, acc: 0.00%] [G loss: 0.328262]\n",
            "Epoch: 75.91 [D loss: 1.333246, acc: 0.00%] [G loss: 0.328420]\n",
            "Epoch: 75.92 [D loss: 1.392799, acc: 0.00%] [G loss: 0.327446]\n",
            "Epoch: 75.93 [D loss: 1.337852, acc: 0.00%] [G loss: 0.327425]\n",
            "Epoch: 75.94 [D loss: 1.299090, acc: 0.00%] [G loss: 0.327530]\n",
            "Epoch: 75.95 [D loss: 1.391866, acc: 0.00%] [G loss: 0.326713]\n",
            "Epoch: 75.96 [D loss: 1.330548, acc: 0.00%] [G loss: 0.327272]\n",
            "Epoch: 75.97 [D loss: 1.301344, acc: 0.00%] [G loss: 0.327493]\n",
            "Epoch: 75.98 [D loss: 1.314738, acc: 0.00%] [G loss: 0.327587]\n",
            "Epoch: 75.99 [D loss: 1.338651, acc: 0.00%] [G loss: 0.327347]\n",
            "Epoch: 75.100 [D loss: 1.341854, acc: 0.00%] [G loss: 0.329743]\n",
            "Epoch: 75.101 [D loss: 1.374833, acc: 0.00%] [G loss: 0.327432]\n",
            "Epoch: 75.102 [D loss: 1.463035, acc: 0.00%] [G loss: 0.328794]\n",
            "Epoch: 75.103 [D loss: 1.411683, acc: 0.00%] [G loss: 0.327668]\n",
            "Epoch: 75.104 [D loss: 1.397695, acc: 0.00%] [G loss: 0.329164]\n",
            "Epoch: 75.105 [D loss: 1.375293, acc: 0.00%] [G loss: 0.326935]\n",
            "Epoch: 75.106 [D loss: 1.384016, acc: 0.00%] [G loss: 0.327473]\n",
            "Epoch: 75.107 [D loss: 1.314997, acc: 0.00%] [G loss: 0.329320]\n",
            "Epoch: 76.0 [D loss: 1.349338, acc: 0.00%] [G loss: 0.328068]\n",
            "Epoch: 76.1 [D loss: 1.334656, acc: 0.00%] [G loss: 0.327806]\n",
            "Epoch: 76.2 [D loss: 1.368795, acc: 0.00%] [G loss: 0.326620]\n",
            "Epoch: 76.3 [D loss: 1.400346, acc: 0.00%] [G loss: 0.328718]\n",
            "Epoch: 76.4 [D loss: 1.416925, acc: 0.00%] [G loss: 0.327544]\n",
            "Epoch: 76.5 [D loss: 1.400017, acc: 0.00%] [G loss: 0.328328]\n",
            "Epoch: 76.6 [D loss: 1.359551, acc: 0.00%] [G loss: 0.326765]\n",
            "Epoch: 76.7 [D loss: 1.332735, acc: 0.00%] [G loss: 0.326692]\n",
            "Epoch: 76.8 [D loss: 1.310102, acc: 0.00%] [G loss: 0.328202]\n",
            "Epoch: 76.9 [D loss: 1.333892, acc: 0.00%] [G loss: 0.327494]\n",
            "Epoch: 76.10 [D loss: 1.350324, acc: 0.00%] [G loss: 0.327152]\n",
            "Epoch: 76.11 [D loss: 1.309920, acc: 0.00%] [G loss: 0.327570]\n",
            "Epoch: 76.12 [D loss: 1.319771, acc: 0.00%] [G loss: 0.328824]\n",
            "Epoch: 76.13 [D loss: 1.397640, acc: 0.00%] [G loss: 0.328327]\n",
            "Epoch: 76.14 [D loss: 1.398542, acc: 0.00%] [G loss: 0.328410]\n",
            "Epoch: 76.15 [D loss: 1.396998, acc: 0.00%] [G loss: 0.327648]\n",
            "Epoch: 76.16 [D loss: 1.399189, acc: 0.00%] [G loss: 0.327672]\n",
            "Epoch: 76.17 [D loss: 1.371714, acc: 0.00%] [G loss: 0.327203]\n",
            "Epoch: 76.18 [D loss: 1.336597, acc: 0.00%] [G loss: 0.329348]\n",
            "Epoch: 76.19 [D loss: 1.338177, acc: 0.00%] [G loss: 0.327725]\n",
            "Epoch: 76.20 [D loss: 1.357668, acc: 0.00%] [G loss: 0.326566]\n",
            "Epoch: 76.21 [D loss: 1.361637, acc: 0.00%] [G loss: 0.327108]\n",
            "Epoch: 76.22 [D loss: 1.375028, acc: 0.00%] [G loss: 0.328918]\n",
            "Epoch: 76.23 [D loss: 1.426385, acc: 0.00%] [G loss: 0.329085]\n",
            "Epoch: 76.24 [D loss: 1.416015, acc: 0.00%] [G loss: 0.326541]\n",
            "Epoch: 76.25 [D loss: 1.405379, acc: 0.00%] [G loss: 0.327514]\n",
            "Epoch: 76.26 [D loss: 1.408183, acc: 0.00%] [G loss: 0.328659]\n",
            "Epoch: 76.27 [D loss: 1.359820, acc: 0.00%] [G loss: 0.329518]\n",
            "Epoch: 76.28 [D loss: 1.367063, acc: 0.00%] [G loss: 0.326237]\n",
            "Epoch: 76.29 [D loss: 1.377480, acc: 0.00%] [G loss: 0.328460]\n",
            "Epoch: 76.30 [D loss: 1.356078, acc: 0.00%] [G loss: 0.328343]\n",
            "Epoch: 76.31 [D loss: 1.356907, acc: 0.00%] [G loss: 0.327571]\n",
            "Epoch: 76.32 [D loss: 1.368469, acc: 0.00%] [G loss: 0.326687]\n",
            "Epoch: 76.33 [D loss: 1.376088, acc: 0.00%] [G loss: 0.328616]\n",
            "Epoch: 76.34 [D loss: 1.384496, acc: 0.00%] [G loss: 0.327696]\n",
            "Epoch: 76.35 [D loss: 1.325553, acc: 0.00%] [G loss: 0.326825]\n",
            "Epoch: 76.36 [D loss: 1.327857, acc: 0.00%] [G loss: 0.327268]\n",
            "Epoch: 76.37 [D loss: 1.375640, acc: 0.00%] [G loss: 0.327863]\n",
            "Epoch: 76.38 [D loss: 1.398602, acc: 0.00%] [G loss: 0.327501]\n",
            "Epoch: 76.39 [D loss: 1.428910, acc: 0.00%] [G loss: 0.328202]\n",
            "Epoch: 76.40 [D loss: 1.388622, acc: 0.00%] [G loss: 0.326752]\n",
            "Epoch: 76.41 [D loss: 1.387872, acc: 0.00%] [G loss: 0.327465]\n",
            "Epoch: 76.42 [D loss: 1.406481, acc: 0.00%] [G loss: 0.327393]\n",
            "Epoch: 76.43 [D loss: 1.307633, acc: 0.00%] [G loss: 0.327247]\n",
            "Epoch: 76.44 [D loss: 1.313529, acc: 0.00%] [G loss: 0.328037]\n",
            "Epoch: 76.45 [D loss: 1.287136, acc: 0.00%] [G loss: 0.327468]\n",
            "Epoch: 76.46 [D loss: 1.292240, acc: 0.00%] [G loss: 0.329179]\n",
            "Epoch: 76.47 [D loss: 1.303053, acc: 0.00%] [G loss: 0.327564]\n",
            "Epoch: 76.48 [D loss: 1.336075, acc: 0.00%] [G loss: 0.328196]\n",
            "Epoch: 76.49 [D loss: 1.396660, acc: 0.00%] [G loss: 0.329437]\n",
            "Epoch: 76.50 [D loss: 1.356874, acc: 0.00%] [G loss: 0.327593]\n",
            "Epoch: 76.51 [D loss: 1.499423, acc: 0.00%] [G loss: 0.327370]\n",
            "Epoch: 76.52 [D loss: 1.436181, acc: 0.00%] [G loss: 0.328393]\n",
            "Epoch: 76.53 [D loss: 1.427818, acc: 0.00%] [G loss: 0.327866]\n",
            "Epoch: 76.54 [D loss: 1.385832, acc: 0.00%] [G loss: 0.327686]\n",
            "Epoch: 76.55 [D loss: 1.408655, acc: 0.00%] [G loss: 0.328898]\n",
            "Epoch: 76.56 [D loss: 1.337108, acc: 0.00%] [G loss: 0.327252]\n",
            "Epoch: 76.57 [D loss: 1.367116, acc: 0.00%] [G loss: 0.326640]\n",
            "Epoch: 76.58 [D loss: 1.318499, acc: 0.00%] [G loss: 0.327284]\n",
            "Epoch: 76.59 [D loss: 1.344553, acc: 0.00%] [G loss: 0.327701]\n",
            "Epoch: 76.60 [D loss: 1.367789, acc: 0.00%] [G loss: 0.328342]\n",
            "Epoch: 76.61 [D loss: 1.401363, acc: 0.00%] [G loss: 0.328470]\n",
            "Epoch: 76.62 [D loss: 1.397180, acc: 0.00%] [G loss: 0.326941]\n",
            "Epoch: 76.63 [D loss: 1.453489, acc: 0.00%] [G loss: 0.327200]\n",
            "Epoch: 76.64 [D loss: 1.443654, acc: 0.00%] [G loss: 0.330163]\n",
            "Epoch: 76.65 [D loss: 1.386464, acc: 0.00%] [G loss: 0.329304]\n",
            "Epoch: 76.66 [D loss: 1.360857, acc: 0.00%] [G loss: 0.328361]\n",
            "Epoch: 76.67 [D loss: 1.338884, acc: 0.00%] [G loss: 0.326609]\n",
            "Epoch: 76.68 [D loss: 1.359254, acc: 0.00%] [G loss: 0.328083]\n",
            "Epoch: 76.69 [D loss: 1.370933, acc: 0.00%] [G loss: 0.328269]\n",
            "Epoch: 76.70 [D loss: 1.315698, acc: 0.00%] [G loss: 0.326818]\n",
            "Epoch: 76.71 [D loss: 1.342568, acc: 0.00%] [G loss: 0.329639]\n",
            "Epoch: 76.72 [D loss: 1.319510, acc: 0.00%] [G loss: 0.327697]\n",
            "Epoch: 76.73 [D loss: 1.368668, acc: 0.00%] [G loss: 0.327229]\n",
            "Epoch: 76.74 [D loss: 1.332377, acc: 0.00%] [G loss: 0.328110]\n",
            "Epoch: 76.75 [D loss: 1.327398, acc: 0.00%] [G loss: 0.329500]\n",
            "Epoch: 76.76 [D loss: 1.325953, acc: 0.00%] [G loss: 0.329803]\n",
            "Epoch: 76.77 [D loss: 1.396173, acc: 0.00%] [G loss: 0.329369]\n",
            "Epoch: 76.78 [D loss: 1.353378, acc: 0.00%] [G loss: 0.327712]\n",
            "Epoch: 76.79 [D loss: 1.421236, acc: 0.00%] [G loss: 0.327970]\n",
            "Epoch: 76.80 [D loss: 1.389235, acc: 0.00%] [G loss: 0.327853]\n",
            "Epoch: 76.81 [D loss: 1.314078, acc: 0.00%] [G loss: 0.327603]\n",
            "Epoch: 76.82 [D loss: 1.417273, acc: 0.00%] [G loss: 0.327278]\n",
            "Epoch: 76.83 [D loss: 1.437686, acc: 0.00%] [G loss: 0.326898]\n",
            "Epoch: 76.84 [D loss: 1.398124, acc: 0.00%] [G loss: 0.326809]\n",
            "Epoch: 76.85 [D loss: 1.364882, acc: 0.00%] [G loss: 0.327670]\n",
            "Epoch: 76.86 [D loss: 1.392942, acc: 0.00%] [G loss: 0.327511]\n",
            "Epoch: 76.87 [D loss: 1.368444, acc: 0.00%] [G loss: 0.327982]\n",
            "Epoch: 76.88 [D loss: 1.354574, acc: 0.00%] [G loss: 0.326213]\n",
            "Epoch: 76.89 [D loss: 1.353672, acc: 0.00%] [G loss: 0.326796]\n",
            "Epoch: 76.90 [D loss: 1.335902, acc: 0.00%] [G loss: 0.328022]\n",
            "Epoch: 76.91 [D loss: 1.361731, acc: 0.00%] [G loss: 0.327141]\n",
            "Epoch: 76.92 [D loss: 1.388495, acc: 0.00%] [G loss: 0.327941]\n",
            "Epoch: 76.93 [D loss: 1.410470, acc: 0.00%] [G loss: 0.327901]\n",
            "Epoch: 76.94 [D loss: 1.398356, acc: 0.00%] [G loss: 0.327621]\n",
            "Epoch: 76.95 [D loss: 1.374626, acc: 0.00%] [G loss: 0.328795]\n",
            "Epoch: 76.96 [D loss: 1.391772, acc: 0.00%] [G loss: 0.328455]\n",
            "Epoch: 76.97 [D loss: 1.373060, acc: 0.00%] [G loss: 0.327362]\n",
            "Epoch: 76.98 [D loss: 1.329774, acc: 0.00%] [G loss: 0.327023]\n",
            "Epoch: 76.99 [D loss: 1.361070, acc: 0.00%] [G loss: 0.328505]\n",
            "Epoch: 76.100 [D loss: 1.337738, acc: 0.00%] [G loss: 0.327623]\n",
            "Epoch: 76.101 [D loss: 1.307919, acc: 0.00%] [G loss: 0.329633]\n",
            "Epoch: 76.102 [D loss: 1.361089, acc: 0.00%] [G loss: 0.328314]\n",
            "Epoch: 76.103 [D loss: 1.351399, acc: 0.00%] [G loss: 0.326656]\n",
            "Epoch: 76.104 [D loss: 1.385353, acc: 0.00%] [G loss: 0.328166]\n",
            "Epoch: 76.105 [D loss: 1.429566, acc: 0.00%] [G loss: 0.327343]\n",
            "Epoch: 76.106 [D loss: 1.396348, acc: 0.00%] [G loss: 0.327347]\n",
            "Epoch: 76.107 [D loss: 1.416418, acc: 0.00%] [G loss: 0.328875]\n",
            "Epoch: 77.0 [D loss: 1.387053, acc: 0.00%] [G loss: 0.326934]\n",
            "Epoch: 77.1 [D loss: 1.362101, acc: 0.00%] [G loss: 0.328425]\n",
            "Epoch: 77.2 [D loss: 1.371945, acc: 0.00%] [G loss: 0.326523]\n",
            "Epoch: 77.3 [D loss: 1.315689, acc: 0.00%] [G loss: 0.328201]\n",
            "Epoch: 77.4 [D loss: 1.318886, acc: 0.00%] [G loss: 0.327435]\n",
            "Epoch: 77.5 [D loss: 1.285609, acc: 0.00%] [G loss: 0.326945]\n",
            "Epoch: 77.6 [D loss: 1.305521, acc: 0.00%] [G loss: 0.326577]\n",
            "Epoch: 77.7 [D loss: 1.285872, acc: 0.00%] [G loss: 0.327982]\n",
            "Epoch: 77.8 [D loss: 1.306303, acc: 0.00%] [G loss: 0.328801]\n",
            "Epoch: 77.9 [D loss: 1.356408, acc: 0.00%] [G loss: 0.327341]\n",
            "Epoch: 77.10 [D loss: 1.379127, acc: 0.00%] [G loss: 0.327369]\n",
            "Epoch: 77.11 [D loss: 1.457889, acc: 0.00%] [G loss: 0.328090]\n",
            "Epoch: 77.12 [D loss: 1.376102, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 77.13 [D loss: 1.474050, acc: 0.00%] [G loss: 0.328227]\n",
            "Epoch: 77.14 [D loss: 1.421556, acc: 0.00%] [G loss: 0.328279]\n",
            "Epoch: 77.15 [D loss: 1.376007, acc: 0.00%] [G loss: 0.326290]\n",
            "Epoch: 77.16 [D loss: 1.309719, acc: 0.00%] [G loss: 0.327829]\n",
            "Epoch: 77.17 [D loss: 1.362024, acc: 0.00%] [G loss: 0.329686]\n",
            "Epoch: 77.18 [D loss: 1.393735, acc: 0.00%] [G loss: 0.327570]\n",
            "Epoch: 77.19 [D loss: 1.366251, acc: 0.00%] [G loss: 0.328001]\n",
            "Epoch: 77.20 [D loss: 1.374679, acc: 0.00%] [G loss: 0.328475]\n",
            "Epoch: 77.21 [D loss: 1.399405, acc: 0.00%] [G loss: 0.327775]\n",
            "Epoch: 77.22 [D loss: 1.414279, acc: 0.00%] [G loss: 0.327356]\n",
            "Epoch: 77.23 [D loss: 1.427510, acc: 0.00%] [G loss: 0.328758]\n",
            "Epoch: 77.24 [D loss: 1.359933, acc: 0.00%] [G loss: 0.327527]\n",
            "Epoch: 77.25 [D loss: 1.331623, acc: 0.00%] [G loss: 0.328686]\n",
            "Epoch: 77.26 [D loss: 1.382694, acc: 0.00%] [G loss: 0.327396]\n",
            "Epoch: 77.27 [D loss: 1.383002, acc: 0.00%] [G loss: 0.326246]\n",
            "Epoch: 77.28 [D loss: 1.366923, acc: 0.00%] [G loss: 0.327278]\n",
            "Epoch: 77.29 [D loss: 1.352815, acc: 0.00%] [G loss: 0.326950]\n",
            "Epoch: 77.30 [D loss: 1.350231, acc: 0.00%] [G loss: 0.327725]\n",
            "Epoch: 77.31 [D loss: 1.375564, acc: 0.00%] [G loss: 0.326832]\n",
            "Epoch: 77.32 [D loss: 1.356412, acc: 0.00%] [G loss: 0.326830]\n",
            "Epoch: 77.33 [D loss: 1.364464, acc: 0.00%] [G loss: 0.328185]\n",
            "Epoch: 77.34 [D loss: 1.371021, acc: 0.00%] [G loss: 0.327009]\n",
            "Epoch: 77.35 [D loss: 1.366332, acc: 0.00%] [G loss: 0.327362]\n",
            "Epoch: 77.36 [D loss: 1.365781, acc: 0.00%] [G loss: 0.326567]\n",
            "Epoch: 77.37 [D loss: 1.372038, acc: 0.00%] [G loss: 0.328797]\n",
            "Epoch: 77.38 [D loss: 1.363414, acc: 0.00%] [G loss: 0.328414]\n",
            "Epoch: 77.39 [D loss: 1.384726, acc: 0.00%] [G loss: 0.326426]\n",
            "Epoch: 77.40 [D loss: 1.379138, acc: 0.00%] [G loss: 0.326834]\n",
            "Epoch: 77.41 [D loss: 1.337591, acc: 0.00%] [G loss: 0.326946]\n",
            "Epoch: 77.42 [D loss: 1.340922, acc: 0.00%] [G loss: 0.326798]\n",
            "Epoch: 77.43 [D loss: 1.356734, acc: 0.00%] [G loss: 0.326047]\n",
            "Epoch: 77.44 [D loss: 1.356904, acc: 0.00%] [G loss: 0.327491]\n",
            "Epoch: 77.45 [D loss: 1.348317, acc: 0.00%] [G loss: 0.328241]\n",
            "Epoch: 77.46 [D loss: 1.425152, acc: 0.00%] [G loss: 0.328079]\n",
            "Epoch: 77.47 [D loss: 1.393124, acc: 0.00%] [G loss: 0.327277]\n",
            "Epoch: 77.48 [D loss: 1.426783, acc: 0.00%] [G loss: 0.328037]\n",
            "Epoch: 77.49 [D loss: 1.354401, acc: 0.00%] [G loss: 0.326935]\n",
            "Epoch: 77.50 [D loss: 1.370801, acc: 0.00%] [G loss: 0.327421]\n",
            "Epoch: 77.51 [D loss: 1.420778, acc: 0.00%] [G loss: 0.327704]\n",
            "Epoch: 77.52 [D loss: 1.357001, acc: 0.00%] [G loss: 0.327510]\n",
            "Epoch: 77.53 [D loss: 1.371448, acc: 0.00%] [G loss: 0.327069]\n",
            "Epoch: 77.54 [D loss: 1.399012, acc: 0.00%] [G loss: 0.328539]\n",
            "Epoch: 77.55 [D loss: 1.352845, acc: 0.00%] [G loss: 0.327706]\n",
            "Epoch: 77.56 [D loss: 1.393917, acc: 0.00%] [G loss: 0.328095]\n",
            "Epoch: 77.57 [D loss: 1.435160, acc: 0.00%] [G loss: 0.327387]\n",
            "Epoch: 77.58 [D loss: 1.395434, acc: 0.00%] [G loss: 0.327320]\n",
            "Epoch: 77.59 [D loss: 1.414755, acc: 0.00%] [G loss: 0.326726]\n",
            "Epoch: 77.60 [D loss: 1.328275, acc: 0.00%] [G loss: 0.327120]\n",
            "Epoch: 77.61 [D loss: 1.404547, acc: 0.00%] [G loss: 0.326999]\n",
            "Epoch: 77.62 [D loss: 1.371105, acc: 0.00%] [G loss: 0.327406]\n",
            "Epoch: 77.63 [D loss: 1.370022, acc: 0.00%] [G loss: 0.328052]\n",
            "Epoch: 77.64 [D loss: 1.317536, acc: 0.00%] [G loss: 0.326318]\n",
            "Epoch: 77.65 [D loss: 1.351394, acc: 0.00%] [G loss: 0.327625]\n",
            "Epoch: 77.66 [D loss: 1.373494, acc: 0.00%] [G loss: 0.327064]\n",
            "Epoch: 77.67 [D loss: 1.354569, acc: 0.00%] [G loss: 0.328340]\n",
            "Epoch: 77.68 [D loss: 1.331748, acc: 0.00%] [G loss: 0.327015]\n",
            "Epoch: 77.69 [D loss: 1.371285, acc: 0.00%] [G loss: 0.326953]\n",
            "Epoch: 77.70 [D loss: 1.387143, acc: 0.00%] [G loss: 0.326958]\n",
            "Epoch: 77.71 [D loss: 1.446028, acc: 0.00%] [G loss: 0.328310]\n",
            "Epoch: 77.72 [D loss: 1.425344, acc: 0.00%] [G loss: 0.328977]\n",
            "Epoch: 77.73 [D loss: 1.367296, acc: 0.00%] [G loss: 0.327066]\n",
            "Epoch: 77.74 [D loss: 1.319838, acc: 0.00%] [G loss: 0.328273]\n",
            "Epoch: 77.75 [D loss: 1.321800, acc: 0.00%] [G loss: 0.329249]\n",
            "Epoch: 77.76 [D loss: 1.376523, acc: 0.00%] [G loss: 0.327385]\n",
            "Epoch: 77.77 [D loss: 1.329207, acc: 0.00%] [G loss: 0.326753]\n",
            "Epoch: 77.78 [D loss: 1.340969, acc: 0.00%] [G loss: 0.328995]\n",
            "Epoch: 77.79 [D loss: 1.335240, acc: 0.00%] [G loss: 0.327056]\n",
            "Epoch: 77.80 [D loss: 1.356143, acc: 0.00%] [G loss: 0.327103]\n",
            "Epoch: 77.81 [D loss: 1.395320, acc: 0.00%] [G loss: 0.326748]\n",
            "Epoch: 77.82 [D loss: 1.384666, acc: 0.00%] [G loss: 0.328001]\n",
            "Epoch: 77.83 [D loss: 1.398399, acc: 0.00%] [G loss: 0.327687]\n",
            "Epoch: 77.84 [D loss: 1.387308, acc: 0.00%] [G loss: 0.326872]\n",
            "Epoch: 77.85 [D loss: 1.408594, acc: 0.00%] [G loss: 0.327294]\n",
            "Epoch: 77.86 [D loss: 1.357477, acc: 0.00%] [G loss: 0.327086]\n",
            "Epoch: 77.87 [D loss: 1.363714, acc: 0.00%] [G loss: 0.327156]\n",
            "Epoch: 77.88 [D loss: 1.324098, acc: 0.00%] [G loss: 0.327291]\n",
            "Epoch: 77.89 [D loss: 1.402862, acc: 0.00%] [G loss: 0.326722]\n",
            "Epoch: 77.90 [D loss: 1.390785, acc: 0.00%] [G loss: 0.326951]\n",
            "Epoch: 77.91 [D loss: 1.366354, acc: 0.00%] [G loss: 0.328134]\n",
            "Epoch: 77.92 [D loss: 1.414896, acc: 0.00%] [G loss: 0.326658]\n",
            "Epoch: 77.93 [D loss: 1.389156, acc: 0.00%] [G loss: 0.327709]\n",
            "Epoch: 77.94 [D loss: 1.314337, acc: 0.00%] [G loss: 0.326989]\n",
            "Epoch: 77.95 [D loss: 1.391333, acc: 0.00%] [G loss: 0.327560]\n",
            "Epoch: 77.96 [D loss: 1.297705, acc: 0.00%] [G loss: 0.327594]\n",
            "Epoch: 77.97 [D loss: 1.307233, acc: 0.00%] [G loss: 0.328911]\n",
            "Epoch: 77.98 [D loss: 1.304199, acc: 0.00%] [G loss: 0.328579]\n",
            "Epoch: 77.99 [D loss: 1.314022, acc: 0.00%] [G loss: 0.328216]\n",
            "Epoch: 77.100 [D loss: 1.384112, acc: 0.00%] [G loss: 0.327230]\n",
            "Epoch: 77.101 [D loss: 1.396514, acc: 0.00%] [G loss: 0.328171]\n",
            "Epoch: 77.102 [D loss: 1.451347, acc: 0.00%] [G loss: 0.329281]\n",
            "Epoch: 77.103 [D loss: 1.432281, acc: 0.00%] [G loss: 0.327315]\n",
            "Epoch: 77.104 [D loss: 1.418097, acc: 0.00%] [G loss: 0.328013]\n",
            "Epoch: 77.105 [D loss: 1.378836, acc: 0.00%] [G loss: 0.326950]\n",
            "Epoch: 77.106 [D loss: 1.333003, acc: 0.00%] [G loss: 0.326964]\n",
            "Epoch: 77.107 [D loss: 1.345105, acc: 0.00%] [G loss: 0.327764]\n",
            "Epoch: 78.0 [D loss: 1.369276, acc: 0.00%] [G loss: 0.327912]\n",
            "Epoch: 78.1 [D loss: 1.359761, acc: 0.00%] [G loss: 0.327085]\n",
            "Epoch: 78.2 [D loss: 1.400511, acc: 0.00%] [G loss: 0.326704]\n",
            "Epoch: 78.3 [D loss: 1.357201, acc: 0.00%] [G loss: 0.328401]\n",
            "Epoch: 78.4 [D loss: 1.365027, acc: 0.00%] [G loss: 0.327073]\n",
            "Epoch: 78.5 [D loss: 1.402833, acc: 0.00%] [G loss: 0.326737]\n",
            "Epoch: 78.6 [D loss: 1.351212, acc: 0.00%] [G loss: 0.328596]\n",
            "Epoch: 78.7 [D loss: 1.357559, acc: 0.00%] [G loss: 0.327296]\n",
            "Epoch: 78.8 [D loss: 1.366774, acc: 0.00%] [G loss: 0.327586]\n",
            "Epoch: 78.9 [D loss: 1.382746, acc: 0.00%] [G loss: 0.329031]\n",
            "Epoch: 78.10 [D loss: 1.357764, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 78.11 [D loss: 1.414188, acc: 0.00%] [G loss: 0.326535]\n",
            "Epoch: 78.12 [D loss: 1.405860, acc: 0.00%] [G loss: 0.327813]\n",
            "Epoch: 78.13 [D loss: 1.414892, acc: 0.00%] [G loss: 0.327092]\n",
            "Epoch: 78.14 [D loss: 1.351489, acc: 0.00%] [G loss: 0.327149]\n",
            "Epoch: 78.15 [D loss: 1.335596, acc: 0.00%] [G loss: 0.326753]\n",
            "Epoch: 78.16 [D loss: 1.354076, acc: 0.00%] [G loss: 0.328174]\n",
            "Epoch: 78.17 [D loss: 1.386138, acc: 0.00%] [G loss: 0.328989]\n",
            "Epoch: 78.18 [D loss: 1.261807, acc: 0.00%] [G loss: 0.326985]\n",
            "Epoch: 78.19 [D loss: 1.303733, acc: 0.00%] [G loss: 0.327295]\n",
            "Epoch: 78.20 [D loss: 1.341237, acc: 0.00%] [G loss: 0.327811]\n",
            "Epoch: 78.21 [D loss: 1.389123, acc: 0.00%] [G loss: 0.326507]\n",
            "Epoch: 78.22 [D loss: 1.382769, acc: 0.00%] [G loss: 0.327057]\n",
            "Epoch: 78.23 [D loss: 1.402391, acc: 0.00%] [G loss: 0.326886]\n",
            "Epoch: 78.24 [D loss: 1.448771, acc: 0.00%] [G loss: 0.328539]\n",
            "Epoch: 78.25 [D loss: 1.411296, acc: 0.00%] [G loss: 0.327851]\n",
            "Epoch: 78.26 [D loss: 1.359693, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 78.27 [D loss: 1.382251, acc: 0.00%] [G loss: 0.328300]\n",
            "Epoch: 78.28 [D loss: 1.364420, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 78.29 [D loss: 1.356133, acc: 0.00%] [G loss: 0.327301]\n",
            "Epoch: 78.30 [D loss: 1.379003, acc: 0.00%] [G loss: 0.328172]\n",
            "Epoch: 78.31 [D loss: 1.343000, acc: 0.00%] [G loss: 0.327659]\n",
            "Epoch: 78.32 [D loss: 1.326207, acc: 0.00%] [G loss: 0.328505]\n",
            "Epoch: 78.33 [D loss: 1.401205, acc: 0.00%] [G loss: 0.327321]\n",
            "Epoch: 78.34 [D loss: 1.355218, acc: 0.00%] [G loss: 0.327855]\n",
            "Epoch: 78.35 [D loss: 1.387064, acc: 0.00%] [G loss: 0.328549]\n",
            "Epoch: 78.36 [D loss: 1.405116, acc: 0.00%] [G loss: 0.329084]\n",
            "Epoch: 78.37 [D loss: 1.402145, acc: 0.00%] [G loss: 0.327528]\n",
            "Epoch: 78.38 [D loss: 1.396490, acc: 0.00%] [G loss: 0.327027]\n",
            "Epoch: 78.39 [D loss: 1.374893, acc: 0.00%] [G loss: 0.329216]\n",
            "Epoch: 78.40 [D loss: 1.323161, acc: 0.00%] [G loss: 0.327627]\n",
            "Epoch: 78.41 [D loss: 1.296557, acc: 0.00%] [G loss: 0.330074]\n",
            "Epoch: 78.42 [D loss: 1.302747, acc: 0.00%] [G loss: 0.328887]\n",
            "Epoch: 78.43 [D loss: 1.357799, acc: 0.00%] [G loss: 0.327358]\n",
            "Epoch: 78.44 [D loss: 1.330828, acc: 0.00%] [G loss: 0.327986]\n",
            "Epoch: 78.45 [D loss: 1.389704, acc: 0.00%] [G loss: 0.326925]\n",
            "Epoch: 78.46 [D loss: 1.360773, acc: 0.00%] [G loss: 0.327997]\n",
            "Epoch: 78.47 [D loss: 1.385041, acc: 0.00%] [G loss: 0.327533]\n",
            "Epoch: 78.48 [D loss: 1.378968, acc: 0.00%] [G loss: 0.327147]\n",
            "Epoch: 78.49 [D loss: 1.318437, acc: 0.00%] [G loss: 0.327043]\n",
            "Epoch: 78.50 [D loss: 1.326640, acc: 0.00%] [G loss: 0.327795]\n",
            "Epoch: 78.51 [D loss: 1.273850, acc: 0.00%] [G loss: 0.328789]\n",
            "Epoch: 78.52 [D loss: 1.352743, acc: 0.00%] [G loss: 0.326688]\n",
            "Epoch: 78.53 [D loss: 1.328145, acc: 0.00%] [G loss: 0.327914]\n",
            "Epoch: 78.54 [D loss: 1.335247, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 78.55 [D loss: 1.405824, acc: 0.00%] [G loss: 0.330064]\n",
            "Epoch: 78.56 [D loss: 1.409718, acc: 0.00%] [G loss: 0.327808]\n",
            "Epoch: 78.57 [D loss: 1.375757, acc: 0.00%] [G loss: 0.327605]\n",
            "Epoch: 78.58 [D loss: 1.446190, acc: 0.00%] [G loss: 0.326848]\n",
            "Epoch: 78.59 [D loss: 1.362411, acc: 0.00%] [G loss: 0.328873]\n",
            "Epoch: 78.60 [D loss: 1.421959, acc: 0.00%] [G loss: 0.327776]\n",
            "Epoch: 78.61 [D loss: 1.379643, acc: 0.00%] [G loss: 0.328639]\n",
            "Epoch: 78.62 [D loss: 1.359476, acc: 0.00%] [G loss: 0.328594]\n",
            "Epoch: 78.63 [D loss: 1.338847, acc: 0.00%] [G loss: 0.327182]\n",
            "Epoch: 78.64 [D loss: 1.336521, acc: 0.00%] [G loss: 0.327293]\n",
            "Epoch: 78.65 [D loss: 1.398218, acc: 0.00%] [G loss: 0.327230]\n",
            "Epoch: 78.66 [D loss: 1.378084, acc: 0.00%] [G loss: 0.327525]\n",
            "Epoch: 78.67 [D loss: 1.346356, acc: 0.00%] [G loss: 0.327060]\n",
            "Epoch: 78.68 [D loss: 1.375958, acc: 0.00%] [G loss: 0.327634]\n",
            "Epoch: 78.69 [D loss: 1.372310, acc: 0.00%] [G loss: 0.329262]\n",
            "Epoch: 78.70 [D loss: 1.395961, acc: 0.00%] [G loss: 0.327822]\n",
            "Epoch: 78.71 [D loss: 1.391404, acc: 0.00%] [G loss: 0.326733]\n",
            "Epoch: 78.72 [D loss: 1.414955, acc: 0.00%] [G loss: 0.326085]\n",
            "Epoch: 78.73 [D loss: 1.454669, acc: 0.00%] [G loss: 0.327775]\n",
            "Epoch: 78.74 [D loss: 1.381460, acc: 0.00%] [G loss: 0.327467]\n",
            "Epoch: 78.75 [D loss: 1.392045, acc: 0.00%] [G loss: 0.327092]\n",
            "Epoch: 78.76 [D loss: 1.372922, acc: 0.00%] [G loss: 0.327102]\n",
            "Epoch: 78.77 [D loss: 1.377511, acc: 0.00%] [G loss: 0.327875]\n",
            "Epoch: 78.78 [D loss: 1.326314, acc: 0.00%] [G loss: 0.326815]\n",
            "Epoch: 78.79 [D loss: 1.351088, acc: 0.00%] [G loss: 0.326664]\n",
            "Epoch: 78.80 [D loss: 1.354305, acc: 0.00%] [G loss: 0.326830]\n",
            "Epoch: 78.81 [D loss: 1.311740, acc: 0.00%] [G loss: 0.326844]\n",
            "Epoch: 78.82 [D loss: 1.336621, acc: 0.00%] [G loss: 0.327111]\n",
            "Epoch: 78.83 [D loss: 1.308887, acc: 0.00%] [G loss: 0.326520]\n",
            "Epoch: 78.84 [D loss: 1.315333, acc: 0.00%] [G loss: 0.328363]\n",
            "Epoch: 78.85 [D loss: 1.381558, acc: 0.00%] [G loss: 0.328324]\n",
            "Epoch: 78.86 [D loss: 1.371035, acc: 0.00%] [G loss: 0.327468]\n",
            "Epoch: 78.87 [D loss: 1.437319, acc: 0.00%] [G loss: 0.328447]\n",
            "Epoch: 78.88 [D loss: 1.326873, acc: 0.00%] [G loss: 0.327766]\n",
            "Epoch: 78.89 [D loss: 1.409193, acc: 0.00%] [G loss: 0.327841]\n",
            "Epoch: 78.90 [D loss: 1.455178, acc: 0.00%] [G loss: 0.327932]\n",
            "Epoch: 78.91 [D loss: 1.387104, acc: 0.00%] [G loss: 0.328247]\n",
            "Epoch: 78.92 [D loss: 1.350772, acc: 0.00%] [G loss: 0.326381]\n",
            "Epoch: 78.93 [D loss: 1.365556, acc: 0.00%] [G loss: 0.328053]\n",
            "Epoch: 78.94 [D loss: 1.320971, acc: 0.00%] [G loss: 0.327567]\n",
            "Epoch: 78.95 [D loss: 1.313685, acc: 0.00%] [G loss: 0.326737]\n",
            "Epoch: 78.96 [D loss: 1.393122, acc: 0.00%] [G loss: 0.328092]\n",
            "Epoch: 78.97 [D loss: 1.336302, acc: 0.00%] [G loss: 0.327839]\n",
            "Epoch: 78.98 [D loss: 1.339004, acc: 0.00%] [G loss: 0.327978]\n",
            "Epoch: 78.99 [D loss: 1.351334, acc: 0.00%] [G loss: 0.327389]\n",
            "Epoch: 78.100 [D loss: 1.343189, acc: 0.00%] [G loss: 0.327179]\n",
            "Epoch: 78.101 [D loss: 1.383690, acc: 0.00%] [G loss: 0.328958]\n",
            "Epoch: 78.102 [D loss: 1.394598, acc: 0.00%] [G loss: 0.327133]\n",
            "Epoch: 78.103 [D loss: 1.377130, acc: 0.00%] [G loss: 0.327761]\n",
            "Epoch: 78.104 [D loss: 1.376025, acc: 0.00%] [G loss: 0.326794]\n",
            "Epoch: 78.105 [D loss: 1.321066, acc: 0.00%] [G loss: 0.328244]\n",
            "Epoch: 78.106 [D loss: 1.346022, acc: 0.00%] [G loss: 0.327913]\n",
            "Epoch: 78.107 [D loss: 1.407207, acc: 0.00%] [G loss: 0.327713]\n",
            "Epoch: 79.0 [D loss: 1.356224, acc: 0.00%] [G loss: 0.326555]\n",
            "Epoch: 79.1 [D loss: 1.378673, acc: 0.00%] [G loss: 0.327392]\n",
            "Epoch: 79.2 [D loss: 1.371210, acc: 0.00%] [G loss: 0.327059]\n",
            "Epoch: 79.3 [D loss: 1.409208, acc: 0.00%] [G loss: 0.327519]\n",
            "Epoch: 79.4 [D loss: 1.412611, acc: 0.00%] [G loss: 0.326542]\n",
            "Epoch: 79.5 [D loss: 1.383864, acc: 0.00%] [G loss: 0.327506]\n",
            "Epoch: 79.6 [D loss: 1.367288, acc: 0.00%] [G loss: 0.328053]\n",
            "Epoch: 79.7 [D loss: 1.369762, acc: 0.00%] [G loss: 0.326934]\n",
            "Epoch: 79.8 [D loss: 1.365745, acc: 0.00%] [G loss: 0.327331]\n",
            "Epoch: 79.9 [D loss: 1.400116, acc: 0.00%] [G loss: 0.326717]\n",
            "Epoch: 79.10 [D loss: 1.390046, acc: 0.00%] [G loss: 0.326925]\n",
            "Epoch: 79.11 [D loss: 1.382763, acc: 0.00%] [G loss: 0.327738]\n",
            "Epoch: 79.12 [D loss: 1.379730, acc: 0.00%] [G loss: 0.327413]\n",
            "Epoch: 79.13 [D loss: 1.421498, acc: 0.00%] [G loss: 0.327420]\n",
            "Epoch: 79.14 [D loss: 1.387228, acc: 0.00%] [G loss: 0.327706]\n",
            "Epoch: 79.15 [D loss: 1.371486, acc: 0.00%] [G loss: 0.327433]\n",
            "Epoch: 79.16 [D loss: 1.321982, acc: 0.00%] [G loss: 0.328606]\n",
            "Epoch: 79.17 [D loss: 1.358207, acc: 0.00%] [G loss: 0.326610]\n",
            "Epoch: 79.18 [D loss: 1.349370, acc: 0.00%] [G loss: 0.328479]\n",
            "Epoch: 79.19 [D loss: 1.304334, acc: 0.00%] [G loss: 0.327734]\n",
            "Epoch: 79.20 [D loss: 1.299692, acc: 0.00%] [G loss: 0.328845]\n",
            "Epoch: 79.21 [D loss: 1.402693, acc: 0.00%] [G loss: 0.327573]\n",
            "Epoch: 79.22 [D loss: 1.408485, acc: 0.00%] [G loss: 0.327657]\n",
            "Epoch: 79.23 [D loss: 1.442958, acc: 0.00%] [G loss: 0.327487]\n",
            "Epoch: 79.24 [D loss: 1.391081, acc: 0.00%] [G loss: 0.327746]\n",
            "Epoch: 79.25 [D loss: 1.441993, acc: 0.00%] [G loss: 0.327514]\n",
            "Epoch: 79.26 [D loss: 1.341146, acc: 0.00%] [G loss: 0.327836]\n",
            "Epoch: 79.27 [D loss: 1.337059, acc: 0.00%] [G loss: 0.327656]\n",
            "Epoch: 79.28 [D loss: 1.283333, acc: 0.00%] [G loss: 0.327396]\n",
            "Epoch: 79.29 [D loss: 1.262720, acc: 0.00%] [G loss: 0.327201]\n",
            "Epoch: 79.30 [D loss: 1.305596, acc: 0.00%] [G loss: 0.331193]\n",
            "Epoch: 79.31 [D loss: 1.306431, acc: 0.00%] [G loss: 0.326555]\n",
            "Epoch: 79.32 [D loss: 1.400238, acc: 0.00%] [G loss: 0.328253]\n",
            "Epoch: 79.33 [D loss: 1.490434, acc: 0.00%] [G loss: 0.329060]\n",
            "Epoch: 79.34 [D loss: 1.425731, acc: 0.00%] [G loss: 0.329423]\n",
            "Epoch: 79.35 [D loss: 1.454162, acc: 0.00%] [G loss: 0.328696]\n",
            "Epoch: 79.36 [D loss: 1.410591, acc: 0.00%] [G loss: 0.327018]\n",
            "Epoch: 79.37 [D loss: 1.368213, acc: 0.00%] [G loss: 0.328643]\n",
            "Epoch: 79.38 [D loss: 1.330552, acc: 0.00%] [G loss: 0.326872]\n",
            "Epoch: 79.39 [D loss: 1.345288, acc: 0.00%] [G loss: 0.328877]\n",
            "Epoch: 79.40 [D loss: 1.273026, acc: 0.00%] [G loss: 0.328946]\n",
            "Epoch: 79.41 [D loss: 1.278375, acc: 0.00%] [G loss: 0.329624]\n",
            "Epoch: 79.42 [D loss: 1.358603, acc: 0.00%] [G loss: 0.327953]\n",
            "Epoch: 79.43 [D loss: 1.389040, acc: 0.00%] [G loss: 0.326762]\n",
            "Epoch: 79.44 [D loss: 1.473526, acc: 0.00%] [G loss: 0.327194]\n",
            "Epoch: 79.45 [D loss: 1.467714, acc: 0.00%] [G loss: 0.329076]\n",
            "Epoch: 79.46 [D loss: 1.423453, acc: 0.00%] [G loss: 0.328972]\n",
            "Epoch: 79.47 [D loss: 1.421534, acc: 0.00%] [G loss: 0.328521]\n",
            "Epoch: 79.48 [D loss: 1.382622, acc: 0.00%] [G loss: 0.328022]\n",
            "Epoch: 79.49 [D loss: 1.341039, acc: 0.00%] [G loss: 0.326863]\n",
            "Epoch: 79.50 [D loss: 1.338316, acc: 0.00%] [G loss: 0.328073]\n",
            "Epoch: 79.51 [D loss: 1.310581, acc: 0.00%] [G loss: 0.330799]\n",
            "Epoch: 79.52 [D loss: 1.333963, acc: 0.00%] [G loss: 0.328239]\n",
            "Epoch: 79.53 [D loss: 1.322441, acc: 0.00%] [G loss: 0.328181]\n",
            "Epoch: 79.54 [D loss: 1.373877, acc: 0.00%] [G loss: 0.327315]\n",
            "Epoch: 79.55 [D loss: 1.367710, acc: 0.00%] [G loss: 0.329270]\n",
            "Epoch: 79.56 [D loss: 1.387755, acc: 0.00%] [G loss: 0.329834]\n",
            "Epoch: 79.57 [D loss: 1.400139, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 79.58 [D loss: 1.481996, acc: 0.00%] [G loss: 0.327162]\n",
            "Epoch: 79.59 [D loss: 1.345026, acc: 0.00%] [G loss: 0.328931]\n",
            "Epoch: 79.60 [D loss: 1.394071, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 79.61 [D loss: 1.323888, acc: 0.00%] [G loss: 0.327911]\n",
            "Epoch: 79.62 [D loss: 1.320342, acc: 0.00%] [G loss: 0.327474]\n",
            "Epoch: 79.63 [D loss: 1.331928, acc: 0.00%] [G loss: 0.327860]\n",
            "Epoch: 79.64 [D loss: 1.347418, acc: 0.00%] [G loss: 0.326812]\n",
            "Epoch: 79.65 [D loss: 1.376117, acc: 0.00%] [G loss: 0.326765]\n",
            "Epoch: 79.66 [D loss: 1.462338, acc: 0.00%] [G loss: 0.326839]\n",
            "Epoch: 79.67 [D loss: 1.390747, acc: 0.00%] [G loss: 0.327145]\n",
            "Epoch: 79.68 [D loss: 1.486621, acc: 0.00%] [G loss: 0.330020]\n",
            "Epoch: 79.69 [D loss: 1.438310, acc: 0.00%] [G loss: 0.328266]\n",
            "Epoch: 79.70 [D loss: 1.386656, acc: 0.00%] [G loss: 0.326908]\n",
            "Epoch: 79.71 [D loss: 1.350582, acc: 0.00%] [G loss: 0.328651]\n",
            "Epoch: 79.72 [D loss: 1.313261, acc: 0.00%] [G loss: 0.329329]\n",
            "Epoch: 79.73 [D loss: 1.316838, acc: 0.00%] [G loss: 0.327434]\n",
            "Epoch: 79.74 [D loss: 1.320966, acc: 0.00%] [G loss: 0.327983]\n",
            "Epoch: 79.75 [D loss: 1.308504, acc: 0.00%] [G loss: 0.327617]\n",
            "Epoch: 79.76 [D loss: 1.339582, acc: 0.00%] [G loss: 0.327298]\n",
            "Epoch: 79.77 [D loss: 1.394196, acc: 0.00%] [G loss: 0.327565]\n",
            "Epoch: 79.78 [D loss: 1.404758, acc: 0.00%] [G loss: 0.326707]\n",
            "Epoch: 79.79 [D loss: 1.384703, acc: 0.00%] [G loss: 0.328015]\n",
            "Epoch: 79.80 [D loss: 1.362271, acc: 0.00%] [G loss: 0.327263]\n",
            "Epoch: 79.81 [D loss: 1.380460, acc: 0.00%] [G loss: 0.327607]\n",
            "Epoch: 79.82 [D loss: 1.356840, acc: 0.00%] [G loss: 0.327349]\n",
            "Epoch: 79.83 [D loss: 1.351613, acc: 0.00%] [G loss: 0.326335]\n",
            "Epoch: 79.84 [D loss: 1.346369, acc: 0.00%] [G loss: 0.327938]\n",
            "Epoch: 79.85 [D loss: 1.324514, acc: 0.00%] [G loss: 0.328941]\n",
            "Epoch: 79.86 [D loss: 1.366623, acc: 0.00%] [G loss: 0.327761]\n",
            "Epoch: 79.87 [D loss: 1.373421, acc: 0.00%] [G loss: 0.326740]\n",
            "Epoch: 79.88 [D loss: 1.412388, acc: 0.00%] [G loss: 0.328566]\n",
            "Epoch: 79.89 [D loss: 1.403099, acc: 0.00%] [G loss: 0.327838]\n",
            "Epoch: 79.90 [D loss: 1.414959, acc: 0.00%] [G loss: 0.327836]\n",
            "Epoch: 79.91 [D loss: 1.424951, acc: 0.00%] [G loss: 0.328253]\n",
            "Epoch: 79.92 [D loss: 1.394155, acc: 0.00%] [G loss: 0.326713]\n",
            "Epoch: 79.93 [D loss: 1.369671, acc: 0.00%] [G loss: 0.326973]\n",
            "Epoch: 79.94 [D loss: 1.351509, acc: 0.00%] [G loss: 0.328189]\n",
            "Epoch: 79.95 [D loss: 1.297528, acc: 0.00%] [G loss: 0.328351]\n",
            "Epoch: 79.96 [D loss: 1.296477, acc: 0.00%] [G loss: 0.328372]\n",
            "Epoch: 79.97 [D loss: 1.317572, acc: 0.00%] [G loss: 0.327916]\n",
            "Epoch: 79.98 [D loss: 1.381256, acc: 0.00%] [G loss: 0.327260]\n",
            "Epoch: 79.99 [D loss: 1.335149, acc: 0.00%] [G loss: 0.326804]\n",
            "Epoch: 79.100 [D loss: 1.371509, acc: 0.00%] [G loss: 0.329047]\n",
            "Epoch: 79.101 [D loss: 1.401808, acc: 0.00%] [G loss: 0.327082]\n",
            "Epoch: 79.102 [D loss: 1.404174, acc: 0.00%] [G loss: 0.327543]\n",
            "Epoch: 79.103 [D loss: 1.313831, acc: 0.00%] [G loss: 0.327217]\n",
            "Epoch: 79.104 [D loss: 1.404732, acc: 0.00%] [G loss: 0.327680]\n",
            "Epoch: 79.105 [D loss: 1.365746, acc: 0.00%] [G loss: 0.326374]\n",
            "Epoch: 79.106 [D loss: 1.359378, acc: 0.00%] [G loss: 0.328079]\n",
            "Epoch: 79.107 [D loss: 1.317544, acc: 0.00%] [G loss: 0.327662]\n",
            "Epoch: 80.0 [D loss: 1.305500, acc: 0.00%] [G loss: 0.328955]\n",
            "Epoch: 80.1 [D loss: 1.342052, acc: 0.00%] [G loss: 0.327497]\n",
            "Epoch: 80.2 [D loss: 1.337724, acc: 0.00%] [G loss: 0.326767]\n",
            "Epoch: 80.3 [D loss: 1.350864, acc: 0.00%] [G loss: 0.326946]\n",
            "Epoch: 80.4 [D loss: 1.420215, acc: 0.00%] [G loss: 0.327252]\n",
            "Epoch: 80.5 [D loss: 1.426381, acc: 0.00%] [G loss: 0.327327]\n",
            "Epoch: 80.6 [D loss: 1.419662, acc: 0.00%] [G loss: 0.328024]\n",
            "Epoch: 80.7 [D loss: 1.382814, acc: 0.00%] [G loss: 0.327183]\n",
            "Epoch: 80.8 [D loss: 1.371157, acc: 0.00%] [G loss: 0.327418]\n",
            "Epoch: 80.9 [D loss: 1.338877, acc: 0.00%] [G loss: 0.327789]\n",
            "Epoch: 80.10 [D loss: 1.385481, acc: 0.00%] [G loss: 0.327486]\n",
            "Epoch: 80.11 [D loss: 1.316091, acc: 0.00%] [G loss: 0.327424]\n",
            "Epoch: 80.12 [D loss: 1.384152, acc: 0.00%] [G loss: 0.327274]\n",
            "Epoch: 80.13 [D loss: 1.371734, acc: 0.00%] [G loss: 0.327634]\n",
            "Epoch: 80.14 [D loss: 1.343016, acc: 0.00%] [G loss: 0.328116]\n",
            "Epoch: 80.15 [D loss: 1.378493, acc: 0.00%] [G loss: 0.327764]\n",
            "Epoch: 80.16 [D loss: 1.406310, acc: 0.00%] [G loss: 0.327309]\n",
            "Epoch: 80.17 [D loss: 1.428970, acc: 0.00%] [G loss: 0.328180]\n",
            "Epoch: 80.18 [D loss: 1.386094, acc: 0.00%] [G loss: 0.327411]\n",
            "Epoch: 80.19 [D loss: 1.405620, acc: 0.00%] [G loss: 0.326659]\n",
            "Epoch: 80.20 [D loss: 1.340379, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 80.21 [D loss: 1.284060, acc: 0.00%] [G loss: 0.327189]\n",
            "Epoch: 80.22 [D loss: 1.311036, acc: 0.00%] [G loss: 0.328046]\n",
            "Epoch: 80.23 [D loss: 1.349928, acc: 0.00%] [G loss: 0.327397]\n",
            "Epoch: 80.24 [D loss: 1.337242, acc: 0.00%] [G loss: 0.327801]\n",
            "Epoch: 80.25 [D loss: 1.342287, acc: 0.00%] [G loss: 0.326423]\n",
            "Epoch: 80.26 [D loss: 1.342027, acc: 0.00%] [G loss: 0.326955]\n",
            "Epoch: 80.27 [D loss: 1.405508, acc: 0.00%] [G loss: 0.329175]\n",
            "Epoch: 80.28 [D loss: 1.431177, acc: 0.00%] [G loss: 0.327926]\n",
            "Epoch: 80.29 [D loss: 1.427596, acc: 0.00%] [G loss: 0.328661]\n",
            "Epoch: 80.30 [D loss: 1.358720, acc: 0.00%] [G loss: 0.326940]\n",
            "Epoch: 80.31 [D loss: 1.320334, acc: 0.00%] [G loss: 0.328223]\n",
            "Epoch: 80.32 [D loss: 1.353807, acc: 0.00%] [G loss: 0.328633]\n",
            "Epoch: 80.33 [D loss: 1.326203, acc: 0.00%] [G loss: 0.326599]\n",
            "Epoch: 80.34 [D loss: 1.366709, acc: 0.00%] [G loss: 0.326121]\n",
            "Epoch: 80.35 [D loss: 1.386249, acc: 0.00%] [G loss: 0.327948]\n",
            "Epoch: 80.36 [D loss: 1.394066, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 80.37 [D loss: 1.443495, acc: 0.00%] [G loss: 0.327983]\n",
            "Epoch: 80.38 [D loss: 1.417265, acc: 0.00%] [G loss: 0.326635]\n",
            "Epoch: 80.39 [D loss: 1.361683, acc: 0.00%] [G loss: 0.326895]\n",
            "Epoch: 80.40 [D loss: 1.354957, acc: 0.00%] [G loss: 0.328657]\n",
            "Epoch: 80.41 [D loss: 1.396064, acc: 0.00%] [G loss: 0.328491]\n",
            "Epoch: 80.42 [D loss: 1.382630, acc: 0.00%] [G loss: 0.327960]\n",
            "Epoch: 80.43 [D loss: 1.370402, acc: 0.00%] [G loss: 0.327196]\n",
            "Epoch: 80.44 [D loss: 1.375888, acc: 0.00%] [G loss: 0.328038]\n",
            "Epoch: 80.45 [D loss: 1.364824, acc: 0.00%] [G loss: 0.329259]\n",
            "Epoch: 80.46 [D loss: 1.331711, acc: 0.00%] [G loss: 0.328271]\n",
            "Epoch: 80.47 [D loss: 1.388166, acc: 0.00%] [G loss: 0.327966]\n",
            "Epoch: 80.48 [D loss: 1.423142, acc: 0.00%] [G loss: 0.329523]\n",
            "Epoch: 80.49 [D loss: 1.431114, acc: 0.00%] [G loss: 0.329087]\n",
            "Epoch: 80.50 [D loss: 1.450353, acc: 0.00%] [G loss: 0.327242]\n",
            "Epoch: 80.51 [D loss: 1.468275, acc: 0.00%] [G loss: 0.329749]\n",
            "Epoch: 80.52 [D loss: 1.372032, acc: 0.00%] [G loss: 0.328504]\n",
            "Epoch: 80.53 [D loss: 1.368203, acc: 0.00%] [G loss: 0.326674]\n",
            "Epoch: 80.54 [D loss: 1.358390, acc: 0.00%] [G loss: 0.327376]\n",
            "Epoch: 80.55 [D loss: 1.308444, acc: 0.00%] [G loss: 0.329691]\n",
            "Epoch: 80.56 [D loss: 1.332129, acc: 0.00%] [G loss: 0.327560]\n",
            "Epoch: 80.57 [D loss: 1.333975, acc: 0.00%] [G loss: 0.328704]\n",
            "Epoch: 80.58 [D loss: 1.365943, acc: 0.00%] [G loss: 0.327802]\n",
            "Epoch: 80.59 [D loss: 1.428235, acc: 0.00%] [G loss: 0.327137]\n",
            "Epoch: 80.60 [D loss: 1.327333, acc: 0.00%] [G loss: 0.327325]\n",
            "Epoch: 80.61 [D loss: 1.381257, acc: 0.00%] [G loss: 0.327701]\n",
            "Epoch: 80.62 [D loss: 1.371714, acc: 0.00%] [G loss: 0.328006]\n",
            "Epoch: 80.63 [D loss: 1.353685, acc: 0.00%] [G loss: 0.328456]\n",
            "Epoch: 80.64 [D loss: 1.384208, acc: 0.00%] [G loss: 0.327649]\n",
            "Epoch: 80.65 [D loss: 1.347849, acc: 0.00%] [G loss: 0.327392]\n",
            "Epoch: 80.66 [D loss: 1.373676, acc: 0.00%] [G loss: 0.327898]\n",
            "Epoch: 80.67 [D loss: 1.323593, acc: 0.00%] [G loss: 0.327021]\n",
            "Epoch: 80.68 [D loss: 1.336736, acc: 0.00%] [G loss: 0.326786]\n",
            "Epoch: 80.69 [D loss: 1.392542, acc: 0.00%] [G loss: 0.327586]\n",
            "Epoch: 80.70 [D loss: 1.365088, acc: 0.00%] [G loss: 0.326048]\n",
            "Epoch: 80.71 [D loss: 1.376142, acc: 0.00%] [G loss: 0.327683]\n",
            "Epoch: 80.72 [D loss: 1.325812, acc: 0.00%] [G loss: 0.326871]\n",
            "Epoch: 80.73 [D loss: 1.378352, acc: 0.00%] [G loss: 0.327499]\n",
            "Epoch: 80.74 [D loss: 1.369851, acc: 0.00%] [G loss: 0.329461]\n",
            "Epoch: 80.75 [D loss: 1.339314, acc: 0.00%] [G loss: 0.327707]\n",
            "Epoch: 80.76 [D loss: 1.356276, acc: 0.00%] [G loss: 0.327690]\n",
            "Epoch: 80.77 [D loss: 1.337683, acc: 0.00%] [G loss: 0.329386]\n",
            "Epoch: 80.78 [D loss: 1.345942, acc: 0.00%] [G loss: 0.328145]\n",
            "Epoch: 80.79 [D loss: 1.356969, acc: 0.00%] [G loss: 0.328279]\n",
            "Epoch: 80.80 [D loss: 1.303440, acc: 0.00%] [G loss: 0.328103]\n",
            "Epoch: 80.81 [D loss: 1.376075, acc: 0.00%] [G loss: 0.327738]\n",
            "Epoch: 80.82 [D loss: 1.378290, acc: 0.00%] [G loss: 0.326883]\n",
            "Epoch: 80.83 [D loss: 1.372390, acc: 0.00%] [G loss: 0.327263]\n",
            "Epoch: 80.84 [D loss: 1.404908, acc: 0.00%] [G loss: 0.327063]\n",
            "Epoch: 80.85 [D loss: 1.344761, acc: 0.00%] [G loss: 0.327296]\n",
            "Epoch: 80.86 [D loss: 1.438785, acc: 0.00%] [G loss: 0.328800]\n",
            "Epoch: 80.87 [D loss: 1.404303, acc: 0.00%] [G loss: 0.327603]\n",
            "Epoch: 80.88 [D loss: 1.362635, acc: 0.00%] [G loss: 0.327268]\n",
            "Epoch: 80.89 [D loss: 1.367587, acc: 0.00%] [G loss: 0.327682]\n",
            "Epoch: 80.90 [D loss: 1.339448, acc: 0.00%] [G loss: 0.326921]\n",
            "Epoch: 80.91 [D loss: 1.403785, acc: 0.00%] [G loss: 0.327365]\n",
            "Epoch: 80.92 [D loss: 1.374455, acc: 0.00%] [G loss: 0.329274]\n",
            "Epoch: 80.93 [D loss: 1.362776, acc: 0.00%] [G loss: 0.328046]\n",
            "Epoch: 80.94 [D loss: 1.395546, acc: 0.00%] [G loss: 0.328608]\n",
            "Epoch: 80.95 [D loss: 1.380056, acc: 0.00%] [G loss: 0.328057]\n",
            "Epoch: 80.96 [D loss: 1.345305, acc: 0.00%] [G loss: 0.326415]\n",
            "Epoch: 80.97 [D loss: 1.349821, acc: 0.00%] [G loss: 0.327150]\n",
            "Epoch: 80.98 [D loss: 1.327484, acc: 0.00%] [G loss: 0.327424]\n",
            "Epoch: 80.99 [D loss: 1.360149, acc: 0.00%] [G loss: 0.328040]\n",
            "Epoch: 80.100 [D loss: 1.344244, acc: 0.00%] [G loss: 0.328666]\n",
            "Epoch: 80.101 [D loss: 1.371239, acc: 0.00%] [G loss: 0.327359]\n",
            "Epoch: 80.102 [D loss: 1.369202, acc: 0.00%] [G loss: 0.326697]\n",
            "Epoch: 80.103 [D loss: 1.357192, acc: 0.00%] [G loss: 0.327217]\n",
            "Epoch: 80.104 [D loss: 1.349460, acc: 0.00%] [G loss: 0.327723]\n",
            "Epoch: 80.105 [D loss: 1.381796, acc: 0.00%] [G loss: 0.327120]\n",
            "Epoch: 80.106 [D loss: 1.341028, acc: 0.00%] [G loss: 0.327693]\n",
            "Epoch: 80.107 [D loss: 1.405411, acc: 0.00%] [G loss: 0.326834]\n",
            "INFO:tensorflow:Assets written to: GAN_weights/dis_0.00000340_weights/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: GAN_weights/gen_0.00000340_weights/assets\n",
            "Epoch: 81.0 [D loss: 1.398904, acc: 0.00%] [G loss: 0.326557]\n",
            "Epoch: 81.1 [D loss: 1.423330, acc: 0.00%] [G loss: 0.327034]\n",
            "Epoch: 81.2 [D loss: 1.389158, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 81.3 [D loss: 1.393202, acc: 0.00%] [G loss: 0.329976]\n",
            "Epoch: 81.4 [D loss: 1.364613, acc: 0.00%] [G loss: 0.327737]\n",
            "Epoch: 81.5 [D loss: 1.353030, acc: 0.00%] [G loss: 0.327792]\n",
            "Epoch: 81.6 [D loss: 1.358056, acc: 0.00%] [G loss: 0.326146]\n",
            "Epoch: 81.7 [D loss: 1.369871, acc: 0.00%] [G loss: 0.327609]\n",
            "Epoch: 81.8 [D loss: 1.288534, acc: 0.00%] [G loss: 0.327741]\n",
            "Epoch: 81.9 [D loss: 1.368042, acc: 0.00%] [G loss: 0.328172]\n",
            "Epoch: 81.10 [D loss: 1.376268, acc: 0.00%] [G loss: 0.327158]\n",
            "Epoch: 81.11 [D loss: 1.378973, acc: 0.00%] [G loss: 0.328141]\n",
            "Epoch: 81.12 [D loss: 1.407461, acc: 0.00%] [G loss: 0.327467]\n",
            "Epoch: 81.13 [D loss: 1.418825, acc: 0.00%] [G loss: 0.327613]\n",
            "Epoch: 81.14 [D loss: 1.433784, acc: 0.00%] [G loss: 0.327545]\n",
            "Epoch: 81.15 [D loss: 1.391681, acc: 0.00%] [G loss: 0.327154]\n",
            "Epoch: 81.16 [D loss: 1.321770, acc: 0.00%] [G loss: 0.326476]\n",
            "Epoch: 81.17 [D loss: 1.353670, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 81.18 [D loss: 1.347293, acc: 0.00%] [G loss: 0.327409]\n",
            "Epoch: 81.19 [D loss: 1.329456, acc: 0.00%] [G loss: 0.327155]\n",
            "Epoch: 81.20 [D loss: 1.345455, acc: 0.00%] [G loss: 0.327903]\n",
            "Epoch: 81.21 [D loss: 1.323437, acc: 0.00%] [G loss: 0.326062]\n",
            "Epoch: 81.22 [D loss: 1.357458, acc: 0.00%] [G loss: 0.327755]\n",
            "Epoch: 81.23 [D loss: 1.346619, acc: 0.00%] [G loss: 0.326894]\n",
            "Epoch: 81.24 [D loss: 1.365862, acc: 0.00%] [G loss: 0.327767]\n",
            "Epoch: 81.25 [D loss: 1.345319, acc: 0.00%] [G loss: 0.327389]\n",
            "Epoch: 81.26 [D loss: 1.355763, acc: 0.00%] [G loss: 0.326702]\n",
            "Epoch: 81.27 [D loss: 1.351497, acc: 0.00%] [G loss: 0.327613]\n",
            "Epoch: 81.28 [D loss: 1.350559, acc: 0.00%] [G loss: 0.327565]\n",
            "Epoch: 81.29 [D loss: 1.347761, acc: 0.00%] [G loss: 0.328103]\n",
            "Epoch: 81.30 [D loss: 1.364584, acc: 0.00%] [G loss: 0.328677]\n",
            "Epoch: 81.31 [D loss: 1.397025, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 81.32 [D loss: 1.420007, acc: 0.00%] [G loss: 0.328040]\n",
            "Epoch: 81.33 [D loss: 1.390051, acc: 0.00%] [G loss: 0.328431]\n",
            "Epoch: 81.34 [D loss: 1.392215, acc: 0.00%] [G loss: 0.327042]\n",
            "Epoch: 81.35 [D loss: 1.343700, acc: 0.00%] [G loss: 0.327471]\n",
            "Epoch: 81.36 [D loss: 1.351873, acc: 0.00%] [G loss: 0.327299]\n",
            "Epoch: 81.37 [D loss: 1.421112, acc: 0.00%] [G loss: 0.326406]\n",
            "Epoch: 81.38 [D loss: 1.295229, acc: 0.00%] [G loss: 0.327906]\n",
            "Epoch: 81.39 [D loss: 1.311155, acc: 0.00%] [G loss: 0.327185]\n",
            "Epoch: 81.40 [D loss: 1.372028, acc: 0.00%] [G loss: 0.326807]\n",
            "Epoch: 81.41 [D loss: 1.412478, acc: 0.00%] [G loss: 0.327156]\n",
            "Epoch: 81.42 [D loss: 1.424875, acc: 0.00%] [G loss: 0.328055]\n",
            "Epoch: 81.43 [D loss: 1.405294, acc: 0.00%] [G loss: 0.327679]\n",
            "Epoch: 81.44 [D loss: 1.398064, acc: 0.00%] [G loss: 0.327159]\n",
            "Epoch: 81.45 [D loss: 1.357190, acc: 0.00%] [G loss: 0.327880]\n",
            "Epoch: 81.46 [D loss: 1.335511, acc: 0.00%] [G loss: 0.327390]\n",
            "Epoch: 81.47 [D loss: 1.377660, acc: 0.00%] [G loss: 0.327108]\n",
            "Epoch: 81.48 [D loss: 1.359103, acc: 0.00%] [G loss: 0.327602]\n",
            "Epoch: 81.49 [D loss: 1.384638, acc: 0.00%] [G loss: 0.327099]\n",
            "Epoch: 81.50 [D loss: 1.337336, acc: 0.00%] [G loss: 0.327065]\n",
            "Epoch: 81.51 [D loss: 1.382454, acc: 0.00%] [G loss: 0.327948]\n",
            "Epoch: 81.52 [D loss: 1.324882, acc: 0.00%] [G loss: 0.328063]\n",
            "Epoch: 81.53 [D loss: 1.423738, acc: 0.00%] [G loss: 0.326477]\n",
            "Epoch: 81.54 [D loss: 1.378607, acc: 0.00%] [G loss: 0.327853]\n",
            "Epoch: 81.55 [D loss: 1.389236, acc: 0.00%] [G loss: 0.327996]\n",
            "Epoch: 81.56 [D loss: 1.344867, acc: 0.00%] [G loss: 0.326543]\n",
            "Epoch: 81.57 [D loss: 1.339583, acc: 0.00%] [G loss: 0.327665]\n",
            "Epoch: 81.58 [D loss: 1.338942, acc: 0.00%] [G loss: 0.327267]\n",
            "Epoch: 81.59 [D loss: 1.365558, acc: 0.00%] [G loss: 0.326874]\n",
            "Epoch: 81.60 [D loss: 1.355924, acc: 0.00%] [G loss: 0.327047]\n",
            "Epoch: 81.61 [D loss: 1.325565, acc: 0.00%] [G loss: 0.327612]\n",
            "Epoch: 81.62 [D loss: 1.376596, acc: 0.00%] [G loss: 0.327117]\n",
            "Epoch: 81.63 [D loss: 1.376628, acc: 0.00%] [G loss: 0.326661]\n",
            "Epoch: 81.64 [D loss: 1.394586, acc: 0.00%] [G loss: 0.329822]\n",
            "Epoch: 81.65 [D loss: 1.401628, acc: 0.00%] [G loss: 0.328574]\n",
            "Epoch: 81.66 [D loss: 1.388657, acc: 0.00%] [G loss: 0.326798]\n",
            "Epoch: 81.67 [D loss: 1.352368, acc: 0.00%] [G loss: 0.328661]\n",
            "Epoch: 81.68 [D loss: 1.367948, acc: 0.00%] [G loss: 0.328778]\n",
            "Epoch: 81.69 [D loss: 1.379875, acc: 0.00%] [G loss: 0.327090]\n",
            "Epoch: 81.70 [D loss: 1.314801, acc: 0.00%] [G loss: 0.327846]\n",
            "Epoch: 81.71 [D loss: 1.367889, acc: 0.00%] [G loss: 0.326643]\n",
            "Epoch: 81.72 [D loss: 1.360096, acc: 0.00%] [G loss: 0.326726]\n",
            "Epoch: 81.73 [D loss: 1.310009, acc: 0.00%] [G loss: 0.327959]\n",
            "Epoch: 81.74 [D loss: 1.372256, acc: 0.00%] [G loss: 0.327942]\n",
            "Epoch: 81.75 [D loss: 1.379538, acc: 0.00%] [G loss: 0.326654]\n",
            "Epoch: 81.76 [D loss: 1.416722, acc: 0.00%] [G loss: 0.327212]\n",
            "Epoch: 81.77 [D loss: 1.382359, acc: 0.00%] [G loss: 0.326849]\n",
            "Epoch: 81.78 [D loss: 1.381941, acc: 0.00%] [G loss: 0.327804]\n",
            "Epoch: 81.79 [D loss: 1.338300, acc: 0.00%] [G loss: 0.328897]\n",
            "Epoch: 81.80 [D loss: 1.355976, acc: 0.00%] [G loss: 0.327756]\n",
            "Epoch: 81.81 [D loss: 1.342379, acc: 0.00%] [G loss: 0.327219]\n",
            "Epoch: 81.82 [D loss: 1.365582, acc: 0.00%] [G loss: 0.327019]\n",
            "Epoch: 81.83 [D loss: 1.361555, acc: 0.00%] [G loss: 0.327086]\n",
            "Epoch: 81.84 [D loss: 1.362987, acc: 0.00%] [G loss: 0.327801]\n",
            "Epoch: 81.85 [D loss: 1.335330, acc: 0.00%] [G loss: 0.329191]\n",
            "Epoch: 81.86 [D loss: 1.353804, acc: 0.00%] [G loss: 0.327830]\n",
            "Epoch: 81.87 [D loss: 1.371169, acc: 0.00%] [G loss: 0.326827]\n",
            "Epoch: 81.88 [D loss: 1.396351, acc: 0.00%] [G loss: 0.327104]\n",
            "Epoch: 81.89 [D loss: 1.412875, acc: 0.00%] [G loss: 0.328438]\n",
            "Epoch: 81.90 [D loss: 1.449693, acc: 0.00%] [G loss: 0.328320]\n",
            "Epoch: 81.91 [D loss: 1.397281, acc: 0.00%] [G loss: 0.326991]\n",
            "Epoch: 81.92 [D loss: 1.391264, acc: 0.00%] [G loss: 0.327854]\n",
            "Epoch: 81.93 [D loss: 1.366885, acc: 0.00%] [G loss: 0.328277]\n",
            "Epoch: 81.94 [D loss: 1.354392, acc: 0.00%] [G loss: 0.326319]\n",
            "Epoch: 81.95 [D loss: 1.316346, acc: 0.00%] [G loss: 0.327069]\n",
            "Epoch: 81.96 [D loss: 1.326172, acc: 0.00%] [G loss: 0.329748]\n",
            "Epoch: 81.97 [D loss: 1.319634, acc: 0.00%] [G loss: 0.329047]\n",
            "Epoch: 81.98 [D loss: 1.355012, acc: 0.00%] [G loss: 0.326297]\n",
            "Epoch: 81.99 [D loss: 1.383284, acc: 0.00%] [G loss: 0.326250]\n",
            "Epoch: 81.100 [D loss: 1.383477, acc: 0.00%] [G loss: 0.327433]\n",
            "Epoch: 81.101 [D loss: 1.440624, acc: 0.00%] [G loss: 0.327421]\n",
            "Epoch: 81.102 [D loss: 1.379885, acc: 0.00%] [G loss: 0.329136]\n",
            "Epoch: 81.103 [D loss: 1.315938, acc: 0.00%] [G loss: 0.329158]\n",
            "Epoch: 81.104 [D loss: 1.317105, acc: 0.00%] [G loss: 0.327559]\n",
            "Epoch: 81.105 [D loss: 1.338642, acc: 0.00%] [G loss: 0.328572]\n",
            "Epoch: 81.106 [D loss: 1.333046, acc: 0.00%] [G loss: 0.327758]\n",
            "Epoch: 81.107 [D loss: 1.311750, acc: 0.00%] [G loss: 0.327600]\n",
            "Epoch: 82.0 [D loss: 1.374840, acc: 0.00%] [G loss: 0.326296]\n",
            "Epoch: 82.1 [D loss: 1.382667, acc: 0.00%] [G loss: 0.326955]\n",
            "Epoch: 82.2 [D loss: 1.391420, acc: 0.00%] [G loss: 0.326349]\n",
            "Epoch: 82.3 [D loss: 1.485860, acc: 0.00%] [G loss: 0.327123]\n",
            "Epoch: 82.4 [D loss: 1.390679, acc: 0.00%] [G loss: 0.327346]\n",
            "Epoch: 82.5 [D loss: 1.322511, acc: 0.00%] [G loss: 0.327247]\n",
            "Epoch: 82.6 [D loss: 1.361827, acc: 0.00%] [G loss: 0.327601]\n",
            "Epoch: 82.7 [D loss: 1.321278, acc: 0.00%] [G loss: 0.327369]\n",
            "Epoch: 82.8 [D loss: 1.317827, acc: 0.00%] [G loss: 0.327240]\n",
            "Epoch: 82.9 [D loss: 1.306144, acc: 0.00%] [G loss: 0.326821]\n",
            "Epoch: 82.10 [D loss: 1.365000, acc: 0.00%] [G loss: 0.327433]\n",
            "Epoch: 82.11 [D loss: 1.366939, acc: 0.00%] [G loss: 0.327190]\n",
            "Epoch: 82.12 [D loss: 1.359739, acc: 0.00%] [G loss: 0.327403]\n",
            "Epoch: 82.13 [D loss: 1.353121, acc: 0.00%] [G loss: 0.327065]\n",
            "Epoch: 82.14 [D loss: 1.437276, acc: 0.00%] [G loss: 0.328616]\n",
            "Epoch: 82.15 [D loss: 1.379492, acc: 0.00%] [G loss: 0.329161]\n",
            "Epoch: 82.16 [D loss: 1.339790, acc: 0.00%] [G loss: 0.327893]\n",
            "Epoch: 82.17 [D loss: 1.479025, acc: 0.00%] [G loss: 0.328677]\n",
            "Epoch: 82.18 [D loss: 1.429547, acc: 0.00%] [G loss: 0.328414]\n",
            "Epoch: 82.19 [D loss: 1.377023, acc: 0.00%] [G loss: 0.327420]\n",
            "Epoch: 82.20 [D loss: 1.339796, acc: 0.00%] [G loss: 0.326687]\n",
            "Epoch: 82.21 [D loss: 1.325462, acc: 0.00%] [G loss: 0.327933]\n",
            "Epoch: 82.22 [D loss: 1.327173, acc: 0.00%] [G loss: 0.328132]\n",
            "Epoch: 82.23 [D loss: 1.306113, acc: 0.00%] [G loss: 0.327354]\n",
            "Epoch: 82.24 [D loss: 1.323749, acc: 0.00%] [G loss: 0.328774]\n",
            "Epoch: 82.25 [D loss: 1.369731, acc: 0.00%] [G loss: 0.328005]\n",
            "Epoch: 82.26 [D loss: 1.335140, acc: 0.00%] [G loss: 0.327265]\n",
            "Epoch: 82.27 [D loss: 1.388149, acc: 0.00%] [G loss: 0.326892]\n",
            "Epoch: 82.28 [D loss: 1.433214, acc: 0.00%] [G loss: 0.328019]\n",
            "Epoch: 82.29 [D loss: 1.417419, acc: 0.00%] [G loss: 0.327158]\n",
            "Epoch: 82.30 [D loss: 1.332769, acc: 0.00%] [G loss: 0.326371]\n",
            "Epoch: 82.31 [D loss: 1.344006, acc: 0.00%] [G loss: 0.327488]\n",
            "Epoch: 82.32 [D loss: 1.351600, acc: 0.00%] [G loss: 0.327804]\n",
            "Epoch: 82.33 [D loss: 1.341915, acc: 0.00%] [G loss: 0.327286]\n",
            "Epoch: 82.34 [D loss: 1.334689, acc: 0.00%] [G loss: 0.328309]\n",
            "Epoch: 82.35 [D loss: 1.381666, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 82.36 [D loss: 1.332977, acc: 0.00%] [G loss: 0.328040]\n",
            "Epoch: 82.37 [D loss: 1.368097, acc: 0.00%] [G loss: 0.328112]\n",
            "Epoch: 82.38 [D loss: 1.350570, acc: 0.00%] [G loss: 0.326822]\n",
            "Epoch: 82.39 [D loss: 1.375379, acc: 0.00%] [G loss: 0.326102]\n",
            "Epoch: 82.40 [D loss: 1.374373, acc: 0.00%] [G loss: 0.326934]\n",
            "Epoch: 82.41 [D loss: 1.367701, acc: 0.00%] [G loss: 0.327985]\n",
            "Epoch: 82.42 [D loss: 1.356119, acc: 0.00%] [G loss: 0.326566]\n",
            "Epoch: 82.43 [D loss: 1.324636, acc: 0.00%] [G loss: 0.326785]\n",
            "Epoch: 82.44 [D loss: 1.318951, acc: 0.00%] [G loss: 0.328765]\n",
            "Epoch: 82.45 [D loss: 1.310540, acc: 0.00%] [G loss: 0.328091]\n",
            "Epoch: 82.46 [D loss: 1.320295, acc: 0.00%] [G loss: 0.327693]\n",
            "Epoch: 82.47 [D loss: 1.348421, acc: 0.00%] [G loss: 0.327562]\n",
            "Epoch: 82.48 [D loss: 1.393465, acc: 0.00%] [G loss: 0.327618]\n",
            "Epoch: 82.49 [D loss: 1.390570, acc: 0.00%] [G loss: 0.326740]\n",
            "Epoch: 82.50 [D loss: 1.436267, acc: 0.00%] [G loss: 0.327791]\n",
            "Epoch: 82.51 [D loss: 1.450405, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 82.52 [D loss: 1.383588, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 82.53 [D loss: 1.417387, acc: 0.00%] [G loss: 0.326949]\n",
            "Epoch: 82.54 [D loss: 1.315991, acc: 0.00%] [G loss: 0.327477]\n",
            "Epoch: 82.55 [D loss: 1.294258, acc: 0.00%] [G loss: 0.327173]\n",
            "Epoch: 82.56 [D loss: 1.321984, acc: 0.00%] [G loss: 0.327743]\n",
            "Epoch: 82.57 [D loss: 1.381653, acc: 0.00%] [G loss: 0.327383]\n",
            "Epoch: 82.58 [D loss: 1.425980, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 82.59 [D loss: 1.411793, acc: 0.00%] [G loss: 0.328420]\n",
            "Epoch: 82.60 [D loss: 1.392358, acc: 0.00%] [G loss: 0.329176]\n",
            "Epoch: 82.61 [D loss: 1.398625, acc: 0.00%] [G loss: 0.328101]\n",
            "Epoch: 82.62 [D loss: 1.425983, acc: 0.00%] [G loss: 0.327256]\n",
            "Epoch: 82.63 [D loss: 1.401043, acc: 0.00%] [G loss: 0.327060]\n",
            "Epoch: 82.64 [D loss: 1.306013, acc: 0.00%] [G loss: 0.328550]\n",
            "Epoch: 82.65 [D loss: 1.364162, acc: 0.00%] [G loss: 0.327876]\n",
            "Epoch: 82.66 [D loss: 1.272703, acc: 0.00%] [G loss: 0.328745]\n",
            "Epoch: 82.67 [D loss: 1.331168, acc: 0.00%] [G loss: 0.327045]\n",
            "Epoch: 82.68 [D loss: 1.354834, acc: 0.00%] [G loss: 0.327616]\n",
            "Epoch: 82.69 [D loss: 1.402042, acc: 0.00%] [G loss: 0.327966]\n",
            "Epoch: 82.70 [D loss: 1.385267, acc: 0.00%] [G loss: 0.326766]\n",
            "Epoch: 82.71 [D loss: 1.441384, acc: 0.00%] [G loss: 0.327439]\n",
            "Epoch: 82.72 [D loss: 1.411956, acc: 0.00%] [G loss: 0.327890]\n",
            "Epoch: 82.73 [D loss: 1.374210, acc: 0.00%] [G loss: 0.329187]\n",
            "Epoch: 82.74 [D loss: 1.332188, acc: 0.00%] [G loss: 0.327810]\n",
            "Epoch: 82.75 [D loss: 1.362945, acc: 0.00%] [G loss: 0.327865]\n",
            "Epoch: 82.76 [D loss: 1.347584, acc: 0.00%] [G loss: 0.326739]\n",
            "Epoch: 82.77 [D loss: 1.352430, acc: 0.00%] [G loss: 0.329292]\n",
            "Epoch: 82.78 [D loss: 1.364393, acc: 0.00%] [G loss: 0.327807]\n",
            "Epoch: 82.79 [D loss: 1.339100, acc: 0.00%] [G loss: 0.329285]\n",
            "Epoch: 82.80 [D loss: 1.355249, acc: 0.00%] [G loss: 0.326820]\n",
            "Epoch: 82.81 [D loss: 1.419270, acc: 0.00%] [G loss: 0.327129]\n",
            "Epoch: 82.82 [D loss: 1.374250, acc: 0.00%] [G loss: 0.326742]\n",
            "Epoch: 82.83 [D loss: 1.377159, acc: 0.00%] [G loss: 0.327641]\n",
            "Epoch: 82.84 [D loss: 1.321001, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 82.85 [D loss: 1.305564, acc: 0.00%] [G loss: 0.327085]\n",
            "Epoch: 82.86 [D loss: 1.387517, acc: 0.00%] [G loss: 0.326230]\n",
            "Epoch: 82.87 [D loss: 1.370903, acc: 0.00%] [G loss: 0.326851]\n",
            "Epoch: 82.88 [D loss: 1.352270, acc: 0.00%] [G loss: 0.326863]\n",
            "Epoch: 82.89 [D loss: 1.410379, acc: 0.00%] [G loss: 0.326842]\n",
            "Epoch: 82.90 [D loss: 1.431539, acc: 0.00%] [G loss: 0.328879]\n",
            "Epoch: 82.91 [D loss: 1.426157, acc: 0.00%] [G loss: 0.327872]\n",
            "Epoch: 82.92 [D loss: 1.368950, acc: 0.00%] [G loss: 0.327897]\n",
            "Epoch: 82.93 [D loss: 1.384047, acc: 0.00%] [G loss: 0.326371]\n",
            "Epoch: 82.94 [D loss: 1.374010, acc: 0.00%] [G loss: 0.327412]\n",
            "Epoch: 82.95 [D loss: 1.397739, acc: 0.00%] [G loss: 0.326590]\n",
            "Epoch: 82.96 [D loss: 1.354871, acc: 0.00%] [G loss: 0.326374]\n",
            "Epoch: 82.97 [D loss: 1.330957, acc: 0.00%] [G loss: 0.327425]\n",
            "Epoch: 82.98 [D loss: 1.361825, acc: 0.00%] [G loss: 0.327864]\n",
            "Epoch: 82.99 [D loss: 1.442594, acc: 0.00%] [G loss: 0.326836]\n",
            "Epoch: 82.100 [D loss: 1.385090, acc: 0.00%] [G loss: 0.326228]\n",
            "Epoch: 82.101 [D loss: 1.396966, acc: 0.00%] [G loss: 0.327192]\n",
            "Epoch: 82.102 [D loss: 1.345837, acc: 0.00%] [G loss: 0.327473]\n",
            "Epoch: 82.103 [D loss: 1.339924, acc: 0.00%] [G loss: 0.327016]\n",
            "Epoch: 82.104 [D loss: 1.282408, acc: 0.00%] [G loss: 0.328360]\n",
            "Epoch: 82.105 [D loss: 1.335112, acc: 0.00%] [G loss: 0.326707]\n",
            "Epoch: 82.106 [D loss: 1.292521, acc: 0.00%] [G loss: 0.328848]\n",
            "Epoch: 82.107 [D loss: 1.385652, acc: 0.00%] [G loss: 0.327265]\n",
            "Epoch: 83.0 [D loss: 1.400827, acc: 0.00%] [G loss: 0.327046]\n",
            "Epoch: 83.1 [D loss: 1.412362, acc: 0.00%] [G loss: 0.326974]\n",
            "Epoch: 83.2 [D loss: 1.383274, acc: 0.00%] [G loss: 0.328359]\n",
            "Epoch: 83.3 [D loss: 1.417571, acc: 0.00%] [G loss: 0.327857]\n",
            "Epoch: 83.4 [D loss: 1.382213, acc: 0.00%] [G loss: 0.327475]\n",
            "Epoch: 83.5 [D loss: 1.421879, acc: 0.00%] [G loss: 0.327485]\n",
            "Epoch: 83.6 [D loss: 1.385317, acc: 0.00%] [G loss: 0.328693]\n",
            "Epoch: 83.7 [D loss: 1.326677, acc: 0.00%] [G loss: 0.326911]\n",
            "Epoch: 83.8 [D loss: 1.353572, acc: 0.00%] [G loss: 0.326927]\n",
            "Epoch: 83.9 [D loss: 1.354246, acc: 0.00%] [G loss: 0.326191]\n",
            "Epoch: 83.10 [D loss: 1.337695, acc: 0.00%] [G loss: 0.328232]\n",
            "Epoch: 83.11 [D loss: 1.309182, acc: 0.00%] [G loss: 0.326487]\n",
            "Epoch: 83.12 [D loss: 1.356405, acc: 0.00%] [G loss: 0.326868]\n",
            "Epoch: 83.13 [D loss: 1.345716, acc: 0.00%] [G loss: 0.327407]\n",
            "Epoch: 83.14 [D loss: 1.363790, acc: 0.00%] [G loss: 0.327093]\n",
            "Epoch: 83.15 [D loss: 1.332490, acc: 0.00%] [G loss: 0.327526]\n",
            "Epoch: 83.16 [D loss: 1.350092, acc: 0.00%] [G loss: 0.327856]\n",
            "Epoch: 83.17 [D loss: 1.390137, acc: 0.00%] [G loss: 0.327280]\n",
            "Epoch: 83.18 [D loss: 1.348397, acc: 0.00%] [G loss: 0.326707]\n",
            "Epoch: 83.19 [D loss: 1.415259, acc: 0.00%] [G loss: 0.328473]\n",
            "Epoch: 83.20 [D loss: 1.381411, acc: 0.00%] [G loss: 0.329087]\n",
            "Epoch: 83.21 [D loss: 1.363118, acc: 0.00%] [G loss: 0.327222]\n",
            "Epoch: 83.22 [D loss: 1.370807, acc: 0.00%] [G loss: 0.326557]\n",
            "Epoch: 83.23 [D loss: 1.319453, acc: 0.00%] [G loss: 0.327532]\n",
            "Epoch: 83.24 [D loss: 1.360148, acc: 0.00%] [G loss: 0.328250]\n",
            "Epoch: 83.25 [D loss: 1.380520, acc: 0.00%] [G loss: 0.327416]\n",
            "Epoch: 83.26 [D loss: 1.341679, acc: 0.00%] [G loss: 0.327543]\n",
            "Epoch: 83.27 [D loss: 1.421606, acc: 0.00%] [G loss: 0.327964]\n",
            "Epoch: 83.28 [D loss: 1.426976, acc: 0.00%] [G loss: 0.327472]\n",
            "Epoch: 83.29 [D loss: 1.385158, acc: 0.00%] [G loss: 0.328466]\n",
            "Epoch: 83.30 [D loss: 1.376722, acc: 0.00%] [G loss: 0.328224]\n",
            "Epoch: 83.31 [D loss: 1.374583, acc: 0.00%] [G loss: 0.326661]\n",
            "Epoch: 83.32 [D loss: 1.374598, acc: 0.00%] [G loss: 0.327061]\n",
            "Epoch: 83.33 [D loss: 1.380141, acc: 0.00%] [G loss: 0.327540]\n",
            "Epoch: 83.34 [D loss: 1.288934, acc: 0.00%] [G loss: 0.327297]\n",
            "Epoch: 83.35 [D loss: 1.335176, acc: 0.00%] [G loss: 0.327546]\n",
            "Epoch: 83.36 [D loss: 1.364555, acc: 0.00%] [G loss: 0.326957]\n",
            "Epoch: 83.37 [D loss: 1.375690, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 83.38 [D loss: 1.332120, acc: 0.00%] [G loss: 0.328028]\n",
            "Epoch: 83.39 [D loss: 1.365654, acc: 0.00%] [G loss: 0.328011]\n",
            "Epoch: 83.40 [D loss: 1.363953, acc: 0.00%] [G loss: 0.326735]\n",
            "Epoch: 83.41 [D loss: 1.418028, acc: 0.00%] [G loss: 0.327422]\n",
            "Epoch: 83.42 [D loss: 1.403371, acc: 0.00%] [G loss: 0.326527]\n",
            "Epoch: 83.43 [D loss: 1.378807, acc: 0.00%] [G loss: 0.328607]\n",
            "Epoch: 83.44 [D loss: 1.390925, acc: 0.00%] [G loss: 0.326685]\n",
            "Epoch: 83.45 [D loss: 1.330639, acc: 0.00%] [G loss: 0.329120]\n",
            "Epoch: 83.46 [D loss: 1.367108, acc: 0.00%] [G loss: 0.326146]\n",
            "Epoch: 83.47 [D loss: 1.401446, acc: 0.00%] [G loss: 0.326807]\n",
            "Epoch: 83.48 [D loss: 1.364639, acc: 0.00%] [G loss: 0.327460]\n",
            "Epoch: 83.49 [D loss: 1.315419, acc: 0.00%] [G loss: 0.327772]\n",
            "Epoch: 83.50 [D loss: 1.341129, acc: 0.00%] [G loss: 0.328009]\n",
            "Epoch: 83.51 [D loss: 1.336966, acc: 0.00%] [G loss: 0.327281]\n",
            "Epoch: 83.52 [D loss: 1.390674, acc: 0.00%] [G loss: 0.327316]\n",
            "Epoch: 83.53 [D loss: 1.393584, acc: 0.00%] [G loss: 0.327316]\n",
            "Epoch: 83.54 [D loss: 1.423534, acc: 0.00%] [G loss: 0.327663]\n",
            "Epoch: 83.55 [D loss: 1.395947, acc: 0.00%] [G loss: 0.328891]\n",
            "Epoch: 83.56 [D loss: 1.399928, acc: 0.00%] [G loss: 0.328842]\n",
            "Epoch: 83.57 [D loss: 1.341023, acc: 0.00%] [G loss: 0.326789]\n",
            "Epoch: 83.58 [D loss: 1.311171, acc: 0.00%] [G loss: 0.328992]\n",
            "Epoch: 83.59 [D loss: 1.323396, acc: 0.00%] [G loss: 0.327929]\n",
            "Epoch: 83.60 [D loss: 1.323400, acc: 0.00%] [G loss: 0.326039]\n",
            "Epoch: 83.61 [D loss: 1.360695, acc: 0.00%] [G loss: 0.326195]\n",
            "Epoch: 83.62 [D loss: 1.362360, acc: 0.00%] [G loss: 0.327552]\n",
            "Epoch: 83.63 [D loss: 1.407981, acc: 0.00%] [G loss: 0.328538]\n",
            "Epoch: 83.64 [D loss: 1.431253, acc: 0.00%] [G loss: 0.327375]\n",
            "Epoch: 83.65 [D loss: 1.372617, acc: 0.00%] [G loss: 0.326757]\n",
            "Epoch: 83.66 [D loss: 1.342519, acc: 0.00%] [G loss: 0.327289]\n",
            "Epoch: 83.67 [D loss: 1.350489, acc: 0.00%] [G loss: 0.327747]\n",
            "Epoch: 83.68 [D loss: 1.305284, acc: 0.00%] [G loss: 0.328374]\n",
            "Epoch: 83.69 [D loss: 1.310521, acc: 0.00%] [G loss: 0.328166]\n",
            "Epoch: 83.70 [D loss: 1.406694, acc: 0.00%] [G loss: 0.327257]\n",
            "Epoch: 83.71 [D loss: 1.336191, acc: 0.00%] [G loss: 0.327433]\n",
            "Epoch: 83.72 [D loss: 1.385872, acc: 0.00%] [G loss: 0.328164]\n",
            "Epoch: 83.73 [D loss: 1.329997, acc: 0.00%] [G loss: 0.326220]\n",
            "Epoch: 83.74 [D loss: 1.346842, acc: 0.00%] [G loss: 0.327867]\n",
            "Epoch: 83.75 [D loss: 1.354041, acc: 0.00%] [G loss: 0.327272]\n",
            "Epoch: 83.76 [D loss: 1.382395, acc: 0.00%] [G loss: 0.327721]\n",
            "Epoch: 83.77 [D loss: 1.325123, acc: 0.00%] [G loss: 0.326676]\n",
            "Epoch: 83.78 [D loss: 1.370950, acc: 0.00%] [G loss: 0.328806]\n",
            "Epoch: 83.79 [D loss: 1.341365, acc: 0.00%] [G loss: 0.327558]\n",
            "Epoch: 83.80 [D loss: 1.400167, acc: 0.00%] [G loss: 0.329159]\n",
            "Epoch: 83.81 [D loss: 1.387642, acc: 0.00%] [G loss: 0.327503]\n",
            "Epoch: 83.82 [D loss: 1.417964, acc: 0.00%] [G loss: 0.328738]\n",
            "Epoch: 83.83 [D loss: 1.432224, acc: 0.00%] [G loss: 0.326694]\n",
            "Epoch: 83.84 [D loss: 1.342865, acc: 0.00%] [G loss: 0.327151]\n",
            "Epoch: 83.85 [D loss: 1.374420, acc: 0.00%] [G loss: 0.326156]\n",
            "Epoch: 83.86 [D loss: 1.374627, acc: 0.00%] [G loss: 0.327480]\n",
            "Epoch: 83.87 [D loss: 1.299161, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 83.88 [D loss: 1.340225, acc: 0.00%] [G loss: 0.327477]\n",
            "Epoch: 83.89 [D loss: 1.358867, acc: 0.00%] [G loss: 0.328882]\n",
            "Epoch: 83.90 [D loss: 1.360576, acc: 0.00%] [G loss: 0.328822]\n",
            "Epoch: 83.91 [D loss: 1.339798, acc: 0.00%] [G loss: 0.327246]\n",
            "Epoch: 83.92 [D loss: 1.348042, acc: 0.00%] [G loss: 0.328034]\n",
            "Epoch: 83.93 [D loss: 1.335162, acc: 0.00%] [G loss: 0.327481]\n",
            "Epoch: 83.94 [D loss: 1.398316, acc: 0.00%] [G loss: 0.327272]\n",
            "Epoch: 83.95 [D loss: 1.382653, acc: 0.00%] [G loss: 0.327317]\n",
            "Epoch: 83.96 [D loss: 1.365709, acc: 0.00%] [G loss: 0.327190]\n",
            "Epoch: 83.97 [D loss: 1.387366, acc: 0.00%] [G loss: 0.326483]\n",
            "Epoch: 83.98 [D loss: 1.309361, acc: 0.00%] [G loss: 0.328397]\n",
            "Epoch: 83.99 [D loss: 1.362932, acc: 0.00%] [G loss: 0.326169]\n",
            "Epoch: 83.100 [D loss: 1.372459, acc: 0.00%] [G loss: 0.326670]\n",
            "Epoch: 83.101 [D loss: 1.347304, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 83.102 [D loss: 1.417794, acc: 0.00%] [G loss: 0.327406]\n",
            "Epoch: 83.103 [D loss: 1.399108, acc: 0.00%] [G loss: 0.327237]\n",
            "Epoch: 83.104 [D loss: 1.428625, acc: 0.00%] [G loss: 0.328236]\n",
            "Epoch: 83.105 [D loss: 1.403560, acc: 0.00%] [G loss: 0.328019]\n",
            "Epoch: 83.106 [D loss: 1.404049, acc: 0.00%] [G loss: 0.329250]\n",
            "Epoch: 83.107 [D loss: 1.342218, acc: 0.00%] [G loss: 0.327745]\n",
            "Epoch: 84.0 [D loss: 1.344689, acc: 0.00%] [G loss: 0.326401]\n",
            "Epoch: 84.1 [D loss: 1.326568, acc: 0.00%] [G loss: 0.328284]\n",
            "Epoch: 84.2 [D loss: 1.318177, acc: 0.00%] [G loss: 0.328879]\n",
            "Epoch: 84.3 [D loss: 1.311571, acc: 0.00%] [G loss: 0.327583]\n",
            "Epoch: 84.4 [D loss: 1.393588, acc: 0.00%] [G loss: 0.328655]\n",
            "Epoch: 84.5 [D loss: 1.372943, acc: 0.00%] [G loss: 0.326824]\n",
            "Epoch: 84.6 [D loss: 1.425373, acc: 0.00%] [G loss: 0.327982]\n",
            "Epoch: 84.7 [D loss: 1.432600, acc: 0.00%] [G loss: 0.329895]\n",
            "Epoch: 84.8 [D loss: 1.419141, acc: 0.00%] [G loss: 0.329069]\n",
            "Epoch: 84.9 [D loss: 1.317668, acc: 0.00%] [G loss: 0.326624]\n",
            "Epoch: 84.10 [D loss: 1.426528, acc: 0.00%] [G loss: 0.326260]\n",
            "Epoch: 84.11 [D loss: 1.362237, acc: 0.00%] [G loss: 0.328492]\n",
            "Epoch: 84.12 [D loss: 1.392879, acc: 0.00%] [G loss: 0.327441]\n",
            "Epoch: 84.13 [D loss: 1.336669, acc: 0.00%] [G loss: 0.328400]\n",
            "Epoch: 84.14 [D loss: 1.340322, acc: 0.00%] [G loss: 0.326136]\n",
            "Epoch: 84.15 [D loss: 1.351295, acc: 0.00%] [G loss: 0.327309]\n",
            "Epoch: 84.16 [D loss: 1.379796, acc: 0.00%] [G loss: 0.327360]\n",
            "Epoch: 84.17 [D loss: 1.409170, acc: 0.00%] [G loss: 0.327484]\n",
            "Epoch: 84.18 [D loss: 1.368405, acc: 0.00%] [G loss: 0.326130]\n",
            "Epoch: 84.19 [D loss: 1.354083, acc: 0.00%] [G loss: 0.326691]\n",
            "Epoch: 84.20 [D loss: 1.382974, acc: 0.00%] [G loss: 0.326860]\n",
            "Epoch: 84.21 [D loss: 1.302817, acc: 0.00%] [G loss: 0.327991]\n",
            "Epoch: 84.22 [D loss: 1.378699, acc: 0.00%] [G loss: 0.326880]\n",
            "Epoch: 84.23 [D loss: 1.339132, acc: 0.00%] [G loss: 0.327379]\n",
            "Epoch: 84.24 [D loss: 1.341077, acc: 0.00%] [G loss: 0.326961]\n",
            "Epoch: 84.25 [D loss: 1.323103, acc: 0.00%] [G loss: 0.326495]\n",
            "Epoch: 84.26 [D loss: 1.267980, acc: 0.00%] [G loss: 0.326146]\n",
            "Epoch: 84.27 [D loss: 1.312030, acc: 0.00%] [G loss: 0.327829]\n",
            "Epoch: 84.28 [D loss: 1.369869, acc: 0.00%] [G loss: 0.327946]\n",
            "Epoch: 84.29 [D loss: 1.380533, acc: 0.00%] [G loss: 0.326840]\n",
            "Epoch: 84.30 [D loss: 1.402670, acc: 0.00%] [G loss: 0.326777]\n",
            "Epoch: 84.31 [D loss: 1.412176, acc: 0.00%] [G loss: 0.328627]\n",
            "Epoch: 84.32 [D loss: 1.387185, acc: 0.00%] [G loss: 0.327327]\n",
            "Epoch: 84.33 [D loss: 1.369002, acc: 0.00%] [G loss: 0.328335]\n",
            "Epoch: 84.34 [D loss: 1.376887, acc: 0.00%] [G loss: 0.326153]\n",
            "Epoch: 84.35 [D loss: 1.337736, acc: 0.00%] [G loss: 0.328401]\n",
            "Epoch: 84.36 [D loss: 1.331288, acc: 0.00%] [G loss: 0.327663]\n",
            "Epoch: 84.37 [D loss: 1.365317, acc: 0.00%] [G loss: 0.326571]\n",
            "Epoch: 84.38 [D loss: 1.355358, acc: 0.00%] [G loss: 0.328274]\n",
            "Epoch: 84.39 [D loss: 1.343217, acc: 0.00%] [G loss: 0.327755]\n",
            "Epoch: 84.40 [D loss: 1.353782, acc: 0.00%] [G loss: 0.327832]\n",
            "Epoch: 84.41 [D loss: 1.412612, acc: 0.00%] [G loss: 0.329321]\n",
            "Epoch: 84.42 [D loss: 1.390852, acc: 0.00%] [G loss: 0.328498]\n",
            "Epoch: 84.43 [D loss: 1.388079, acc: 0.00%] [G loss: 0.328131]\n",
            "Epoch: 84.44 [D loss: 1.401796, acc: 0.00%] [G loss: 0.328296]\n",
            "Epoch: 84.45 [D loss: 1.334762, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 84.46 [D loss: 1.313245, acc: 0.00%] [G loss: 0.327134]\n",
            "Epoch: 84.47 [D loss: 1.346210, acc: 0.00%] [G loss: 0.328578]\n",
            "Epoch: 84.48 [D loss: 1.328247, acc: 0.00%] [G loss: 0.328293]\n",
            "Epoch: 84.49 [D loss: 1.367640, acc: 0.00%] [G loss: 0.328085]\n",
            "Epoch: 84.50 [D loss: 1.397936, acc: 0.00%] [G loss: 0.328144]\n",
            "Epoch: 84.51 [D loss: 1.408731, acc: 0.00%] [G loss: 0.327944]\n",
            "Epoch: 84.52 [D loss: 1.441171, acc: 0.00%] [G loss: 0.328216]\n",
            "Epoch: 84.53 [D loss: 1.444983, acc: 0.00%] [G loss: 0.328246]\n",
            "Epoch: 84.54 [D loss: 1.409065, acc: 0.00%] [G loss: 0.327942]\n",
            "Epoch: 84.55 [D loss: 1.364167, acc: 0.00%] [G loss: 0.328379]\n",
            "Epoch: 84.56 [D loss: 1.347192, acc: 0.00%] [G loss: 0.327886]\n",
            "Epoch: 84.57 [D loss: 1.343369, acc: 0.00%] [G loss: 0.327647]\n",
            "Epoch: 84.58 [D loss: 1.331929, acc: 0.00%] [G loss: 0.327165]\n",
            "Epoch: 84.59 [D loss: 1.311732, acc: 0.00%] [G loss: 0.328257]\n",
            "Epoch: 84.60 [D loss: 1.376402, acc: 0.00%] [G loss: 0.328260]\n",
            "Epoch: 84.61 [D loss: 1.350240, acc: 0.00%] [G loss: 0.327558]\n",
            "Epoch: 84.62 [D loss: 1.392478, acc: 0.00%] [G loss: 0.328260]\n",
            "Epoch: 84.63 [D loss: 1.437384, acc: 0.00%] [G loss: 0.327381]\n",
            "Epoch: 84.64 [D loss: 1.414414, acc: 0.00%] [G loss: 0.326931]\n",
            "Epoch: 84.65 [D loss: 1.368242, acc: 0.00%] [G loss: 0.327066]\n",
            "Epoch: 84.66 [D loss: 1.362895, acc: 0.00%] [G loss: 0.327130]\n",
            "Epoch: 84.67 [D loss: 1.340516, acc: 0.00%] [G loss: 0.327258]\n",
            "Epoch: 84.68 [D loss: 1.306321, acc: 0.00%] [G loss: 0.327549]\n",
            "Epoch: 84.69 [D loss: 1.335922, acc: 0.00%] [G loss: 0.328082]\n",
            "Epoch: 84.70 [D loss: 1.320912, acc: 0.00%] [G loss: 0.328288]\n",
            "Epoch: 84.71 [D loss: 1.316036, acc: 0.00%] [G loss: 0.328164]\n",
            "Epoch: 84.72 [D loss: 1.316718, acc: 0.00%] [G loss: 0.327152]\n",
            "Epoch: 84.73 [D loss: 1.337463, acc: 0.00%] [G loss: 0.328644]\n",
            "Epoch: 84.74 [D loss: 1.377358, acc: 0.00%] [G loss: 0.326870]\n",
            "Epoch: 84.75 [D loss: 1.404985, acc: 0.00%] [G loss: 0.327729]\n",
            "Epoch: 84.76 [D loss: 1.350958, acc: 0.00%] [G loss: 0.327283]\n",
            "Epoch: 84.77 [D loss: 1.404984, acc: 0.00%] [G loss: 0.327253]\n",
            "Epoch: 84.78 [D loss: 1.384199, acc: 0.00%] [G loss: 0.327946]\n",
            "Epoch: 84.79 [D loss: 1.430477, acc: 0.00%] [G loss: 0.327165]\n",
            "Epoch: 84.80 [D loss: 1.379394, acc: 0.00%] [G loss: 0.327485]\n",
            "Epoch: 84.81 [D loss: 1.334931, acc: 0.00%] [G loss: 0.326740]\n",
            "Epoch: 84.82 [D loss: 1.382253, acc: 0.00%] [G loss: 0.327962]\n",
            "Epoch: 84.83 [D loss: 1.356493, acc: 0.00%] [G loss: 0.327356]\n",
            "Epoch: 84.84 [D loss: 1.351156, acc: 0.00%] [G loss: 0.327596]\n",
            "Epoch: 84.85 [D loss: 1.367141, acc: 0.00%] [G loss: 0.327858]\n",
            "Epoch: 84.86 [D loss: 1.380551, acc: 0.00%] [G loss: 0.328264]\n",
            "Epoch: 84.87 [D loss: 1.331851, acc: 0.00%] [G loss: 0.327548]\n",
            "Epoch: 84.88 [D loss: 1.397766, acc: 0.00%] [G loss: 0.327202]\n",
            "Epoch: 84.89 [D loss: 1.337253, acc: 0.00%] [G loss: 0.328627]\n",
            "Epoch: 84.90 [D loss: 1.335256, acc: 0.00%] [G loss: 0.327599]\n",
            "Epoch: 84.91 [D loss: 1.366746, acc: 0.00%] [G loss: 0.326574]\n",
            "Epoch: 84.92 [D loss: 1.345295, acc: 0.00%] [G loss: 0.327312]\n",
            "Epoch: 84.93 [D loss: 1.383424, acc: 0.00%] [G loss: 0.326849]\n",
            "Epoch: 84.94 [D loss: 1.398997, acc: 0.00%] [G loss: 0.327679]\n",
            "Epoch: 84.95 [D loss: 1.405362, acc: 0.00%] [G loss: 0.327245]\n",
            "Epoch: 84.96 [D loss: 1.373218, acc: 0.00%] [G loss: 0.328061]\n",
            "Epoch: 84.97 [D loss: 1.382252, acc: 0.00%] [G loss: 0.327715]\n",
            "Epoch: 84.98 [D loss: 1.382172, acc: 0.00%] [G loss: 0.327503]\n",
            "Epoch: 84.99 [D loss: 1.434509, acc: 0.00%] [G loss: 0.328158]\n",
            "Epoch: 84.100 [D loss: 1.356015, acc: 0.00%] [G loss: 0.326969]\n",
            "Epoch: 84.101 [D loss: 1.350770, acc: 0.00%] [G loss: 0.326605]\n",
            "Epoch: 84.102 [D loss: 1.309841, acc: 0.00%] [G loss: 0.327516]\n",
            "Epoch: 84.103 [D loss: 1.336945, acc: 0.00%] [G loss: 0.328253]\n",
            "Epoch: 84.104 [D loss: 1.352672, acc: 0.00%] [G loss: 0.327444]\n",
            "Epoch: 84.105 [D loss: 1.339059, acc: 0.00%] [G loss: 0.326925]\n",
            "Epoch: 84.106 [D loss: 1.406730, acc: 0.00%] [G loss: 0.328685]\n",
            "Epoch: 84.107 [D loss: 1.424842, acc: 0.00%] [G loss: 0.327549]\n",
            "Epoch: 85.0 [D loss: 1.441752, acc: 0.00%] [G loss: 0.327386]\n",
            "Epoch: 85.1 [D loss: 1.415989, acc: 0.00%] [G loss: 0.326621]\n",
            "Epoch: 85.2 [D loss: 1.335944, acc: 0.00%] [G loss: 0.327765]\n",
            "Epoch: 85.3 [D loss: 1.359547, acc: 0.00%] [G loss: 0.327664]\n",
            "Epoch: 85.4 [D loss: 1.286926, acc: 0.00%] [G loss: 0.327039]\n",
            "Epoch: 85.5 [D loss: 1.309440, acc: 0.00%] [G loss: 0.327065]\n",
            "Epoch: 85.6 [D loss: 1.320745, acc: 0.00%] [G loss: 0.329061]\n",
            "Epoch: 85.7 [D loss: 1.272265, acc: 0.00%] [G loss: 0.328108]\n",
            "Epoch: 85.8 [D loss: 1.365054, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 85.9 [D loss: 1.325572, acc: 0.00%] [G loss: 0.329138]\n",
            "Epoch: 85.10 [D loss: 1.388700, acc: 0.00%] [G loss: 0.326686]\n",
            "Epoch: 85.11 [D loss: 1.403897, acc: 0.00%] [G loss: 0.327762]\n",
            "Epoch: 85.12 [D loss: 1.398647, acc: 0.00%] [G loss: 0.329695]\n",
            "Epoch: 85.13 [D loss: 1.409344, acc: 0.00%] [G loss: 0.327515]\n",
            "Epoch: 85.14 [D loss: 1.402976, acc: 0.00%] [G loss: 0.327261]\n",
            "Epoch: 85.15 [D loss: 1.358920, acc: 0.00%] [G loss: 0.326363]\n",
            "Epoch: 85.16 [D loss: 1.323545, acc: 0.00%] [G loss: 0.328593]\n",
            "Epoch: 85.17 [D loss: 1.368528, acc: 0.00%] [G loss: 0.329520]\n",
            "Epoch: 85.18 [D loss: 1.343955, acc: 0.00%] [G loss: 0.328685]\n",
            "Epoch: 85.19 [D loss: 1.420164, acc: 0.00%] [G loss: 0.328829]\n",
            "Epoch: 85.20 [D loss: 1.419778, acc: 0.00%] [G loss: 0.327733]\n",
            "Epoch: 85.21 [D loss: 1.351664, acc: 0.00%] [G loss: 0.327322]\n",
            "Epoch: 85.22 [D loss: 1.381212, acc: 0.00%] [G loss: 0.328059]\n",
            "Epoch: 85.23 [D loss: 1.331739, acc: 0.00%] [G loss: 0.327465]\n",
            "Epoch: 85.24 [D loss: 1.359469, acc: 0.00%] [G loss: 0.329024]\n",
            "Epoch: 85.25 [D loss: 1.360630, acc: 0.00%] [G loss: 0.327061]\n",
            "Epoch: 85.26 [D loss: 1.349976, acc: 0.00%] [G loss: 0.327075]\n",
            "Epoch: 85.27 [D loss: 1.387550, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 85.28 [D loss: 1.360868, acc: 0.00%] [G loss: 0.327483]\n",
            "Epoch: 85.29 [D loss: 1.393553, acc: 0.00%] [G loss: 0.327814]\n",
            "Epoch: 85.30 [D loss: 1.394133, acc: 0.00%] [G loss: 0.326669]\n",
            "Epoch: 85.31 [D loss: 1.380487, acc: 0.00%] [G loss: 0.326755]\n",
            "Epoch: 85.32 [D loss: 1.317495, acc: 0.00%] [G loss: 0.328976]\n",
            "Epoch: 85.33 [D loss: 1.365320, acc: 0.00%] [G loss: 0.328138]\n",
            "Epoch: 85.34 [D loss: 1.355438, acc: 0.00%] [G loss: 0.327767]\n",
            "Epoch: 85.35 [D loss: 1.301719, acc: 0.00%] [G loss: 0.327942]\n",
            "Epoch: 85.36 [D loss: 1.302038, acc: 0.00%] [G loss: 0.326846]\n",
            "Epoch: 85.37 [D loss: 1.368774, acc: 0.00%] [G loss: 0.327187]\n",
            "Epoch: 85.38 [D loss: 1.383372, acc: 0.00%] [G loss: 0.327298]\n",
            "Epoch: 85.39 [D loss: 1.422642, acc: 0.00%] [G loss: 0.327775]\n",
            "Epoch: 85.40 [D loss: 1.354205, acc: 0.00%] [G loss: 0.327364]\n",
            "Epoch: 85.41 [D loss: 1.411137, acc: 0.00%] [G loss: 0.327568]\n",
            "Epoch: 85.42 [D loss: 1.397389, acc: 0.00%] [G loss: 0.327647]\n",
            "Epoch: 85.43 [D loss: 1.377426, acc: 0.00%] [G loss: 0.326690]\n",
            "Epoch: 85.44 [D loss: 1.337177, acc: 0.00%] [G loss: 0.327082]\n",
            "Epoch: 85.45 [D loss: 1.335235, acc: 0.00%] [G loss: 0.327766]\n",
            "Epoch: 85.46 [D loss: 1.318854, acc: 0.00%] [G loss: 0.327816]\n",
            "Epoch: 85.47 [D loss: 1.316502, acc: 0.00%] [G loss: 0.327062]\n",
            "Epoch: 85.48 [D loss: 1.325591, acc: 0.00%] [G loss: 0.327190]\n",
            "Epoch: 85.49 [D loss: 1.347308, acc: 0.00%] [G loss: 0.327123]\n",
            "Epoch: 85.50 [D loss: 1.410945, acc: 0.00%] [G loss: 0.326603]\n",
            "Epoch: 85.51 [D loss: 1.388232, acc: 0.00%] [G loss: 0.327827]\n",
            "Epoch: 85.52 [D loss: 1.403455, acc: 0.00%] [G loss: 0.327995]\n",
            "Epoch: 85.53 [D loss: 1.406643, acc: 0.00%] [G loss: 0.327061]\n",
            "Epoch: 85.54 [D loss: 1.375761, acc: 0.00%] [G loss: 0.327389]\n",
            "Epoch: 85.55 [D loss: 1.351503, acc: 0.00%] [G loss: 0.327320]\n",
            "Epoch: 85.56 [D loss: 1.370429, acc: 0.00%] [G loss: 0.326881]\n",
            "Epoch: 85.57 [D loss: 1.366143, acc: 0.00%] [G loss: 0.327047]\n",
            "Epoch: 85.58 [D loss: 1.376050, acc: 0.00%] [G loss: 0.328001]\n",
            "Epoch: 85.59 [D loss: 1.405724, acc: 0.00%] [G loss: 0.327854]\n",
            "Epoch: 85.60 [D loss: 1.381066, acc: 0.00%] [G loss: 0.327615]\n",
            "Epoch: 85.61 [D loss: 1.360323, acc: 0.00%] [G loss: 0.327500]\n",
            "Epoch: 85.62 [D loss: 1.395092, acc: 0.00%] [G loss: 0.327489]\n",
            "Epoch: 85.63 [D loss: 1.322578, acc: 0.00%] [G loss: 0.327532]\n",
            "Epoch: 85.64 [D loss: 1.339436, acc: 0.00%] [G loss: 0.327672]\n",
            "Epoch: 85.65 [D loss: 1.316491, acc: 0.00%] [G loss: 0.327971]\n",
            "Epoch: 85.66 [D loss: 1.360864, acc: 0.00%] [G loss: 0.326950]\n",
            "Epoch: 85.67 [D loss: 1.312923, acc: 0.00%] [G loss: 0.327404]\n",
            "Epoch: 85.68 [D loss: 1.392537, acc: 0.00%] [G loss: 0.327539]\n",
            "Epoch: 85.69 [D loss: 1.391771, acc: 0.00%] [G loss: 0.326876]\n",
            "Epoch: 85.70 [D loss: 1.378674, acc: 0.00%] [G loss: 0.327017]\n",
            "Epoch: 85.71 [D loss: 1.428994, acc: 0.00%] [G loss: 0.327737]\n",
            "Epoch: 85.72 [D loss: 1.382418, acc: 0.00%] [G loss: 0.327166]\n",
            "Epoch: 85.73 [D loss: 1.367353, acc: 0.00%] [G loss: 0.325990]\n",
            "Epoch: 85.74 [D loss: 1.329718, acc: 0.00%] [G loss: 0.328406]\n",
            "Epoch: 85.75 [D loss: 1.364229, acc: 0.00%] [G loss: 0.326876]\n",
            "Epoch: 85.76 [D loss: 1.399620, acc: 0.00%] [G loss: 0.328194]\n",
            "Epoch: 85.77 [D loss: 1.424744, acc: 0.00%] [G loss: 0.327223]\n",
            "Epoch: 85.78 [D loss: 1.330931, acc: 0.00%] [G loss: 0.327410]\n",
            "Epoch: 85.79 [D loss: 1.376366, acc: 0.00%] [G loss: 0.328041]\n",
            "Epoch: 85.80 [D loss: 1.358981, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 85.81 [D loss: 1.341371, acc: 0.00%] [G loss: 0.327882]\n",
            "Epoch: 85.82 [D loss: 1.345899, acc: 0.00%] [G loss: 0.327156]\n",
            "Epoch: 85.83 [D loss: 1.316129, acc: 0.00%] [G loss: 0.327375]\n",
            "Epoch: 85.84 [D loss: 1.308209, acc: 0.00%] [G loss: 0.328292]\n",
            "Epoch: 85.85 [D loss: 1.349634, acc: 0.00%] [G loss: 0.327046]\n",
            "Epoch: 85.86 [D loss: 1.310915, acc: 0.00%] [G loss: 0.328303]\n",
            "Epoch: 85.87 [D loss: 1.432216, acc: 0.00%] [G loss: 0.327060]\n",
            "Epoch: 85.88 [D loss: 1.415654, acc: 0.00%] [G loss: 0.327870]\n",
            "Epoch: 85.89 [D loss: 1.421326, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 85.90 [D loss: 1.422433, acc: 0.00%] [G loss: 0.328790]\n",
            "Epoch: 85.91 [D loss: 1.405818, acc: 0.00%] [G loss: 0.327069]\n",
            "Epoch: 85.92 [D loss: 1.360058, acc: 0.00%] [G loss: 0.327067]\n",
            "Epoch: 85.93 [D loss: 1.359863, acc: 0.00%] [G loss: 0.327795]\n",
            "Epoch: 85.94 [D loss: 1.320819, acc: 0.00%] [G loss: 0.327058]\n",
            "Epoch: 85.95 [D loss: 1.341491, acc: 0.00%] [G loss: 0.327203]\n",
            "Epoch: 85.96 [D loss: 1.358678, acc: 0.00%] [G loss: 0.326572]\n",
            "Epoch: 85.97 [D loss: 1.335707, acc: 0.00%] [G loss: 0.328178]\n",
            "Epoch: 85.98 [D loss: 1.394941, acc: 0.00%] [G loss: 0.328472]\n",
            "Epoch: 85.99 [D loss: 1.378361, acc: 0.00%] [G loss: 0.328310]\n",
            "Epoch: 85.100 [D loss: 1.378453, acc: 0.00%] [G loss: 0.326388]\n",
            "Epoch: 85.101 [D loss: 1.376027, acc: 0.00%] [G loss: 0.326705]\n",
            "Epoch: 85.102 [D loss: 1.356417, acc: 0.00%] [G loss: 0.328604]\n",
            "Epoch: 85.103 [D loss: 1.309408, acc: 0.00%] [G loss: 0.328229]\n",
            "Epoch: 85.104 [D loss: 1.333489, acc: 0.00%] [G loss: 0.327966]\n",
            "Epoch: 85.105 [D loss: 1.319715, acc: 0.00%] [G loss: 0.326987]\n",
            "Epoch: 85.106 [D loss: 1.323991, acc: 0.00%] [G loss: 0.327901]\n",
            "Epoch: 85.107 [D loss: 1.287896, acc: 0.00%] [G loss: 0.327057]\n",
            "Epoch: 86.0 [D loss: 1.366110, acc: 0.00%] [G loss: 0.327390]\n",
            "Epoch: 86.1 [D loss: 1.378210, acc: 0.00%] [G loss: 0.328755]\n",
            "Epoch: 86.2 [D loss: 1.355595, acc: 0.00%] [G loss: 0.327396]\n",
            "Epoch: 86.3 [D loss: 1.438627, acc: 0.00%] [G loss: 0.327214]\n",
            "Epoch: 86.4 [D loss: 1.347636, acc: 0.00%] [G loss: 0.328067]\n",
            "Epoch: 86.5 [D loss: 1.384668, acc: 0.00%] [G loss: 0.327009]\n",
            "Epoch: 86.6 [D loss: 1.393494, acc: 0.00%] [G loss: 0.327227]\n",
            "Epoch: 86.7 [D loss: 1.366979, acc: 0.00%] [G loss: 0.327396]\n",
            "Epoch: 86.8 [D loss: 1.297152, acc: 0.00%] [G loss: 0.327145]\n",
            "Epoch: 86.9 [D loss: 1.328652, acc: 0.00%] [G loss: 0.327763]\n",
            "Epoch: 86.10 [D loss: 1.334382, acc: 0.00%] [G loss: 0.327329]\n",
            "Epoch: 86.11 [D loss: 1.368646, acc: 0.00%] [G loss: 0.327583]\n",
            "Epoch: 86.12 [D loss: 1.371673, acc: 0.00%] [G loss: 0.327141]\n",
            "Epoch: 86.13 [D loss: 1.418033, acc: 0.00%] [G loss: 0.327415]\n",
            "Epoch: 86.14 [D loss: 1.383697, acc: 0.00%] [G loss: 0.328698]\n",
            "Epoch: 86.15 [D loss: 1.374508, acc: 0.00%] [G loss: 0.328781]\n",
            "Epoch: 86.16 [D loss: 1.383705, acc: 0.00%] [G loss: 0.326402]\n",
            "Epoch: 86.17 [D loss: 1.345733, acc: 0.00%] [G loss: 0.327874]\n",
            "Epoch: 86.18 [D loss: 1.371450, acc: 0.00%] [G loss: 0.327407]\n",
            "Epoch: 86.19 [D loss: 1.444471, acc: 0.00%] [G loss: 0.328438]\n",
            "Epoch: 86.20 [D loss: 1.385573, acc: 0.00%] [G loss: 0.326497]\n",
            "Epoch: 86.21 [D loss: 1.343053, acc: 0.00%] [G loss: 0.327156]\n",
            "Epoch: 86.22 [D loss: 1.305604, acc: 0.00%] [G loss: 0.329394]\n",
            "Epoch: 86.23 [D loss: 1.318769, acc: 0.00%] [G loss: 0.327112]\n",
            "Epoch: 86.24 [D loss: 1.336133, acc: 0.00%] [G loss: 0.326896]\n",
            "Epoch: 86.25 [D loss: 1.371677, acc: 0.00%] [G loss: 0.327312]\n",
            "Epoch: 86.26 [D loss: 1.412883, acc: 0.00%] [G loss: 0.327361]\n",
            "Epoch: 86.27 [D loss: 1.410778, acc: 0.00%] [G loss: 0.326964]\n",
            "Epoch: 86.28 [D loss: 1.394380, acc: 0.00%] [G loss: 0.327865]\n",
            "Epoch: 86.29 [D loss: 1.385032, acc: 0.00%] [G loss: 0.327065]\n",
            "Epoch: 86.30 [D loss: 1.367537, acc: 0.00%] [G loss: 0.327323]\n",
            "Epoch: 86.31 [D loss: 1.331186, acc: 0.00%] [G loss: 0.327240]\n",
            "Epoch: 86.32 [D loss: 1.364053, acc: 0.00%] [G loss: 0.326868]\n",
            "Epoch: 86.33 [D loss: 1.358807, acc: 0.00%] [G loss: 0.328489]\n",
            "Epoch: 86.34 [D loss: 1.318152, acc: 0.00%] [G loss: 0.329931]\n",
            "Epoch: 86.35 [D loss: 1.364236, acc: 0.00%] [G loss: 0.328187]\n",
            "Epoch: 86.36 [D loss: 1.368063, acc: 0.00%] [G loss: 0.326907]\n",
            "Epoch: 86.37 [D loss: 1.451849, acc: 0.00%] [G loss: 0.327562]\n",
            "Epoch: 86.38 [D loss: 1.483506, acc: 0.00%] [G loss: 0.328745]\n",
            "Epoch: 86.39 [D loss: 1.492371, acc: 0.00%] [G loss: 0.328937]\n",
            "Epoch: 86.40 [D loss: 1.398789, acc: 0.00%] [G loss: 0.327256]\n",
            "Epoch: 86.41 [D loss: 1.345934, acc: 0.00%] [G loss: 0.327992]\n",
            "Epoch: 86.42 [D loss: 1.304046, acc: 0.00%] [G loss: 0.327740]\n",
            "Epoch: 86.43 [D loss: 1.253194, acc: 0.00%] [G loss: 0.328802]\n",
            "Epoch: 86.44 [D loss: 1.291296, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 86.45 [D loss: 1.352064, acc: 0.00%] [G loss: 0.327374]\n",
            "Epoch: 86.46 [D loss: 1.436650, acc: 0.00%] [G loss: 0.327373]\n",
            "Epoch: 86.47 [D loss: 1.405249, acc: 0.00%] [G loss: 0.327136]\n",
            "Epoch: 86.48 [D loss: 1.451151, acc: 0.00%] [G loss: 0.327599]\n",
            "Epoch: 86.49 [D loss: 1.427928, acc: 0.00%] [G loss: 0.326604]\n",
            "Epoch: 86.50 [D loss: 1.408042, acc: 0.00%] [G loss: 0.329220]\n",
            "Epoch: 86.51 [D loss: 1.438182, acc: 0.00%] [G loss: 0.327899]\n",
            "Epoch: 86.52 [D loss: 1.405205, acc: 0.00%] [G loss: 0.326908]\n",
            "Epoch: 86.53 [D loss: 1.331082, acc: 0.00%] [G loss: 0.328296]\n",
            "Epoch: 86.54 [D loss: 1.295142, acc: 0.00%] [G loss: 0.327201]\n",
            "Epoch: 86.55 [D loss: 1.315123, acc: 0.00%] [G loss: 0.327992]\n",
            "Epoch: 86.56 [D loss: 1.318594, acc: 0.00%] [G loss: 0.327549]\n",
            "Epoch: 86.57 [D loss: 1.331060, acc: 0.00%] [G loss: 0.327080]\n",
            "Epoch: 86.58 [D loss: 1.370238, acc: 0.00%] [G loss: 0.327084]\n",
            "Epoch: 86.59 [D loss: 1.341184, acc: 0.00%] [G loss: 0.327468]\n",
            "Epoch: 86.60 [D loss: 1.342521, acc: 0.00%] [G loss: 0.327237]\n",
            "Epoch: 86.61 [D loss: 1.367419, acc: 0.00%] [G loss: 0.327371]\n",
            "Epoch: 86.62 [D loss: 1.370763, acc: 0.00%] [G loss: 0.326594]\n",
            "Epoch: 86.63 [D loss: 1.366946, acc: 0.00%] [G loss: 0.326720]\n",
            "Epoch: 86.64 [D loss: 1.352267, acc: 0.00%] [G loss: 0.328136]\n",
            "Epoch: 86.65 [D loss: 1.406523, acc: 0.00%] [G loss: 0.328316]\n",
            "Epoch: 86.66 [D loss: 1.375615, acc: 0.00%] [G loss: 0.327129]\n",
            "Epoch: 86.67 [D loss: 1.395466, acc: 0.00%] [G loss: 0.327110]\n",
            "Epoch: 86.68 [D loss: 1.401000, acc: 0.00%] [G loss: 0.326146]\n",
            "Epoch: 86.69 [D loss: 1.323405, acc: 0.00%] [G loss: 0.327272]\n",
            "Epoch: 86.70 [D loss: 1.359359, acc: 0.00%] [G loss: 0.326756]\n",
            "Epoch: 86.71 [D loss: 1.329191, acc: 0.00%] [G loss: 0.328038]\n",
            "Epoch: 86.72 [D loss: 1.372025, acc: 0.00%] [G loss: 0.326737]\n",
            "Epoch: 86.73 [D loss: 1.369695, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 86.74 [D loss: 1.368809, acc: 0.00%] [G loss: 0.327382]\n",
            "Epoch: 86.75 [D loss: 1.353023, acc: 0.00%] [G loss: 0.328501]\n",
            "Epoch: 86.76 [D loss: 1.389493, acc: 0.00%] [G loss: 0.328048]\n",
            "Epoch: 86.77 [D loss: 1.393098, acc: 0.00%] [G loss: 0.326548]\n",
            "Epoch: 86.78 [D loss: 1.401181, acc: 0.00%] [G loss: 0.328334]\n",
            "Epoch: 86.79 [D loss: 1.425970, acc: 0.00%] [G loss: 0.328832]\n",
            "Epoch: 86.80 [D loss: 1.385883, acc: 0.00%] [G loss: 0.326937]\n",
            "Epoch: 86.81 [D loss: 1.346782, acc: 0.00%] [G loss: 0.327086]\n",
            "Epoch: 86.82 [D loss: 1.299628, acc: 0.00%] [G loss: 0.328179]\n",
            "Epoch: 86.83 [D loss: 1.368456, acc: 0.00%] [G loss: 0.328296]\n",
            "Epoch: 86.84 [D loss: 1.360999, acc: 0.00%] [G loss: 0.326391]\n",
            "Epoch: 86.85 [D loss: 1.373837, acc: 0.00%] [G loss: 0.327189]\n",
            "Epoch: 86.86 [D loss: 1.422865, acc: 0.00%] [G loss: 0.327196]\n",
            "Epoch: 86.87 [D loss: 1.406919, acc: 0.00%] [G loss: 0.327137]\n",
            "Epoch: 86.88 [D loss: 1.437265, acc: 0.00%] [G loss: 0.328938]\n",
            "Epoch: 86.89 [D loss: 1.379755, acc: 0.00%] [G loss: 0.327904]\n",
            "Epoch: 86.90 [D loss: 1.402324, acc: 0.00%] [G loss: 0.327558]\n",
            "Epoch: 86.91 [D loss: 1.365318, acc: 0.00%] [G loss: 0.327858]\n",
            "Epoch: 86.92 [D loss: 1.372157, acc: 0.00%] [G loss: 0.326722]\n",
            "Epoch: 86.93 [D loss: 1.348381, acc: 0.00%] [G loss: 0.327574]\n",
            "Epoch: 86.94 [D loss: 1.305405, acc: 0.00%] [G loss: 0.327611]\n",
            "Epoch: 86.95 [D loss: 1.350485, acc: 0.00%] [G loss: 0.327641]\n",
            "Epoch: 86.96 [D loss: 1.349158, acc: 0.00%] [G loss: 0.326547]\n",
            "Epoch: 86.97 [D loss: 1.312902, acc: 0.00%] [G loss: 0.326697]\n",
            "Epoch: 86.98 [D loss: 1.431891, acc: 0.00%] [G loss: 0.326910]\n",
            "Epoch: 86.99 [D loss: 1.428371, acc: 0.00%] [G loss: 0.327859]\n",
            "Epoch: 86.100 [D loss: 1.437987, acc: 0.00%] [G loss: 0.327523]\n",
            "Epoch: 86.101 [D loss: 1.446605, acc: 0.00%] [G loss: 0.328292]\n",
            "Epoch: 86.102 [D loss: 1.413238, acc: 0.00%] [G loss: 0.327911]\n",
            "Epoch: 86.103 [D loss: 1.384513, acc: 0.00%] [G loss: 0.327555]\n",
            "Epoch: 86.104 [D loss: 1.324506, acc: 0.00%] [G loss: 0.327008]\n",
            "Epoch: 86.105 [D loss: 1.306524, acc: 0.00%] [G loss: 0.327037]\n",
            "Epoch: 86.106 [D loss: 1.292290, acc: 0.00%] [G loss: 0.328502]\n",
            "Epoch: 86.107 [D loss: 1.283947, acc: 0.00%] [G loss: 0.329343]\n",
            "Epoch: 87.0 [D loss: 1.313203, acc: 0.00%] [G loss: 0.327199]\n",
            "Epoch: 87.1 [D loss: 1.394233, acc: 0.00%] [G loss: 0.326545]\n",
            "Epoch: 87.2 [D loss: 1.443083, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 87.3 [D loss: 1.436892, acc: 0.00%] [G loss: 0.330370]\n",
            "Epoch: 87.4 [D loss: 1.430582, acc: 0.00%] [G loss: 0.328947]\n",
            "Epoch: 87.5 [D loss: 1.392861, acc: 0.00%] [G loss: 0.328571]\n",
            "Epoch: 87.6 [D loss: 1.350629, acc: 0.00%] [G loss: 0.327486]\n",
            "Epoch: 87.7 [D loss: 1.310182, acc: 0.00%] [G loss: 0.327508]\n",
            "Epoch: 87.8 [D loss: 1.276872, acc: 0.00%] [G loss: 0.327038]\n",
            "Epoch: 87.9 [D loss: 1.302743, acc: 0.00%] [G loss: 0.329584]\n",
            "Epoch: 87.10 [D loss: 1.274345, acc: 0.00%] [G loss: 0.326544]\n",
            "Epoch: 87.11 [D loss: 1.354197, acc: 0.00%] [G loss: 0.327038]\n",
            "Epoch: 87.12 [D loss: 1.382205, acc: 0.00%] [G loss: 0.328350]\n",
            "Epoch: 87.13 [D loss: 1.373754, acc: 0.00%] [G loss: 0.327302]\n",
            "Epoch: 87.14 [D loss: 1.397310, acc: 0.00%] [G loss: 0.329157]\n",
            "Epoch: 87.15 [D loss: 1.414069, acc: 0.00%] [G loss: 0.326923]\n",
            "Epoch: 87.16 [D loss: 1.392797, acc: 0.00%] [G loss: 0.330175]\n",
            "Epoch: 87.17 [D loss: 1.362454, acc: 0.00%] [G loss: 0.327613]\n",
            "Epoch: 87.18 [D loss: 1.347548, acc: 0.00%] [G loss: 0.326956]\n",
            "Epoch: 87.19 [D loss: 1.342124, acc: 0.00%] [G loss: 0.329656]\n",
            "Epoch: 87.20 [D loss: 1.276303, acc: 0.00%] [G loss: 0.328008]\n",
            "Epoch: 87.21 [D loss: 1.366140, acc: 0.00%] [G loss: 0.327657]\n",
            "Epoch: 87.22 [D loss: 1.320890, acc: 0.00%] [G loss: 0.327888]\n",
            "Epoch: 87.23 [D loss: 1.381216, acc: 0.00%] [G loss: 0.327337]\n",
            "Epoch: 87.24 [D loss: 1.428245, acc: 0.00%] [G loss: 0.327242]\n",
            "Epoch: 87.25 [D loss: 1.350856, acc: 0.00%] [G loss: 0.328280]\n",
            "Epoch: 87.26 [D loss: 1.405928, acc: 0.00%] [G loss: 0.326513]\n",
            "Epoch: 87.27 [D loss: 1.374244, acc: 0.00%] [G loss: 0.328118]\n",
            "Epoch: 87.28 [D loss: 1.397923, acc: 0.00%] [G loss: 0.327528]\n",
            "Epoch: 87.29 [D loss: 1.318749, acc: 0.00%] [G loss: 0.327348]\n",
            "Epoch: 87.30 [D loss: 1.363883, acc: 0.00%] [G loss: 0.328172]\n",
            "Epoch: 87.31 [D loss: 1.337831, acc: 0.00%] [G loss: 0.328993]\n",
            "Epoch: 87.32 [D loss: 1.348097, acc: 0.00%] [G loss: 0.327041]\n",
            "Epoch: 87.33 [D loss: 1.379129, acc: 0.00%] [G loss: 0.326927]\n",
            "Epoch: 87.34 [D loss: 1.350901, acc: 0.00%] [G loss: 0.327810]\n",
            "Epoch: 87.35 [D loss: 1.455477, acc: 0.00%] [G loss: 0.327079]\n",
            "Epoch: 87.36 [D loss: 1.380205, acc: 0.00%] [G loss: 0.327062]\n",
            "Epoch: 87.37 [D loss: 1.397448, acc: 0.00%] [G loss: 0.328460]\n",
            "Epoch: 87.38 [D loss: 1.402264, acc: 0.00%] [G loss: 0.327645]\n",
            "Epoch: 87.39 [D loss: 1.382509, acc: 0.00%] [G loss: 0.327576]\n",
            "Epoch: 87.40 [D loss: 1.397142, acc: 0.00%] [G loss: 0.326435]\n",
            "Epoch: 87.41 [D loss: 1.334807, acc: 0.00%] [G loss: 0.327025]\n",
            "Epoch: 87.42 [D loss: 1.312224, acc: 0.00%] [G loss: 0.328460]\n",
            "Epoch: 87.43 [D loss: 1.322493, acc: 0.00%] [G loss: 0.328881]\n",
            "Epoch: 87.44 [D loss: 1.281411, acc: 0.00%] [G loss: 0.327662]\n",
            "Epoch: 87.45 [D loss: 1.361614, acc: 0.00%] [G loss: 0.327069]\n",
            "Epoch: 87.46 [D loss: 1.327736, acc: 0.00%] [G loss: 0.328142]\n",
            "Epoch: 87.47 [D loss: 1.373748, acc: 0.00%] [G loss: 0.328405]\n",
            "Epoch: 87.48 [D loss: 1.444495, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 87.49 [D loss: 1.433724, acc: 0.00%] [G loss: 0.327028]\n",
            "Epoch: 87.50 [D loss: 1.492956, acc: 0.00%] [G loss: 0.327569]\n",
            "Epoch: 87.51 [D loss: 1.420012, acc: 0.00%] [G loss: 0.327525]\n",
            "Epoch: 87.52 [D loss: 1.348257, acc: 0.00%] [G loss: 0.326882]\n",
            "Epoch: 87.53 [D loss: 1.356744, acc: 0.00%] [G loss: 0.327040]\n",
            "Epoch: 87.54 [D loss: 1.326839, acc: 0.00%] [G loss: 0.327711]\n",
            "Epoch: 87.55 [D loss: 1.325759, acc: 0.00%] [G loss: 0.326710]\n",
            "Epoch: 87.56 [D loss: 1.335709, acc: 0.00%] [G loss: 0.329237]\n",
            "Epoch: 87.57 [D loss: 1.315562, acc: 0.00%] [G loss: 0.327643]\n",
            "Epoch: 87.58 [D loss: 1.443114, acc: 0.00%] [G loss: 0.326756]\n",
            "Epoch: 87.59 [D loss: 1.444453, acc: 0.00%] [G loss: 0.326806]\n",
            "Epoch: 87.60 [D loss: 1.497485, acc: 0.00%] [G loss: 0.328527]\n",
            "Epoch: 87.61 [D loss: 1.396127, acc: 0.00%] [G loss: 0.329915]\n",
            "Epoch: 87.62 [D loss: 1.438392, acc: 0.00%] [G loss: 0.328166]\n",
            "Epoch: 87.63 [D loss: 1.397058, acc: 0.00%] [G loss: 0.328035]\n",
            "Epoch: 87.64 [D loss: 1.389571, acc: 0.00%] [G loss: 0.326855]\n",
            "Epoch: 87.65 [D loss: 1.312366, acc: 0.00%] [G loss: 0.326930]\n",
            "Epoch: 87.66 [D loss: 1.308091, acc: 0.00%] [G loss: 0.327630]\n",
            "Epoch: 87.67 [D loss: 1.386753, acc: 0.00%] [G loss: 0.328391]\n",
            "Epoch: 87.68 [D loss: 1.300033, acc: 0.00%] [G loss: 0.328553]\n",
            "Epoch: 87.69 [D loss: 1.363340, acc: 0.00%] [G loss: 0.327818]\n",
            "Epoch: 87.70 [D loss: 1.330157, acc: 0.00%] [G loss: 0.328954]\n",
            "Epoch: 87.71 [D loss: 1.390105, acc: 0.00%] [G loss: 0.327418]\n",
            "Epoch: 87.72 [D loss: 1.402711, acc: 0.00%] [G loss: 0.326324]\n",
            "Epoch: 87.73 [D loss: 1.404148, acc: 0.00%] [G loss: 0.327758]\n",
            "Epoch: 87.74 [D loss: 1.365156, acc: 0.00%] [G loss: 0.327555]\n",
            "Epoch: 87.75 [D loss: 1.335697, acc: 0.00%] [G loss: 0.328095]\n",
            "Epoch: 87.76 [D loss: 1.331975, acc: 0.00%] [G loss: 0.327935]\n",
            "Epoch: 87.77 [D loss: 1.327918, acc: 0.00%] [G loss: 0.328839]\n",
            "Epoch: 87.78 [D loss: 1.303762, acc: 0.00%] [G loss: 0.329834]\n",
            "Epoch: 87.79 [D loss: 1.347923, acc: 0.00%] [G loss: 0.327595]\n",
            "Epoch: 87.80 [D loss: 1.371518, acc: 0.00%] [G loss: 0.328258]\n",
            "Epoch: 87.81 [D loss: 1.402912, acc: 0.00%] [G loss: 0.328631]\n",
            "Epoch: 87.82 [D loss: 1.394334, acc: 0.00%] [G loss: 0.327302]\n",
            "Epoch: 87.83 [D loss: 1.398826, acc: 0.00%] [G loss: 0.326929]\n",
            "Epoch: 87.84 [D loss: 1.388600, acc: 0.00%] [G loss: 0.328070]\n",
            "Epoch: 87.85 [D loss: 1.370776, acc: 0.00%] [G loss: 0.327693]\n",
            "Epoch: 87.86 [D loss: 1.362071, acc: 0.00%] [G loss: 0.326787]\n",
            "Epoch: 87.87 [D loss: 1.337091, acc: 0.00%] [G loss: 0.326859]\n",
            "Epoch: 87.88 [D loss: 1.304504, acc: 0.00%] [G loss: 0.327716]\n",
            "Epoch: 87.89 [D loss: 1.353330, acc: 0.00%] [G loss: 0.327168]\n",
            "Epoch: 87.90 [D loss: 1.313863, acc: 0.00%] [G loss: 0.327521]\n",
            "Epoch: 87.91 [D loss: 1.331377, acc: 0.00%] [G loss: 0.326194]\n",
            "Epoch: 87.92 [D loss: 1.328765, acc: 0.00%] [G loss: 0.327630]\n",
            "Epoch: 87.93 [D loss: 1.385099, acc: 0.00%] [G loss: 0.326972]\n",
            "Epoch: 87.94 [D loss: 1.434542, acc: 0.00%] [G loss: 0.327934]\n",
            "Epoch: 87.95 [D loss: 1.410038, acc: 0.00%] [G loss: 0.327743]\n",
            "Epoch: 87.96 [D loss: 1.412650, acc: 0.00%] [G loss: 0.326688]\n",
            "Epoch: 87.97 [D loss: 1.362473, acc: 0.00%] [G loss: 0.327048]\n",
            "Epoch: 87.98 [D loss: 1.410739, acc: 0.00%] [G loss: 0.326560]\n",
            "Epoch: 87.99 [D loss: 1.371986, acc: 0.00%] [G loss: 0.328010]\n",
            "Epoch: 87.100 [D loss: 1.376502, acc: 0.00%] [G loss: 0.328342]\n",
            "Epoch: 87.101 [D loss: 1.335650, acc: 0.00%] [G loss: 0.326477]\n",
            "Epoch: 87.102 [D loss: 1.368994, acc: 0.00%] [G loss: 0.327079]\n",
            "Epoch: 87.103 [D loss: 1.339215, acc: 0.00%] [G loss: 0.327448]\n",
            "Epoch: 87.104 [D loss: 1.337042, acc: 0.00%] [G loss: 0.328441]\n",
            "Epoch: 87.105 [D loss: 1.364399, acc: 0.00%] [G loss: 0.327011]\n",
            "Epoch: 87.106 [D loss: 1.363624, acc: 0.00%] [G loss: 0.326753]\n",
            "Epoch: 87.107 [D loss: 1.408919, acc: 0.00%] [G loss: 0.327524]\n",
            "Epoch: 88.0 [D loss: 1.367679, acc: 0.00%] [G loss: 0.326797]\n",
            "Epoch: 88.1 [D loss: 1.352497, acc: 0.00%] [G loss: 0.326751]\n",
            "Epoch: 88.2 [D loss: 1.377076, acc: 0.00%] [G loss: 0.326480]\n",
            "Epoch: 88.3 [D loss: 1.361941, acc: 0.00%] [G loss: 0.328436]\n",
            "Epoch: 88.4 [D loss: 1.365471, acc: 0.00%] [G loss: 0.327396]\n",
            "Epoch: 88.5 [D loss: 1.334803, acc: 0.00%] [G loss: 0.326901]\n",
            "Epoch: 88.6 [D loss: 1.283609, acc: 0.00%] [G loss: 0.328202]\n",
            "Epoch: 88.7 [D loss: 1.346044, acc: 0.00%] [G loss: 0.327221]\n",
            "Epoch: 88.8 [D loss: 1.369368, acc: 0.00%] [G loss: 0.326394]\n",
            "Epoch: 88.9 [D loss: 1.366000, acc: 0.00%] [G loss: 0.327465]\n",
            "Epoch: 88.10 [D loss: 1.348002, acc: 0.00%] [G loss: 0.327403]\n",
            "Epoch: 88.11 [D loss: 1.350721, acc: 0.00%] [G loss: 0.328216]\n",
            "Epoch: 88.12 [D loss: 1.337941, acc: 0.00%] [G loss: 0.328314]\n",
            "Epoch: 88.13 [D loss: 1.335165, acc: 0.00%] [G loss: 0.328642]\n",
            "Epoch: 88.14 [D loss: 1.360504, acc: 0.00%] [G loss: 0.326674]\n",
            "Epoch: 88.15 [D loss: 1.323502, acc: 0.00%] [G loss: 0.327352]\n",
            "Epoch: 88.16 [D loss: 1.339430, acc: 0.00%] [G loss: 0.328152]\n",
            "Epoch: 88.17 [D loss: 1.306799, acc: 0.00%] [G loss: 0.327040]\n",
            "Epoch: 88.18 [D loss: 1.333330, acc: 0.00%] [G loss: 0.327714]\n",
            "Epoch: 88.19 [D loss: 1.393899, acc: 0.00%] [G loss: 0.326797]\n",
            "Epoch: 88.20 [D loss: 1.410574, acc: 0.00%] [G loss: 0.327960]\n",
            "Epoch: 88.21 [D loss: 1.409779, acc: 0.00%] [G loss: 0.326931]\n",
            "Epoch: 88.22 [D loss: 1.419990, acc: 0.00%] [G loss: 0.327572]\n",
            "Epoch: 88.23 [D loss: 1.398049, acc: 0.00%] [G loss: 0.328029]\n",
            "Epoch: 88.24 [D loss: 1.417323, acc: 0.00%] [G loss: 0.327533]\n",
            "Epoch: 88.25 [D loss: 1.359617, acc: 0.00%] [G loss: 0.328223]\n",
            "Epoch: 88.26 [D loss: 1.368891, acc: 0.00%] [G loss: 0.327142]\n",
            "Epoch: 88.27 [D loss: 1.343005, acc: 0.00%] [G loss: 0.328909]\n",
            "Epoch: 88.28 [D loss: 1.311663, acc: 0.00%] [G loss: 0.328248]\n",
            "Epoch: 88.29 [D loss: 1.353780, acc: 0.00%] [G loss: 0.326873]\n",
            "Epoch: 88.30 [D loss: 1.427108, acc: 0.00%] [G loss: 0.326817]\n",
            "Epoch: 88.31 [D loss: 1.394139, acc: 0.00%] [G loss: 0.328256]\n",
            "Epoch: 88.32 [D loss: 1.399843, acc: 0.00%] [G loss: 0.327995]\n",
            "Epoch: 88.33 [D loss: 1.403443, acc: 0.00%] [G loss: 0.327947]\n",
            "Epoch: 88.34 [D loss: 1.395660, acc: 0.00%] [G loss: 0.327668]\n",
            "Epoch: 88.35 [D loss: 1.334862, acc: 0.00%] [G loss: 0.327048]\n",
            "Epoch: 88.36 [D loss: 1.357611, acc: 0.00%] [G loss: 0.326988]\n",
            "Epoch: 88.37 [D loss: 1.324268, acc: 0.00%] [G loss: 0.326850]\n",
            "Epoch: 88.38 [D loss: 1.299747, acc: 0.00%] [G loss: 0.326379]\n",
            "Epoch: 88.39 [D loss: 1.314525, acc: 0.00%] [G loss: 0.326637]\n",
            "Epoch: 88.40 [D loss: 1.367664, acc: 0.00%] [G loss: 0.326627]\n",
            "Epoch: 88.41 [D loss: 1.368502, acc: 0.00%] [G loss: 0.327172]\n",
            "Epoch: 88.42 [D loss: 1.388071, acc: 0.00%] [G loss: 0.327245]\n",
            "Epoch: 88.43 [D loss: 1.357746, acc: 0.00%] [G loss: 0.327447]\n",
            "Epoch: 88.44 [D loss: 1.417643, acc: 0.00%] [G loss: 0.326584]\n",
            "Epoch: 88.45 [D loss: 1.389078, acc: 0.00%] [G loss: 0.327300]\n",
            "Epoch: 88.46 [D loss: 1.380371, acc: 0.00%] [G loss: 0.327864]\n",
            "Epoch: 88.47 [D loss: 1.372612, acc: 0.00%] [G loss: 0.327469]\n",
            "Epoch: 88.48 [D loss: 1.313910, acc: 0.00%] [G loss: 0.326028]\n",
            "Epoch: 88.49 [D loss: 1.309759, acc: 0.00%] [G loss: 0.327380]\n",
            "Epoch: 88.50 [D loss: 1.285616, acc: 0.00%] [G loss: 0.329037]\n",
            "Epoch: 88.51 [D loss: 1.353496, acc: 0.00%] [G loss: 0.328725]\n",
            "Epoch: 88.52 [D loss: 1.371692, acc: 0.00%] [G loss: 0.326793]\n",
            "Epoch: 88.53 [D loss: 1.379871, acc: 0.00%] [G loss: 0.326533]\n",
            "Epoch: 88.54 [D loss: 1.452701, acc: 0.00%] [G loss: 0.326421]\n",
            "Epoch: 88.55 [D loss: 1.423245, acc: 0.00%] [G loss: 0.327356]\n",
            "Epoch: 88.56 [D loss: 1.439275, acc: 0.00%] [G loss: 0.328149]\n",
            "Epoch: 88.57 [D loss: 1.436341, acc: 0.00%] [G loss: 0.326922]\n",
            "Epoch: 88.58 [D loss: 1.411504, acc: 0.00%] [G loss: 0.326778]\n",
            "Epoch: 88.59 [D loss: 1.377992, acc: 0.00%] [G loss: 0.326580]\n",
            "Epoch: 88.60 [D loss: 1.304687, acc: 0.00%] [G loss: 0.326853]\n",
            "Epoch: 88.61 [D loss: 1.307400, acc: 0.00%] [G loss: 0.328648]\n",
            "Epoch: 88.62 [D loss: 1.313935, acc: 0.00%] [G loss: 0.327834]\n",
            "Epoch: 88.63 [D loss: 1.359701, acc: 0.00%] [G loss: 0.327616]\n",
            "Epoch: 88.64 [D loss: 1.333231, acc: 0.00%] [G loss: 0.327073]\n",
            "Epoch: 88.65 [D loss: 1.410848, acc: 0.00%] [G loss: 0.327504]\n",
            "Epoch: 88.66 [D loss: 1.399004, acc: 0.00%] [G loss: 0.326546]\n",
            "Epoch: 88.67 [D loss: 1.401548, acc: 0.00%] [G loss: 0.329366]\n",
            "Epoch: 88.68 [D loss: 1.433685, acc: 0.00%] [G loss: 0.327545]\n",
            "Epoch: 88.69 [D loss: 1.369928, acc: 0.00%] [G loss: 0.327622]\n",
            "Epoch: 88.70 [D loss: 1.362682, acc: 0.00%] [G loss: 0.326985]\n",
            "Epoch: 88.71 [D loss: 1.375089, acc: 0.00%] [G loss: 0.327877]\n",
            "Epoch: 88.72 [D loss: 1.345320, acc: 0.00%] [G loss: 0.329586]\n",
            "Epoch: 88.73 [D loss: 1.317110, acc: 0.00%] [G loss: 0.326234]\n",
            "Epoch: 88.74 [D loss: 1.349099, acc: 0.00%] [G loss: 0.326466]\n",
            "Epoch: 88.75 [D loss: 1.379616, acc: 0.00%] [G loss: 0.326958]\n",
            "Epoch: 88.76 [D loss: 1.362421, acc: 0.00%] [G loss: 0.328715]\n",
            "Epoch: 88.77 [D loss: 1.359574, acc: 0.00%] [G loss: 0.326675]\n",
            "Epoch: 88.78 [D loss: 1.340404, acc: 0.00%] [G loss: 0.327428]\n",
            "Epoch: 88.79 [D loss: 1.379007, acc: 0.00%] [G loss: 0.327780]\n",
            "Epoch: 88.80 [D loss: 1.318472, acc: 0.00%] [G loss: 0.327271]\n",
            "Epoch: 88.81 [D loss: 1.380828, acc: 0.00%] [G loss: 0.326824]\n",
            "Epoch: 88.82 [D loss: 1.334670, acc: 0.00%] [G loss: 0.327250]\n",
            "Epoch: 88.83 [D loss: 1.346485, acc: 0.00%] [G loss: 0.327322]\n",
            "Epoch: 88.84 [D loss: 1.382620, acc: 0.00%] [G loss: 0.326557]\n",
            "Epoch: 88.85 [D loss: 1.360632, acc: 0.00%] [G loss: 0.326244]\n",
            "Epoch: 88.86 [D loss: 1.375896, acc: 0.00%] [G loss: 0.327361]\n",
            "Epoch: 88.87 [D loss: 1.339398, acc: 0.00%] [G loss: 0.327024]\n",
            "Epoch: 88.88 [D loss: 1.358458, acc: 0.00%] [G loss: 0.328308]\n",
            "Epoch: 88.89 [D loss: 1.370858, acc: 0.00%] [G loss: 0.327286]\n",
            "Epoch: 88.90 [D loss: 1.413412, acc: 0.00%] [G loss: 0.327115]\n",
            "Epoch: 88.91 [D loss: 1.422404, acc: 0.00%] [G loss: 0.327456]\n",
            "Epoch: 88.92 [D loss: 1.426482, acc: 0.00%] [G loss: 0.326367]\n",
            "Epoch: 88.93 [D loss: 1.422660, acc: 0.00%] [G loss: 0.327853]\n",
            "Epoch: 88.94 [D loss: 1.384204, acc: 0.00%] [G loss: 0.328449]\n",
            "Epoch: 88.95 [D loss: 1.380668, acc: 0.00%] [G loss: 0.327372]\n",
            "Epoch: 88.96 [D loss: 1.324115, acc: 0.00%] [G loss: 0.326877]\n",
            "Epoch: 88.97 [D loss: 1.307387, acc: 0.00%] [G loss: 0.327417]\n",
            "Epoch: 88.98 [D loss: 1.339966, acc: 0.00%] [G loss: 0.327936]\n",
            "Epoch: 88.99 [D loss: 1.370170, acc: 0.00%] [G loss: 0.327631]\n",
            "Epoch: 88.100 [D loss: 1.305470, acc: 0.00%] [G loss: 0.327084]\n",
            "Epoch: 88.101 [D loss: 1.348879, acc: 0.00%] [G loss: 0.326606]\n",
            "Epoch: 88.102 [D loss: 1.369432, acc: 0.00%] [G loss: 0.327673]\n",
            "Epoch: 88.103 [D loss: 1.390976, acc: 0.00%] [G loss: 0.327894]\n",
            "Epoch: 88.104 [D loss: 1.394002, acc: 0.00%] [G loss: 0.327301]\n",
            "Epoch: 88.105 [D loss: 1.392834, acc: 0.00%] [G loss: 0.327621]\n",
            "Epoch: 88.106 [D loss: 1.368341, acc: 0.00%] [G loss: 0.326865]\n",
            "Epoch: 88.107 [D loss: 1.382156, acc: 0.00%] [G loss: 0.326929]\n",
            "Epoch: 89.0 [D loss: 1.357437, acc: 0.00%] [G loss: 0.328012]\n",
            "Epoch: 89.1 [D loss: 1.313132, acc: 0.00%] [G loss: 0.327885]\n",
            "Epoch: 89.2 [D loss: 1.329008, acc: 0.00%] [G loss: 0.328782]\n",
            "Epoch: 89.3 [D loss: 1.345304, acc: 0.00%] [G loss: 0.327385]\n",
            "Epoch: 89.4 [D loss: 1.336995, acc: 0.00%] [G loss: 0.326869]\n",
            "Epoch: 89.5 [D loss: 1.376469, acc: 0.00%] [G loss: 0.325916]\n",
            "Epoch: 89.6 [D loss: 1.356750, acc: 0.00%] [G loss: 0.328209]\n",
            "Epoch: 89.7 [D loss: 1.394556, acc: 0.00%] [G loss: 0.326732]\n",
            "Epoch: 89.8 [D loss: 1.419331, acc: 0.00%] [G loss: 0.327087]\n",
            "Epoch: 89.9 [D loss: 1.382903, acc: 0.00%] [G loss: 0.327153]\n",
            "Epoch: 89.10 [D loss: 1.344124, acc: 0.00%] [G loss: 0.329521]\n",
            "Epoch: 89.11 [D loss: 1.355209, acc: 0.00%] [G loss: 0.327124]\n",
            "Epoch: 89.12 [D loss: 1.294416, acc: 0.00%] [G loss: 0.327983]\n",
            "Epoch: 89.13 [D loss: 1.317466, acc: 0.00%] [G loss: 0.327160]\n",
            "Epoch: 89.14 [D loss: 1.314118, acc: 0.00%] [G loss: 0.328470]\n",
            "Epoch: 89.15 [D loss: 1.335829, acc: 0.00%] [G loss: 0.327835]\n",
            "Epoch: 89.16 [D loss: 1.379471, acc: 0.00%] [G loss: 0.326722]\n",
            "Epoch: 89.17 [D loss: 1.407552, acc: 0.00%] [G loss: 0.327638]\n",
            "Epoch: 89.18 [D loss: 1.398926, acc: 0.00%] [G loss: 0.327500]\n",
            "Epoch: 89.19 [D loss: 1.373386, acc: 0.00%] [G loss: 0.326556]\n",
            "Epoch: 89.20 [D loss: 1.398813, acc: 0.00%] [G loss: 0.327339]\n",
            "Epoch: 89.21 [D loss: 1.358123, acc: 0.00%] [G loss: 0.328022]\n",
            "Epoch: 89.22 [D loss: 1.410321, acc: 0.00%] [G loss: 0.328299]\n",
            "Epoch: 89.23 [D loss: 1.380883, acc: 0.00%] [G loss: 0.326483]\n",
            "Epoch: 89.24 [D loss: 1.327871, acc: 0.00%] [G loss: 0.327432]\n",
            "Epoch: 89.25 [D loss: 1.303318, acc: 0.00%] [G loss: 0.327760]\n",
            "Epoch: 89.26 [D loss: 1.334650, acc: 0.00%] [G loss: 0.327129]\n",
            "Epoch: 89.27 [D loss: 1.322893, acc: 0.00%] [G loss: 0.328483]\n",
            "Epoch: 89.28 [D loss: 1.335319, acc: 0.00%] [G loss: 0.326396]\n",
            "Epoch: 89.29 [D loss: 1.368368, acc: 0.00%] [G loss: 0.327819]\n",
            "Epoch: 89.30 [D loss: 1.377013, acc: 0.00%] [G loss: 0.329383]\n",
            "Epoch: 89.31 [D loss: 1.372581, acc: 0.00%] [G loss: 0.326893]\n",
            "Epoch: 89.32 [D loss: 1.394873, acc: 0.00%] [G loss: 0.327183]\n",
            "Epoch: 89.33 [D loss: 1.439531, acc: 0.00%] [G loss: 0.327812]\n",
            "Epoch: 89.34 [D loss: 1.378350, acc: 0.00%] [G loss: 0.327939]\n",
            "Epoch: 89.35 [D loss: 1.354876, acc: 0.00%] [G loss: 0.327516]\n",
            "Epoch: 89.36 [D loss: 1.350969, acc: 0.00%] [G loss: 0.327735]\n",
            "Epoch: 89.37 [D loss: 1.381578, acc: 0.00%] [G loss: 0.326428]\n",
            "Epoch: 89.38 [D loss: 1.315480, acc: 0.00%] [G loss: 0.327509]\n",
            "Epoch: 89.39 [D loss: 1.365059, acc: 0.00%] [G loss: 0.328101]\n",
            "Epoch: 89.40 [D loss: 1.325889, acc: 0.00%] [G loss: 0.327781]\n",
            "Epoch: 89.41 [D loss: 1.340620, acc: 0.00%] [G loss: 0.327685]\n",
            "Epoch: 89.42 [D loss: 1.360828, acc: 0.00%] [G loss: 0.326864]\n",
            "Epoch: 89.43 [D loss: 1.338550, acc: 0.00%] [G loss: 0.328079]\n",
            "Epoch: 89.44 [D loss: 1.371770, acc: 0.00%] [G loss: 0.326602]\n",
            "Epoch: 89.45 [D loss: 1.368230, acc: 0.00%] [G loss: 0.326535]\n",
            "Epoch: 89.46 [D loss: 1.335156, acc: 0.00%] [G loss: 0.327430]\n",
            "Epoch: 89.47 [D loss: 1.361500, acc: 0.00%] [G loss: 0.326864]\n",
            "Epoch: 89.48 [D loss: 1.342834, acc: 0.00%] [G loss: 0.327525]\n",
            "Epoch: 89.49 [D loss: 1.321440, acc: 0.00%] [G loss: 0.326833]\n",
            "Epoch: 89.50 [D loss: 1.335303, acc: 0.00%] [G loss: 0.327205]\n",
            "Epoch: 89.51 [D loss: 1.311789, acc: 0.00%] [G loss: 0.327344]\n",
            "Epoch: 89.52 [D loss: 1.355731, acc: 0.00%] [G loss: 0.326223]\n",
            "Epoch: 89.53 [D loss: 1.363924, acc: 0.00%] [G loss: 0.327255]\n",
            "Epoch: 89.54 [D loss: 1.328220, acc: 0.00%] [G loss: 0.327730]\n",
            "Epoch: 89.55 [D loss: 1.365263, acc: 0.00%] [G loss: 0.327522]\n",
            "Epoch: 89.56 [D loss: 1.374853, acc: 0.00%] [G loss: 0.327088]\n",
            "Epoch: 89.57 [D loss: 1.378320, acc: 0.00%] [G loss: 0.327449]\n",
            "Epoch: 89.58 [D loss: 1.337881, acc: 0.00%] [G loss: 0.327480]\n",
            "Epoch: 89.59 [D loss: 1.345405, acc: 0.00%] [G loss: 0.327017]\n",
            "Epoch: 89.60 [D loss: 1.387302, acc: 0.00%] [G loss: 0.328091]\n",
            "Epoch: 89.61 [D loss: 1.402464, acc: 0.00%] [G loss: 0.326612]\n",
            "Epoch: 89.62 [D loss: 1.409547, acc: 0.00%] [G loss: 0.327641]\n",
            "Epoch: 89.63 [D loss: 1.366640, acc: 0.00%] [G loss: 0.327367]\n",
            "Epoch: 89.64 [D loss: 1.334557, acc: 0.00%] [G loss: 0.327299]\n",
            "Epoch: 89.65 [D loss: 1.389181, acc: 0.00%] [G loss: 0.327219]\n",
            "Epoch: 89.66 [D loss: 1.370328, acc: 0.00%] [G loss: 0.327170]\n",
            "Epoch: 89.67 [D loss: 1.402387, acc: 0.00%] [G loss: 0.326919]\n",
            "Epoch: 89.68 [D loss: 1.369990, acc: 0.00%] [G loss: 0.327764]\n",
            "Epoch: 89.69 [D loss: 1.375848, acc: 0.00%] [G loss: 0.327964]\n",
            "Epoch: 89.70 [D loss: 1.374907, acc: 0.00%] [G loss: 0.327469]\n",
            "Epoch: 89.71 [D loss: 1.383080, acc: 0.00%] [G loss: 0.326681]\n",
            "Epoch: 89.72 [D loss: 1.373386, acc: 0.00%] [G loss: 0.327275]\n",
            "Epoch: 89.73 [D loss: 1.316537, acc: 0.00%] [G loss: 0.329402]\n",
            "Epoch: 89.74 [D loss: 1.330666, acc: 0.00%] [G loss: 0.328073]\n",
            "Epoch: 89.75 [D loss: 1.359873, acc: 0.00%] [G loss: 0.326684]\n",
            "Epoch: 89.76 [D loss: 1.373098, acc: 0.00%] [G loss: 0.326724]\n",
            "Epoch: 89.77 [D loss: 1.351246, acc: 0.00%] [G loss: 0.326658]\n",
            "Epoch: 89.78 [D loss: 1.392909, acc: 0.00%] [G loss: 0.326793]\n",
            "Epoch: 89.79 [D loss: 1.380929, acc: 0.00%] [G loss: 0.328760]\n",
            "Epoch: 89.80 [D loss: 1.411432, acc: 0.00%] [G loss: 0.326996]\n",
            "Epoch: 89.81 [D loss: 1.379779, acc: 0.00%] [G loss: 0.326874]\n",
            "Epoch: 89.82 [D loss: 1.352410, acc: 0.00%] [G loss: 0.327017]\n",
            "Epoch: 89.83 [D loss: 1.412491, acc: 0.00%] [G loss: 0.328093]\n",
            "Epoch: 89.84 [D loss: 1.335049, acc: 0.00%] [G loss: 0.326885]\n",
            "Epoch: 89.85 [D loss: 1.298361, acc: 0.00%] [G loss: 0.327637]\n",
            "Epoch: 89.86 [D loss: 1.329361, acc: 0.00%] [G loss: 0.327301]\n",
            "Epoch: 89.87 [D loss: 1.322089, acc: 0.00%] [G loss: 0.328230]\n",
            "Epoch: 89.88 [D loss: 1.368734, acc: 0.00%] [G loss: 0.327772]\n",
            "Epoch: 89.89 [D loss: 1.361446, acc: 0.00%] [G loss: 0.327646]\n",
            "Epoch: 89.90 [D loss: 1.391598, acc: 0.00%] [G loss: 0.327195]\n",
            "Epoch: 89.91 [D loss: 1.377529, acc: 0.00%] [G loss: 0.327085]\n",
            "Epoch: 89.92 [D loss: 1.428939, acc: 0.00%] [G loss: 0.327948]\n",
            "Epoch: 89.93 [D loss: 1.342013, acc: 0.00%] [G loss: 0.327328]\n",
            "Epoch: 89.94 [D loss: 1.318418, acc: 0.00%] [G loss: 0.326759]\n",
            "Epoch: 89.95 [D loss: 1.320798, acc: 0.00%] [G loss: 0.327595]\n",
            "Epoch: 89.96 [D loss: 1.385076, acc: 0.00%] [G loss: 0.326642]\n",
            "Epoch: 89.97 [D loss: 1.335769, acc: 0.00%] [G loss: 0.326615]\n",
            "Epoch: 89.98 [D loss: 1.380407, acc: 0.00%] [G loss: 0.327592]\n",
            "Epoch: 89.99 [D loss: 1.368946, acc: 0.00%] [G loss: 0.326349]\n",
            "Epoch: 89.100 [D loss: 1.405866, acc: 0.00%] [G loss: 0.326489]\n",
            "Epoch: 89.101 [D loss: 1.373603, acc: 0.00%] [G loss: 0.327368]\n",
            "Epoch: 89.102 [D loss: 1.370824, acc: 0.00%] [G loss: 0.326972]\n",
            "Epoch: 89.103 [D loss: 1.408072, acc: 0.00%] [G loss: 0.327915]\n",
            "Epoch: 89.104 [D loss: 1.343852, acc: 0.00%] [G loss: 0.327767]\n",
            "Epoch: 89.105 [D loss: 1.352156, acc: 0.00%] [G loss: 0.327260]\n",
            "Epoch: 89.106 [D loss: 1.335633, acc: 0.00%] [G loss: 0.326969]\n",
            "Epoch: 89.107 [D loss: 1.312075, acc: 0.00%] [G loss: 0.326559]\n",
            "Epoch: 90.0 [D loss: 1.333302, acc: 0.00%] [G loss: 0.328419]\n",
            "Epoch: 90.1 [D loss: 1.354418, acc: 0.00%] [G loss: 0.327084]\n",
            "Epoch: 90.2 [D loss: 1.350451, acc: 0.00%] [G loss: 0.326772]\n",
            "Epoch: 90.3 [D loss: 1.412843, acc: 0.00%] [G loss: 0.327031]\n",
            "Epoch: 90.4 [D loss: 1.436311, acc: 0.00%] [G loss: 0.327368]\n",
            "Epoch: 90.5 [D loss: 1.394282, acc: 0.00%] [G loss: 0.329472]\n",
            "Epoch: 90.6 [D loss: 1.387552, acc: 0.00%] [G loss: 0.326943]\n",
            "Epoch: 90.7 [D loss: 1.334402, acc: 0.00%] [G loss: 0.326495]\n",
            "Epoch: 90.8 [D loss: 1.330887, acc: 0.00%] [G loss: 0.327079]\n",
            "Epoch: 90.9 [D loss: 1.353281, acc: 0.00%] [G loss: 0.326843]\n",
            "Epoch: 90.10 [D loss: 1.307168, acc: 0.00%] [G loss: 0.327331]\n",
            "Epoch: 90.11 [D loss: 1.263458, acc: 0.00%] [G loss: 0.326720]\n",
            "Epoch: 90.12 [D loss: 1.313100, acc: 0.00%] [G loss: 0.328486]\n",
            "Epoch: 90.13 [D loss: 1.380861, acc: 0.00%] [G loss: 0.326862]\n",
            "Epoch: 90.14 [D loss: 1.342421, acc: 0.00%] [G loss: 0.327155]\n",
            "Epoch: 90.15 [D loss: 1.365589, acc: 0.00%] [G loss: 0.327525]\n",
            "Epoch: 90.16 [D loss: 1.376444, acc: 0.00%] [G loss: 0.327922]\n",
            "Epoch: 90.17 [D loss: 1.350242, acc: 0.00%] [G loss: 0.326456]\n",
            "Epoch: 90.18 [D loss: 1.364230, acc: 0.00%] [G loss: 0.326477]\n",
            "Epoch: 90.19 [D loss: 1.374532, acc: 0.00%] [G loss: 0.326642]\n",
            "Epoch: 90.20 [D loss: 1.307217, acc: 0.00%] [G loss: 0.327609]\n",
            "Epoch: 90.21 [D loss: 1.313249, acc: 0.00%] [G loss: 0.327267]\n",
            "Epoch: 90.22 [D loss: 1.333204, acc: 0.00%] [G loss: 0.327633]\n",
            "Epoch: 90.23 [D loss: 1.404496, acc: 0.00%] [G loss: 0.326843]\n",
            "Epoch: 90.24 [D loss: 1.361938, acc: 0.00%] [G loss: 0.327829]\n",
            "Epoch: 90.25 [D loss: 1.458901, acc: 0.00%] [G loss: 0.327582]\n",
            "Epoch: 90.26 [D loss: 1.422647, acc: 0.00%] [G loss: 0.328122]\n",
            "Epoch: 90.27 [D loss: 1.397386, acc: 0.00%] [G loss: 0.327666]\n",
            "Epoch: 90.28 [D loss: 1.391161, acc: 0.00%] [G loss: 0.328150]\n",
            "Epoch: 90.29 [D loss: 1.399152, acc: 0.00%] [G loss: 0.327186]\n",
            "Epoch: 90.30 [D loss: 1.387918, acc: 0.00%] [G loss: 0.326533]\n",
            "Epoch: 90.31 [D loss: 1.387739, acc: 0.00%] [G loss: 0.327124]\n",
            "Epoch: 90.32 [D loss: 1.369905, acc: 0.00%] [G loss: 0.327192]\n",
            "Epoch: 90.33 [D loss: 1.375216, acc: 0.00%] [G loss: 0.328043]\n",
            "Epoch: 90.34 [D loss: 1.317323, acc: 0.00%] [G loss: 0.326301]\n",
            "Epoch: 90.35 [D loss: 1.329236, acc: 0.00%] [G loss: 0.326964]\n",
            "Epoch: 90.36 [D loss: 1.298757, acc: 0.00%] [G loss: 0.330568]\n",
            "Epoch: 90.37 [D loss: 1.283781, acc: 0.00%] [G loss: 0.325958]\n",
            "Epoch: 90.38 [D loss: 1.353557, acc: 0.00%] [G loss: 0.327235]\n",
            "Epoch: 90.39 [D loss: 1.375756, acc: 0.00%] [G loss: 0.326717]\n",
            "Epoch: 90.40 [D loss: 1.416925, acc: 0.00%] [G loss: 0.327318]\n",
            "Epoch: 90.41 [D loss: 1.416885, acc: 0.00%] [G loss: 0.328597]\n",
            "Epoch: 90.42 [D loss: 1.398250, acc: 0.00%] [G loss: 0.326720]\n",
            "Epoch: 90.43 [D loss: 1.413746, acc: 0.00%] [G loss: 0.327482]\n",
            "Epoch: 90.44 [D loss: 1.380276, acc: 0.00%] [G loss: 0.328276]\n",
            "Epoch: 90.45 [D loss: 1.349644, acc: 0.00%] [G loss: 0.327325]\n",
            "Epoch: 90.46 [D loss: 1.380894, acc: 0.00%] [G loss: 0.327570]\n",
            "Epoch: 90.47 [D loss: 1.332949, acc: 0.00%] [G loss: 0.328272]\n",
            "Epoch: 90.48 [D loss: 1.321385, acc: 0.00%] [G loss: 0.327697]\n",
            "Epoch: 90.49 [D loss: 1.323121, acc: 0.00%] [G loss: 0.327973]\n",
            "Epoch: 90.50 [D loss: 1.347098, acc: 0.00%] [G loss: 0.327036]\n",
            "Epoch: 90.51 [D loss: 1.391738, acc: 0.00%] [G loss: 0.326553]\n",
            "Epoch: 90.52 [D loss: 1.433633, acc: 0.00%] [G loss: 0.327823]\n",
            "Epoch: 90.53 [D loss: 1.404321, acc: 0.00%] [G loss: 0.327117]\n",
            "Epoch: 90.54 [D loss: 1.379948, acc: 0.00%] [G loss: 0.327010]\n",
            "Epoch: 90.55 [D loss: 1.398282, acc: 0.00%] [G loss: 0.327399]\n",
            "Epoch: 90.56 [D loss: 1.409119, acc: 0.00%] [G loss: 0.327025]\n",
            "Epoch: 90.57 [D loss: 1.369192, acc: 0.00%] [G loss: 0.326234]\n",
            "Epoch: 90.58 [D loss: 1.403484, acc: 0.00%] [G loss: 0.327006]\n",
            "Epoch: 90.59 [D loss: 1.326467, acc: 0.00%] [G loss: 0.326734]\n",
            "Epoch: 90.60 [D loss: 1.328499, acc: 0.00%] [G loss: 0.329388]\n",
            "Epoch: 90.61 [D loss: 1.357453, acc: 0.00%] [G loss: 0.327244]\n",
            "Epoch: 90.62 [D loss: 1.303702, acc: 0.00%] [G loss: 0.327757]\n",
            "Epoch: 90.63 [D loss: 1.395600, acc: 0.00%] [G loss: 0.328584]\n",
            "Epoch: 90.64 [D loss: 1.378547, acc: 0.00%] [G loss: 0.327582]\n",
            "Epoch: 90.65 [D loss: 1.414180, acc: 0.00%] [G loss: 0.327028]\n",
            "Epoch: 90.66 [D loss: 1.374644, acc: 0.00%] [G loss: 0.327430]\n",
            "Epoch: 90.67 [D loss: 1.403141, acc: 0.00%] [G loss: 0.326510]\n",
            "Epoch: 90.68 [D loss: 1.326279, acc: 0.00%] [G loss: 0.326553]\n",
            "Epoch: 90.69 [D loss: 1.370874, acc: 0.00%] [G loss: 0.327432]\n",
            "Epoch: 90.70 [D loss: 1.370753, acc: 0.00%] [G loss: 0.326599]\n",
            "Epoch: 90.71 [D loss: 1.403325, acc: 0.00%] [G loss: 0.327384]\n",
            "Epoch: 90.72 [D loss: 1.384354, acc: 0.00%] [G loss: 0.327034]\n",
            "Epoch: 90.73 [D loss: 1.378004, acc: 0.00%] [G loss: 0.329315]\n",
            "Epoch: 90.74 [D loss: 1.407955, acc: 0.00%] [G loss: 0.326364]\n",
            "Epoch: 90.75 [D loss: 1.394381, acc: 0.00%] [G loss: 0.326215]\n",
            "Epoch: 90.76 [D loss: 1.490079, acc: 0.00%] [G loss: 0.328035]\n",
            "Epoch: 90.77 [D loss: 1.475172, acc: 0.00%] [G loss: 0.327764]\n",
            "Epoch: 90.78 [D loss: 1.397845, acc: 0.00%] [G loss: 0.327531]\n",
            "Epoch: 90.79 [D loss: 1.378904, acc: 0.00%] [G loss: 0.328288]\n",
            "Epoch: 90.80 [D loss: 1.419818, acc: 0.00%] [G loss: 0.326394]\n",
            "Epoch: 90.81 [D loss: 1.366761, acc: 0.00%] [G loss: 0.326518]\n",
            "Epoch: 90.82 [D loss: 1.379268, acc: 0.00%] [G loss: 0.327812]\n",
            "Epoch: 90.83 [D loss: 1.300161, acc: 0.00%] [G loss: 0.328416]\n",
            "Epoch: 90.84 [D loss: 1.357529, acc: 0.00%] [G loss: 0.326889]\n",
            "Epoch: 90.85 [D loss: 1.368746, acc: 0.00%] [G loss: 0.328199]\n",
            "Epoch: 90.86 [D loss: 1.384292, acc: 0.00%] [G loss: 0.326692]\n",
            "Epoch: 90.87 [D loss: 1.339221, acc: 0.00%] [G loss: 0.326333]\n",
            "Epoch: 90.88 [D loss: 1.387973, acc: 0.00%] [G loss: 0.326576]\n",
            "Epoch: 90.89 [D loss: 1.348725, acc: 0.00%] [G loss: 0.326876]\n",
            "Epoch: 90.90 [D loss: 1.366764, acc: 0.00%] [G loss: 0.327022]\n",
            "Epoch: 90.91 [D loss: 1.387239, acc: 0.00%] [G loss: 0.326665]\n",
            "Epoch: 90.92 [D loss: 1.448416, acc: 0.00%] [G loss: 0.326583]\n",
            "Epoch: 90.93 [D loss: 1.382789, acc: 0.00%] [G loss: 0.326516]\n",
            "Epoch: 90.94 [D loss: 1.348086, acc: 0.00%] [G loss: 0.328524]\n",
            "Epoch: 90.95 [D loss: 1.401837, acc: 0.00%] [G loss: 0.326545]\n",
            "Epoch: 90.96 [D loss: 1.330682, acc: 0.00%] [G loss: 0.327021]\n",
            "Epoch: 90.97 [D loss: 1.330788, acc: 0.00%] [G loss: 0.327354]\n",
            "Epoch: 90.98 [D loss: 1.331860, acc: 0.00%] [G loss: 0.326351]\n",
            "Epoch: 90.99 [D loss: 1.321121, acc: 0.00%] [G loss: 0.328328]\n",
            "Epoch: 90.100 [D loss: 1.286147, acc: 0.00%] [G loss: 0.327033]\n",
            "Epoch: 90.101 [D loss: 1.335215, acc: 0.00%] [G loss: 0.327232]\n",
            "Epoch: 90.102 [D loss: 1.342690, acc: 0.00%] [G loss: 0.327637]\n",
            "Epoch: 90.103 [D loss: 1.431446, acc: 0.00%] [G loss: 0.327496]\n",
            "Epoch: 90.104 [D loss: 1.389575, acc: 0.00%] [G loss: 0.328222]\n",
            "Epoch: 90.105 [D loss: 1.367396, acc: 0.00%] [G loss: 0.327741]\n",
            "Epoch: 90.106 [D loss: 1.375725, acc: 0.00%] [G loss: 0.327542]\n",
            "Epoch: 90.107 [D loss: 1.363726, acc: 0.00%] [G loss: 0.327000]\n",
            "INFO:tensorflow:Assets written to: GAN_weights/dis_0.00000350_weights/assets\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: GAN_weights/gen_0.00000350_weights/assets\n",
            "Epoch: 91.0 [D loss: 1.345076, acc: 0.00%] [G loss: 0.327895]\n",
            "Epoch: 91.1 [D loss: 1.321231, acc: 0.00%] [G loss: 0.328223]\n",
            "Epoch: 91.2 [D loss: 1.348252, acc: 0.00%] [G loss: 0.326782]\n",
            "Epoch: 91.3 [D loss: 1.399415, acc: 0.00%] [G loss: 0.326404]\n",
            "Epoch: 91.4 [D loss: 1.429698, acc: 0.00%] [G loss: 0.327136]\n",
            "Epoch: 91.5 [D loss: 1.414523, acc: 0.00%] [G loss: 0.328151]\n",
            "Epoch: 91.6 [D loss: 1.442682, acc: 0.00%] [G loss: 0.327032]\n",
            "Epoch: 91.7 [D loss: 1.434041, acc: 0.00%] [G loss: 0.329861]\n",
            "Epoch: 91.8 [D loss: 1.345410, acc: 0.00%] [G loss: 0.326769]\n",
            "Epoch: 91.9 [D loss: 1.338794, acc: 0.00%] [G loss: 0.327380]\n",
            "Epoch: 91.10 [D loss: 1.345911, acc: 0.00%] [G loss: 0.327875]\n",
            "Epoch: 91.11 [D loss: 1.333139, acc: 0.00%] [G loss: 0.326409]\n",
            "Epoch: 91.12 [D loss: 1.378383, acc: 0.00%] [G loss: 0.327512]\n",
            "Epoch: 91.13 [D loss: 1.340689, acc: 0.00%] [G loss: 0.326619]\n",
            "Epoch: 91.14 [D loss: 1.463017, acc: 0.00%] [G loss: 0.327520]\n",
            "Epoch: 91.15 [D loss: 1.371371, acc: 0.00%] [G loss: 0.326747]\n",
            "Epoch: 91.16 [D loss: 1.382662, acc: 0.00%] [G loss: 0.328178]\n",
            "Epoch: 91.17 [D loss: 1.390193, acc: 0.00%] [G loss: 0.327994]\n",
            "Epoch: 91.18 [D loss: 1.346394, acc: 0.00%] [G loss: 0.327581]\n",
            "Epoch: 91.19 [D loss: 1.372105, acc: 0.00%] [G loss: 0.326288]\n",
            "Epoch: 91.20 [D loss: 1.362329, acc: 0.00%] [G loss: 0.328463]\n",
            "Epoch: 91.21 [D loss: 1.325503, acc: 0.00%] [G loss: 0.327691]\n",
            "Epoch: 91.22 [D loss: 1.299798, acc: 0.00%] [G loss: 0.328565]\n",
            "Epoch: 91.23 [D loss: 1.361370, acc: 0.00%] [G loss: 0.326333]\n",
            "Epoch: 91.24 [D loss: 1.406085, acc: 0.00%] [G loss: 0.327156]\n",
            "Epoch: 91.25 [D loss: 1.429439, acc: 0.00%] [G loss: 0.327145]\n",
            "Epoch: 91.26 [D loss: 1.412249, acc: 0.00%] [G loss: 0.328415]\n",
            "Epoch: 91.27 [D loss: 1.380915, acc: 0.00%] [G loss: 0.329090]\n",
            "Epoch: 91.28 [D loss: 1.364485, acc: 0.00%] [G loss: 0.327226]\n",
            "Epoch: 91.29 [D loss: 1.372836, acc: 0.00%] [G loss: 0.326855]\n",
            "Epoch: 91.30 [D loss: 1.376057, acc: 0.00%] [G loss: 0.327051]\n",
            "Epoch: 91.31 [D loss: 1.315547, acc: 0.00%] [G loss: 0.328595]\n",
            "Epoch: 91.32 [D loss: 1.343543, acc: 0.00%] [G loss: 0.327701]\n",
            "Epoch: 91.33 [D loss: 1.358479, acc: 0.00%] [G loss: 0.326440]\n",
            "Epoch: 91.34 [D loss: 1.349604, acc: 0.00%] [G loss: 0.326849]\n",
            "Epoch: 91.35 [D loss: 1.400632, acc: 0.00%] [G loss: 0.327257]\n",
            "Epoch: 91.36 [D loss: 1.368608, acc: 0.00%] [G loss: 0.327443]\n",
            "Epoch: 91.37 [D loss: 1.365447, acc: 0.00%] [G loss: 0.326799]\n",
            "Epoch: 91.38 [D loss: 1.419011, acc: 0.00%] [G loss: 0.326438]\n",
            "Epoch: 91.39 [D loss: 1.370057, acc: 0.00%] [G loss: 0.327277]\n",
            "Epoch: 91.40 [D loss: 1.334804, acc: 0.00%] [G loss: 0.328192]\n",
            "Epoch: 91.41 [D loss: 1.364792, acc: 0.00%] [G loss: 0.328332]\n",
            "Epoch: 91.42 [D loss: 1.326222, acc: 0.00%] [G loss: 0.328120]\n",
            "Epoch: 91.43 [D loss: 1.379258, acc: 0.00%] [G loss: 0.326511]\n",
            "Epoch: 91.44 [D loss: 1.400306, acc: 0.00%] [G loss: 0.328235]\n",
            "Epoch: 91.45 [D loss: 1.374180, acc: 0.00%] [G loss: 0.327466]\n",
            "Epoch: 91.46 [D loss: 1.382136, acc: 0.00%] [G loss: 0.327097]\n",
            "Epoch: 91.47 [D loss: 1.340088, acc: 0.00%] [G loss: 0.326706]\n",
            "Epoch: 91.48 [D loss: 1.328859, acc: 0.00%] [G loss: 0.327432]\n",
            "Epoch: 91.49 [D loss: 1.357531, acc: 0.00%] [G loss: 0.327772]\n",
            "Epoch: 91.50 [D loss: 1.377154, acc: 0.00%] [G loss: 0.327352]\n",
            "Epoch: 91.51 [D loss: 1.384490, acc: 0.00%] [G loss: 0.327510]\n",
            "Epoch: 91.52 [D loss: 1.420336, acc: 0.00%] [G loss: 0.326955]\n",
            "Epoch: 91.53 [D loss: 1.382215, acc: 0.00%] [G loss: 0.327674]\n",
            "Epoch: 91.54 [D loss: 1.404711, acc: 0.00%] [G loss: 0.326987]\n",
            "Epoch: 91.55 [D loss: 1.385137, acc: 0.00%] [G loss: 0.327515]\n",
            "Epoch: 91.56 [D loss: 1.365172, acc: 0.00%] [G loss: 0.326707]\n",
            "Epoch: 91.57 [D loss: 1.326196, acc: 0.00%] [G loss: 0.327643]\n",
            "Epoch: 91.58 [D loss: 1.372696, acc: 0.00%] [G loss: 0.327825]\n",
            "Epoch: 91.59 [D loss: 1.336411, acc: 0.00%] [G loss: 0.327722]\n",
            "Epoch: 91.60 [D loss: 1.355717, acc: 0.00%] [G loss: 0.326834]\n",
            "Epoch: 91.61 [D loss: 1.342146, acc: 0.00%] [G loss: 0.327884]\n",
            "Epoch: 91.62 [D loss: 1.384238, acc: 0.00%] [G loss: 0.327592]\n",
            "Epoch: 91.63 [D loss: 1.408842, acc: 0.00%] [G loss: 0.328419]\n",
            "Epoch: 91.64 [D loss: 1.349506, acc: 0.00%] [G loss: 0.327562]\n",
            "Epoch: 91.65 [D loss: 1.324338, acc: 0.00%] [G loss: 0.327464]\n",
            "Epoch: 91.66 [D loss: 1.345991, acc: 0.00%] [G loss: 0.326515]\n",
            "Epoch: 91.67 [D loss: 1.318546, acc: 0.00%] [G loss: 0.327230]\n",
            "Epoch: 91.68 [D loss: 1.299192, acc: 0.00%] [G loss: 0.327663]\n",
            "Epoch: 91.69 [D loss: 1.346665, acc: 0.00%] [G loss: 0.326196]\n",
            "Epoch: 91.70 [D loss: 1.381850, acc: 0.00%] [G loss: 0.326327]\n",
            "Epoch: 91.71 [D loss: 1.346139, acc: 0.00%] [G loss: 0.326866]\n",
            "Epoch: 91.72 [D loss: 1.430676, acc: 0.00%] [G loss: 0.328196]\n",
            "Epoch: 91.73 [D loss: 1.357767, acc: 0.00%] [G loss: 0.326973]\n",
            "Epoch: 91.74 [D loss: 1.372165, acc: 0.00%] [G loss: 0.326700]\n",
            "Epoch: 91.75 [D loss: 1.351055, acc: 0.00%] [G loss: 0.326467]\n",
            "Epoch: 91.76 [D loss: 1.307595, acc: 0.00%] [G loss: 0.327568]\n",
            "Epoch: 91.77 [D loss: 1.369183, acc: 0.00%] [G loss: 0.326524]\n",
            "Epoch: 91.78 [D loss: 1.331033, acc: 0.00%] [G loss: 0.326162]\n",
            "Epoch: 91.79 [D loss: 1.377973, acc: 0.00%] [G loss: 0.328561]\n",
            "Epoch: 91.80 [D loss: 1.410695, acc: 0.00%] [G loss: 0.328135]\n",
            "Epoch: 91.81 [D loss: 1.412843, acc: 0.00%] [G loss: 0.328611]\n",
            "Epoch: 91.82 [D loss: 1.412557, acc: 0.00%] [G loss: 0.328684]\n",
            "Epoch: 91.83 [D loss: 1.392961, acc: 0.00%] [G loss: 0.327077]\n",
            "Epoch: 91.84 [D loss: 1.335315, acc: 0.00%] [G loss: 0.326966]\n",
            "Epoch: 91.85 [D loss: 1.378318, acc: 0.00%] [G loss: 0.326718]\n",
            "Epoch: 91.86 [D loss: 1.280412, acc: 0.00%] [G loss: 0.327417]\n",
            "Epoch: 91.87 [D loss: 1.301179, acc: 0.00%] [G loss: 0.326526]\n",
            "Epoch: 91.88 [D loss: 1.348912, acc: 0.00%] [G loss: 0.327421]\n",
            "Epoch: 91.89 [D loss: 1.282194, acc: 0.00%] [G loss: 0.328062]\n",
            "Epoch: 91.90 [D loss: 1.395672, acc: 0.00%] [G loss: 0.327697]\n",
            "Epoch: 91.91 [D loss: 1.449903, acc: 0.00%] [G loss: 0.326576]\n",
            "Epoch: 91.92 [D loss: 1.405774, acc: 0.00%] [G loss: 0.328515]\n",
            "Epoch: 91.93 [D loss: 1.373694, acc: 0.00%] [G loss: 0.327969]\n",
            "Epoch: 91.94 [D loss: 1.417124, acc: 0.00%] [G loss: 0.327073]\n",
            "Epoch: 91.95 [D loss: 1.356147, acc: 0.00%] [G loss: 0.328433]\n",
            "Epoch: 91.96 [D loss: 1.398334, acc: 0.00%] [G loss: 0.328265]\n",
            "Epoch: 91.97 [D loss: 1.390899, acc: 0.00%] [G loss: 0.326163]\n",
            "Epoch: 91.98 [D loss: 1.363485, acc: 0.00%] [G loss: 0.328115]\n",
            "Epoch: 91.99 [D loss: 1.367516, acc: 0.00%] [G loss: 0.327305]\n",
            "Epoch: 91.100 [D loss: 1.337721, acc: 0.00%] [G loss: 0.327074]\n",
            "Epoch: 91.101 [D loss: 1.318489, acc: 0.00%] [G loss: 0.326889]\n",
            "Epoch: 91.102 [D loss: 1.431661, acc: 0.00%] [G loss: 0.327987]\n",
            "Epoch: 91.103 [D loss: 1.372285, acc: 0.00%] [G loss: 0.327702]\n",
            "Epoch: 91.104 [D loss: 1.403141, acc: 0.00%] [G loss: 0.326443]\n",
            "Epoch: 91.105 [D loss: 1.433983, acc: 0.00%] [G loss: 0.327221]\n",
            "Epoch: 91.106 [D loss: 1.345944, acc: 0.00%] [G loss: 0.326657]\n",
            "Epoch: 91.107 [D loss: 1.316019, acc: 0.00%] [G loss: 0.327163]\n",
            "Epoch: 92.0 [D loss: 1.344487, acc: 0.00%] [G loss: 0.327789]\n",
            "Epoch: 92.1 [D loss: 1.358736, acc: 0.00%] [G loss: 0.327654]\n",
            "Epoch: 92.2 [D loss: 1.349049, acc: 0.00%] [G loss: 0.326393]\n",
            "Epoch: 92.3 [D loss: 1.347054, acc: 0.00%] [G loss: 0.326755]\n",
            "Epoch: 92.4 [D loss: 1.351548, acc: 0.00%] [G loss: 0.328229]\n",
            "Epoch: 92.5 [D loss: 1.435257, acc: 0.00%] [G loss: 0.327133]\n",
            "Epoch: 92.6 [D loss: 1.382854, acc: 0.00%] [G loss: 0.327530]\n",
            "Epoch: 92.7 [D loss: 1.382086, acc: 0.00%] [G loss: 0.327957]\n",
            "Epoch: 92.8 [D loss: 1.400875, acc: 0.00%] [G loss: 0.327659]\n",
            "Epoch: 92.9 [D loss: 1.337222, acc: 0.00%] [G loss: 0.327292]\n",
            "Epoch: 92.10 [D loss: 1.302559, acc: 0.00%] [G loss: 0.327279]\n",
            "Epoch: 92.11 [D loss: 1.312485, acc: 0.00%] [G loss: 0.327030]\n",
            "Epoch: 92.12 [D loss: 1.351830, acc: 0.00%] [G loss: 0.329324]\n",
            "Epoch: 92.13 [D loss: 1.338045, acc: 0.00%] [G loss: 0.327097]\n",
            "Epoch: 92.14 [D loss: 1.384088, acc: 0.00%] [G loss: 0.328331]\n",
            "Epoch: 92.15 [D loss: 1.395181, acc: 0.00%] [G loss: 0.326441]\n",
            "Epoch: 92.16 [D loss: 1.413033, acc: 0.00%] [G loss: 0.327574]\n",
            "Epoch: 92.17 [D loss: 1.393331, acc: 0.00%] [G loss: 0.327956]\n",
            "Epoch: 92.18 [D loss: 1.428091, acc: 0.00%] [G loss: 0.328068]\n",
            "Epoch: 92.19 [D loss: 1.403140, acc: 0.00%] [G loss: 0.327485]\n",
            "Epoch: 92.20 [D loss: 1.335985, acc: 0.00%] [G loss: 0.327101]\n",
            "Epoch: 92.21 [D loss: 1.320972, acc: 0.00%] [G loss: 0.328471]\n",
            "Epoch: 92.22 [D loss: 1.286901, acc: 0.00%] [G loss: 0.327874]\n",
            "Epoch: 92.23 [D loss: 1.305432, acc: 0.00%] [G loss: 0.327057]\n",
            "Epoch: 92.24 [D loss: 1.317415, acc: 0.00%] [G loss: 0.326326]\n",
            "Epoch: 92.25 [D loss: 1.388978, acc: 0.00%] [G loss: 0.327035]\n",
            "Epoch: 92.26 [D loss: 1.352240, acc: 0.00%] [G loss: 0.327292]\n",
            "Epoch: 92.27 [D loss: 1.357032, acc: 0.00%] [G loss: 0.326489]\n",
            "Epoch: 92.28 [D loss: 1.368950, acc: 0.00%] [G loss: 0.326822]\n",
            "Epoch: 92.29 [D loss: 1.416150, acc: 0.00%] [G loss: 0.326350]\n",
            "Epoch: 92.30 [D loss: 1.402506, acc: 0.00%] [G loss: 0.326812]\n",
            "Epoch: 92.31 [D loss: 1.369298, acc: 0.00%] [G loss: 0.327269]\n",
            "Epoch: 92.32 [D loss: 1.356899, acc: 0.00%] [G loss: 0.326476]\n",
            "Epoch: 92.33 [D loss: 1.363963, acc: 0.00%] [G loss: 0.327032]\n",
            "Epoch: 92.34 [D loss: 1.391759, acc: 0.00%] [G loss: 0.327099]\n",
            "Epoch: 92.35 [D loss: 1.418551, acc: 0.00%] [G loss: 0.326600]\n",
            "Epoch: 92.36 [D loss: 1.337725, acc: 0.00%] [G loss: 0.327187]\n",
            "Epoch: 92.37 [D loss: 1.334791, acc: 0.00%] [G loss: 0.326617]\n",
            "Epoch: 92.38 [D loss: 1.344901, acc: 0.00%] [G loss: 0.327667]\n",
            "Epoch: 92.39 [D loss: 1.326755, acc: 0.00%] [G loss: 0.327205]\n",
            "Epoch: 92.40 [D loss: 1.339634, acc: 0.00%] [G loss: 0.326823]\n",
            "Epoch: 92.41 [D loss: 1.379994, acc: 0.00%] [G loss: 0.326415]\n",
            "Epoch: 92.42 [D loss: 1.358025, acc: 0.00%] [G loss: 0.327278]\n",
            "Epoch: 92.43 [D loss: 1.390183, acc: 0.00%] [G loss: 0.329167]\n",
            "Epoch: 92.44 [D loss: 1.414809, acc: 0.00%] [G loss: 0.327606]\n",
            "Epoch: 92.45 [D loss: 1.340862, acc: 0.00%] [G loss: 0.327479]\n",
            "Epoch: 92.46 [D loss: 1.325516, acc: 0.00%] [G loss: 0.327277]\n",
            "Epoch: 92.47 [D loss: 1.351047, acc: 0.00%] [G loss: 0.327165]\n",
            "Epoch: 92.48 [D loss: 1.383640, acc: 0.00%] [G loss: 0.327020]\n",
            "Epoch: 92.49 [D loss: 1.371466, acc: 0.00%] [G loss: 0.327150]\n",
            "Epoch: 92.50 [D loss: 1.344848, acc: 0.00%] [G loss: 0.327632]\n",
            "Epoch: 92.51 [D loss: 1.322622, acc: 0.00%] [G loss: 0.328427]\n",
            "Epoch: 92.52 [D loss: 1.375971, acc: 0.00%] [G loss: 0.326470]\n",
            "Epoch: 92.53 [D loss: 1.360199, acc: 0.00%] [G loss: 0.326397]\n",
            "Epoch: 92.54 [D loss: 1.368787, acc: 0.00%] [G loss: 0.327742]\n",
            "Epoch: 92.55 [D loss: 1.385529, acc: 0.00%] [G loss: 0.327487]\n",
            "Epoch: 92.56 [D loss: 1.427398, acc: 0.00%] [G loss: 0.326586]\n",
            "Epoch: 92.57 [D loss: 1.379623, acc: 0.00%] [G loss: 0.327160]\n",
            "Epoch: 92.58 [D loss: 1.338950, acc: 0.00%] [G loss: 0.327195]\n",
            "Epoch: 92.59 [D loss: 1.406944, acc: 0.00%] [G loss: 0.327366]\n",
            "Epoch: 92.60 [D loss: 1.355458, acc: 0.00%] [G loss: 0.327170]\n",
            "Epoch: 92.61 [D loss: 1.362269, acc: 0.00%] [G loss: 0.327243]\n",
            "Epoch: 92.62 [D loss: 1.349208, acc: 0.00%] [G loss: 0.327248]\n",
            "Epoch: 92.63 [D loss: 1.318856, acc: 0.00%] [G loss: 0.326708]\n",
            "Epoch: 92.64 [D loss: 1.347971, acc: 0.00%] [G loss: 0.327368]\n",
            "Epoch: 92.65 [D loss: 1.360244, acc: 0.00%] [G loss: 0.327135]\n",
            "Epoch: 92.66 [D loss: 1.388249, acc: 0.00%] [G loss: 0.327137]\n",
            "Epoch: 92.67 [D loss: 1.391111, acc: 0.00%] [G loss: 0.326625]\n",
            "Epoch: 92.68 [D loss: 1.379890, acc: 0.00%] [G loss: 0.328310]\n",
            "Epoch: 92.69 [D loss: 1.465223, acc: 0.00%] [G loss: 0.327809]\n",
            "Epoch: 92.70 [D loss: 1.403911, acc: 0.00%] [G loss: 0.326828]\n",
            "Epoch: 92.71 [D loss: 1.373787, acc: 0.00%] [G loss: 0.327849]\n",
            "Epoch: 92.72 [D loss: 1.338447, acc: 0.00%] [G loss: 0.328236]\n",
            "Epoch: 92.73 [D loss: 1.374813, acc: 0.00%] [G loss: 0.327029]\n",
            "Epoch: 92.74 [D loss: 1.339533, acc: 0.00%] [G loss: 0.328234]\n",
            "Epoch: 92.75 [D loss: 1.407752, acc: 0.00%] [G loss: 0.327583]\n",
            "Epoch: 92.76 [D loss: 1.383999, acc: 0.00%] [G loss: 0.327902]\n",
            "Epoch: 92.77 [D loss: 1.361168, acc: 0.00%] [G loss: 0.327009]\n",
            "Epoch: 92.78 [D loss: 1.405789, acc: 0.00%] [G loss: 0.327121]\n",
            "Epoch: 92.79 [D loss: 1.349510, acc: 0.00%] [G loss: 0.327120]\n",
            "Epoch: 92.80 [D loss: 1.351175, acc: 0.00%] [G loss: 0.327154]\n",
            "Epoch: 92.81 [D loss: 1.305174, acc: 0.00%] [G loss: 0.327480]\n",
            "Epoch: 92.82 [D loss: 1.341244, acc: 0.00%] [G loss: 0.327284]\n",
            "Epoch: 92.83 [D loss: 1.387357, acc: 0.00%] [G loss: 0.326385]\n",
            "Epoch: 92.84 [D loss: 1.373563, acc: 0.00%] [G loss: 0.326412]\n",
            "Epoch: 92.85 [D loss: 1.370624, acc: 0.00%] [G loss: 0.328088]\n",
            "Epoch: 92.86 [D loss: 1.442313, acc: 0.00%] [G loss: 0.326510]\n",
            "Epoch: 92.87 [D loss: 1.395217, acc: 0.00%] [G loss: 0.326490]\n",
            "Epoch: 92.88 [D loss: 1.334649, acc: 0.00%] [G loss: 0.326794]\n",
            "Epoch: 92.89 [D loss: 1.353381, acc: 0.00%] [G loss: 0.327294]\n",
            "Epoch: 92.90 [D loss: 1.343135, acc: 0.00%] [G loss: 0.326593]\n",
            "Epoch: 92.91 [D loss: 1.350061, acc: 0.00%] [G loss: 0.327066]\n",
            "Epoch: 92.92 [D loss: 1.344472, acc: 0.00%] [G loss: 0.326501]\n",
            "Epoch: 92.93 [D loss: 1.376562, acc: 0.00%] [G loss: 0.327325]\n",
            "Epoch: 92.94 [D loss: 1.383223, acc: 0.00%] [G loss: 0.326885]\n",
            "Epoch: 92.95 [D loss: 1.376390, acc: 0.00%] [G loss: 0.327360]\n",
            "Epoch: 92.96 [D loss: 1.417782, acc: 0.00%] [G loss: 0.327267]\n",
            "Epoch: 92.97 [D loss: 1.339199, acc: 0.00%] [G loss: 0.327202]\n",
            "Epoch: 92.98 [D loss: 1.348848, acc: 0.00%] [G loss: 0.327092]\n",
            "Epoch: 92.99 [D loss: 1.341382, acc: 0.00%] [G loss: 0.327619]\n",
            "Epoch: 92.100 [D loss: 1.347051, acc: 0.00%] [G loss: 0.326413]\n",
            "Epoch: 92.101 [D loss: 1.335817, acc: 0.00%] [G loss: 0.326820]\n",
            "Epoch: 92.102 [D loss: 1.408814, acc: 0.00%] [G loss: 0.328087]\n",
            "Epoch: 92.103 [D loss: 1.333177, acc: 0.00%] [G loss: 0.326901]\n",
            "Epoch: 92.104 [D loss: 1.382247, acc: 0.00%] [G loss: 0.327104]\n",
            "Epoch: 92.105 [D loss: 1.387830, acc: 0.00%] [G loss: 0.326596]\n",
            "Epoch: 92.106 [D loss: 1.369867, acc: 0.00%] [G loss: 0.328189]\n",
            "Epoch: 92.107 [D loss: 1.371398, acc: 0.00%] [G loss: 0.327242]\n",
            "Epoch: 93.0 [D loss: 1.386101, acc: 0.00%] [G loss: 0.326904]\n",
            "Epoch: 93.1 [D loss: 1.360528, acc: 0.00%] [G loss: 0.327515]\n",
            "Epoch: 93.2 [D loss: 1.322866, acc: 0.00%] [G loss: 0.327131]\n",
            "Epoch: 93.3 [D loss: 1.322119, acc: 0.00%] [G loss: 0.327800]\n",
            "Epoch: 93.4 [D loss: 1.362395, acc: 0.00%] [G loss: 0.326280]\n",
            "Epoch: 93.5 [D loss: 1.382913, acc: 0.00%] [G loss: 0.327772]\n",
            "Epoch: 93.6 [D loss: 1.356042, acc: 0.00%] [G loss: 0.326817]\n",
            "Epoch: 93.7 [D loss: 1.467643, acc: 0.00%] [G loss: 0.326418]\n",
            "Epoch: 93.8 [D loss: 1.388045, acc: 0.00%] [G loss: 0.328205]\n",
            "Epoch: 93.9 [D loss: 1.325643, acc: 0.00%] [G loss: 0.327111]\n",
            "Epoch: 93.10 [D loss: 1.326180, acc: 0.00%] [G loss: 0.327503]\n",
            "Epoch: 93.11 [D loss: 1.352598, acc: 0.00%] [G loss: 0.328552]\n",
            "Epoch: 93.12 [D loss: 1.308233, acc: 0.00%] [G loss: 0.328606]\n",
            "Epoch: 93.13 [D loss: 1.359746, acc: 0.00%] [G loss: 0.327320]\n",
            "Epoch: 93.14 [D loss: 1.365164, acc: 0.00%] [G loss: 0.328462]\n",
            "Epoch: 93.15 [D loss: 1.364027, acc: 0.00%] [G loss: 0.326704]\n",
            "Epoch: 93.16 [D loss: 1.387099, acc: 0.00%] [G loss: 0.326381]\n",
            "Epoch: 93.17 [D loss: 1.372529, acc: 0.00%] [G loss: 0.326659]\n",
            "Epoch: 93.18 [D loss: 1.419604, acc: 0.00%] [G loss: 0.326733]\n",
            "Epoch: 93.19 [D loss: 1.376076, acc: 0.00%] [G loss: 0.328419]\n",
            "Epoch: 93.20 [D loss: 1.352991, acc: 0.00%] [G loss: 0.327452]\n",
            "Epoch: 93.21 [D loss: 1.350397, acc: 0.00%] [G loss: 0.327373]\n",
            "Epoch: 93.22 [D loss: 1.315753, acc: 0.00%] [G loss: 0.327169]\n",
            "Epoch: 93.23 [D loss: 1.284181, acc: 0.00%] [G loss: 0.327703]\n",
            "Epoch: 93.24 [D loss: 1.345442, acc: 0.00%] [G loss: 0.326934]\n",
            "Epoch: 93.25 [D loss: 1.390627, acc: 0.00%] [G loss: 0.327174]\n",
            "Epoch: 93.26 [D loss: 1.428676, acc: 0.00%] [G loss: 0.326882]\n",
            "Epoch: 93.27 [D loss: 1.427905, acc: 0.00%] [G loss: 0.327141]\n",
            "Epoch: 93.28 [D loss: 1.436897, acc: 0.00%] [G loss: 0.328819]\n",
            "Epoch: 93.29 [D loss: 1.394152, acc: 0.00%] [G loss: 0.326526]\n",
            "Epoch: 93.30 [D loss: 1.344355, acc: 0.00%] [G loss: 0.326945]\n",
            "Epoch: 93.31 [D loss: 1.301949, acc: 0.00%] [G loss: 0.328030]\n",
            "Epoch: 93.32 [D loss: 1.291195, acc: 0.00%] [G loss: 0.328654]\n",
            "Epoch: 93.33 [D loss: 1.321775, acc: 0.00%] [G loss: 0.327966]\n",
            "Epoch: 93.34 [D loss: 1.368149, acc: 0.00%] [G loss: 0.327440]\n",
            "Epoch: 93.35 [D loss: 1.395383, acc: 0.00%] [G loss: 0.327739]\n",
            "Epoch: 93.36 [D loss: 1.438011, acc: 0.00%] [G loss: 0.327960]\n",
            "Epoch: 93.37 [D loss: 1.411464, acc: 0.00%] [G loss: 0.327723]\n",
            "Epoch: 93.38 [D loss: 1.423938, acc: 0.00%] [G loss: 0.328065]\n",
            "Epoch: 93.39 [D loss: 1.421769, acc: 0.00%] [G loss: 0.325864]\n",
            "Epoch: 93.40 [D loss: 1.360252, acc: 0.00%] [G loss: 0.327510]\n",
            "Epoch: 93.41 [D loss: 1.306391, acc: 0.00%] [G loss: 0.326581]\n",
            "Epoch: 93.42 [D loss: 1.279681, acc: 0.00%] [G loss: 0.328381]\n",
            "Epoch: 93.43 [D loss: 1.337341, acc: 0.00%] [G loss: 0.326483]\n",
            "Epoch: 93.44 [D loss: 1.307674, acc: 0.00%] [G loss: 0.326547]\n",
            "Epoch: 93.45 [D loss: 1.388119, acc: 0.00%] [G loss: 0.326624]\n",
            "Epoch: 93.46 [D loss: 1.384786, acc: 0.00%] [G loss: 0.327675]\n",
            "Epoch: 93.47 [D loss: 1.367837, acc: 0.00%] [G loss: 0.327491]\n",
            "Epoch: 93.48 [D loss: 1.369644, acc: 0.00%] [G loss: 0.327131]\n",
            "Epoch: 93.49 [D loss: 1.397141, acc: 0.00%] [G loss: 0.326788]\n",
            "Epoch: 93.50 [D loss: 1.422989, acc: 0.00%] [G loss: 0.327554]\n",
            "Epoch: 93.51 [D loss: 1.358670, acc: 0.00%] [G loss: 0.326675]\n",
            "Epoch: 93.52 [D loss: 1.334884, acc: 0.00%] [G loss: 0.326687]\n",
            "Epoch: 93.53 [D loss: 1.355601, acc: 0.00%] [G loss: 0.327304]\n",
            "Epoch: 93.54 [D loss: 1.391514, acc: 0.00%] [G loss: 0.327136]\n",
            "Epoch: 93.55 [D loss: 1.343179, acc: 0.00%] [G loss: 0.326290]\n",
            "Epoch: 93.56 [D loss: 1.383222, acc: 0.00%] [G loss: 0.326886]\n",
            "Epoch: 93.57 [D loss: 1.385992, acc: 0.00%] [G loss: 0.326721]\n",
            "Epoch: 93.58 [D loss: 1.378843, acc: 0.00%] [G loss: 0.327256]\n",
            "Epoch: 93.59 [D loss: 1.319994, acc: 0.00%] [G loss: 0.326890]\n",
            "Epoch: 93.60 [D loss: 1.400344, acc: 0.00%] [G loss: 0.327066]\n",
            "Epoch: 93.61 [D loss: 1.359937, acc: 0.00%] [G loss: 0.326262]\n",
            "Epoch: 93.62 [D loss: 1.375782, acc: 0.00%] [G loss: 0.326800]\n",
            "Epoch: 93.63 [D loss: 1.385746, acc: 0.00%] [G loss: 0.326141]\n",
            "Epoch: 93.64 [D loss: 1.368978, acc: 0.00%] [G loss: 0.327624]\n",
            "Epoch: 93.65 [D loss: 1.363564, acc: 0.00%] [G loss: 0.326702]\n",
            "Epoch: 93.66 [D loss: 1.294209, acc: 0.00%] [G loss: 0.327404]\n",
            "Epoch: 93.67 [D loss: 1.394933, acc: 0.00%] [G loss: 0.326015]\n",
            "Epoch: 93.68 [D loss: 1.342961, acc: 0.00%] [G loss: 0.328565]\n",
            "Epoch: 93.69 [D loss: 1.355958, acc: 0.00%] [G loss: 0.327146]\n",
            "Epoch: 93.70 [D loss: 1.410557, acc: 0.00%] [G loss: 0.326431]\n",
            "Epoch: 93.71 [D loss: 1.365901, acc: 0.00%] [G loss: 0.327522]\n",
            "Epoch: 93.72 [D loss: 1.366601, acc: 0.00%] [G loss: 0.328016]\n",
            "Epoch: 93.73 [D loss: 1.388825, acc: 0.00%] [G loss: 0.326694]\n",
            "Epoch: 93.74 [D loss: 1.369111, acc: 0.00%] [G loss: 0.327033]\n",
            "Epoch: 93.75 [D loss: 1.361889, acc: 0.00%] [G loss: 0.327144]\n",
            "Epoch: 93.76 [D loss: 1.333599, acc: 0.00%] [G loss: 0.326978]\n",
            "Epoch: 93.77 [D loss: 1.354672, acc: 0.00%] [G loss: 0.327196]\n",
            "Epoch: 93.78 [D loss: 1.378106, acc: 0.00%] [G loss: 0.327265]\n",
            "Epoch: 93.79 [D loss: 1.327854, acc: 0.00%] [G loss: 0.327380]\n",
            "Epoch: 93.80 [D loss: 1.329638, acc: 0.00%] [G loss: 0.326531]\n",
            "Epoch: 93.81 [D loss: 1.394245, acc: 0.00%] [G loss: 0.326998]\n",
            "Epoch: 93.82 [D loss: 1.402163, acc: 0.00%] [G loss: 0.326466]\n",
            "Epoch: 93.83 [D loss: 1.405338, acc: 0.00%] [G loss: 0.327526]\n",
            "Epoch: 93.84 [D loss: 1.403524, acc: 0.00%] [G loss: 0.326812]\n",
            "Epoch: 93.85 [D loss: 1.368569, acc: 0.00%] [G loss: 0.326615]\n",
            "Epoch: 93.86 [D loss: 1.374431, acc: 0.00%] [G loss: 0.327352]\n",
            "Epoch: 93.87 [D loss: 1.395123, acc: 0.00%] [G loss: 0.327061]\n",
            "Epoch: 93.88 [D loss: 1.368888, acc: 0.00%] [G loss: 0.327047]\n",
            "Epoch: 93.89 [D loss: 1.362430, acc: 0.00%] [G loss: 0.327427]\n",
            "Epoch: 93.90 [D loss: 1.350757, acc: 0.00%] [G loss: 0.326825]\n",
            "Epoch: 93.91 [D loss: 1.404684, acc: 0.00%] [G loss: 0.326693]\n",
            "Epoch: 93.92 [D loss: 1.317545, acc: 0.00%] [G loss: 0.326812]\n",
            "Epoch: 93.93 [D loss: 1.351579, acc: 0.00%] [G loss: 0.326381]\n",
            "Epoch: 93.94 [D loss: 1.327772, acc: 0.00%] [G loss: 0.326714]\n",
            "Epoch: 93.95 [D loss: 1.330188, acc: 0.00%] [G loss: 0.326463]\n",
            "Epoch: 93.96 [D loss: 1.313599, acc: 0.00%] [G loss: 0.328716]\n",
            "Epoch: 93.97 [D loss: 1.341623, acc: 0.00%] [G loss: 0.326248]\n",
            "Epoch: 93.98 [D loss: 1.370381, acc: 0.00%] [G loss: 0.327413]\n",
            "Epoch: 93.99 [D loss: 1.426407, acc: 0.00%] [G loss: 0.327032]\n",
            "Epoch: 93.100 [D loss: 1.403833, acc: 0.00%] [G loss: 0.327981]\n",
            "Epoch: 93.101 [D loss: 1.408285, acc: 0.00%] [G loss: 0.327337]\n",
            "Epoch: 93.102 [D loss: 1.417806, acc: 0.00%] [G loss: 0.327451]\n",
            "Epoch: 93.103 [D loss: 1.387003, acc: 0.00%] [G loss: 0.326797]\n",
            "Epoch: 93.104 [D loss: 1.314161, acc: 0.00%] [G loss: 0.327311]\n",
            "Epoch: 93.105 [D loss: 1.340087, acc: 0.00%] [G loss: 0.328388]\n",
            "Epoch: 93.106 [D loss: 1.297867, acc: 0.00%] [G loss: 0.326424]\n",
            "Epoch: 93.107 [D loss: 1.296598, acc: 0.00%] [G loss: 0.327834]\n",
            "Epoch: 94.0 [D loss: 1.290445, acc: 0.00%] [G loss: 0.327141]\n",
            "Epoch: 94.1 [D loss: 1.357810, acc: 0.00%] [G loss: 0.326497]\n",
            "Epoch: 94.2 [D loss: 1.395288, acc: 0.00%] [G loss: 0.327574]\n",
            "Epoch: 94.3 [D loss: 1.384104, acc: 0.00%] [G loss: 0.328626]\n",
            "Epoch: 94.4 [D loss: 1.340055, acc: 0.00%] [G loss: 0.327183]\n",
            "Epoch: 94.5 [D loss: 1.354225, acc: 0.00%] [G loss: 0.326602]\n",
            "Epoch: 94.6 [D loss: 1.339035, acc: 0.00%] [G loss: 0.326365]\n",
            "Epoch: 94.7 [D loss: 1.391789, acc: 0.00%] [G loss: 0.327167]\n",
            "Epoch: 94.8 [D loss: 1.376795, acc: 0.00%] [G loss: 0.327551]\n",
            "Epoch: 94.9 [D loss: 1.404686, acc: 0.00%] [G loss: 0.327829]\n",
            "Epoch: 94.10 [D loss: 1.370629, acc: 0.00%] [G loss: 0.326995]\n",
            "Epoch: 94.11 [D loss: 1.411552, acc: 0.00%] [G loss: 0.327245]\n",
            "Epoch: 94.12 [D loss: 1.354663, acc: 0.00%] [G loss: 0.327762]\n",
            "Epoch: 94.13 [D loss: 1.416735, acc: 0.00%] [G loss: 0.326565]\n",
            "Epoch: 94.14 [D loss: 1.344667, acc: 0.00%] [G loss: 0.326472]\n",
            "Epoch: 94.15 [D loss: 1.364637, acc: 0.00%] [G loss: 0.326756]\n",
            "Epoch: 94.16 [D loss: 1.362965, acc: 0.00%] [G loss: 0.327977]\n",
            "Epoch: 94.17 [D loss: 1.371638, acc: 0.00%] [G loss: 0.326999]\n",
            "Epoch: 94.18 [D loss: 1.417228, acc: 0.00%] [G loss: 0.326358]\n",
            "Epoch: 94.19 [D loss: 1.372269, acc: 0.00%] [G loss: 0.326277]\n",
            "Epoch: 94.20 [D loss: 1.336010, acc: 0.00%] [G loss: 0.329325]\n",
            "Epoch: 94.21 [D loss: 1.318465, acc: 0.00%] [G loss: 0.327359]\n",
            "Epoch: 94.22 [D loss: 1.365356, acc: 0.00%] [G loss: 0.326674]\n",
            "Epoch: 94.23 [D loss: 1.348064, acc: 0.00%] [G loss: 0.327656]\n",
            "Epoch: 94.24 [D loss: 1.349111, acc: 0.00%] [G loss: 0.327279]\n",
            "Epoch: 94.25 [D loss: 1.379687, acc: 0.00%] [G loss: 0.326343]\n",
            "Epoch: 94.26 [D loss: 1.340802, acc: 0.00%] [G loss: 0.326003]\n",
            "Epoch: 94.27 [D loss: 1.380370, acc: 0.00%] [G loss: 0.327502]\n",
            "Epoch: 94.28 [D loss: 1.328302, acc: 0.00%] [G loss: 0.327210]\n",
            "Epoch: 94.29 [D loss: 1.305323, acc: 0.00%] [G loss: 0.327736]\n",
            "Epoch: 94.30 [D loss: 1.353277, acc: 0.00%] [G loss: 0.327291]\n",
            "Epoch: 94.31 [D loss: 1.318381, acc: 0.00%] [G loss: 0.327586]\n",
            "Epoch: 94.32 [D loss: 1.328539, acc: 0.00%] [G loss: 0.327362]\n",
            "Epoch: 94.33 [D loss: 1.325961, acc: 0.00%] [G loss: 0.327556]\n",
            "Epoch: 94.34 [D loss: 1.356343, acc: 0.00%] [G loss: 0.326220]\n",
            "Epoch: 94.35 [D loss: 1.386170, acc: 0.00%] [G loss: 0.326728]\n",
            "Epoch: 94.36 [D loss: 1.309762, acc: 0.00%] [G loss: 0.327310]\n",
            "Epoch: 94.37 [D loss: 1.371876, acc: 0.00%] [G loss: 0.327503]\n",
            "Epoch: 94.38 [D loss: 1.347164, acc: 0.00%] [G loss: 0.326803]\n",
            "Epoch: 94.39 [D loss: 1.375074, acc: 0.00%] [G loss: 0.327143]\n",
            "Epoch: 94.40 [D loss: 1.370785, acc: 0.00%] [G loss: 0.326647]\n",
            "Epoch: 94.41 [D loss: 1.373274, acc: 0.00%] [G loss: 0.328038]\n",
            "Epoch: 94.42 [D loss: 1.360664, acc: 0.00%] [G loss: 0.327299]\n",
            "Epoch: 94.43 [D loss: 1.320280, acc: 0.00%] [G loss: 0.327082]\n",
            "Epoch: 94.44 [D loss: 1.333202, acc: 0.00%] [G loss: 0.327460]\n",
            "Epoch: 94.45 [D loss: 1.345545, acc: 0.00%] [G loss: 0.327907]\n",
            "Epoch: 94.46 [D loss: 1.336134, acc: 0.00%] [G loss: 0.326824]\n",
            "Epoch: 94.47 [D loss: 1.368884, acc: 0.00%] [G loss: 0.327540]\n",
            "Epoch: 94.48 [D loss: 1.374319, acc: 0.00%] [G loss: 0.328802]\n",
            "Epoch: 94.49 [D loss: 1.420981, acc: 0.00%] [G loss: 0.327028]\n",
            "Epoch: 94.50 [D loss: 1.380667, acc: 0.00%] [G loss: 0.326802]\n",
            "Epoch: 94.51 [D loss: 1.345648, acc: 0.00%] [G loss: 0.327005]\n",
            "Epoch: 94.52 [D loss: 1.354984, acc: 0.00%] [G loss: 0.327957]\n",
            "Epoch: 94.53 [D loss: 1.331996, acc: 0.00%] [G loss: 0.326606]\n",
            "Epoch: 94.54 [D loss: 1.364405, acc: 0.00%] [G loss: 0.326774]\n",
            "Epoch: 94.55 [D loss: 1.373078, acc: 0.00%] [G loss: 0.326657]\n",
            "Epoch: 94.56 [D loss: 1.390425, acc: 0.00%] [G loss: 0.327260]\n",
            "Epoch: 94.57 [D loss: 1.427785, acc: 0.00%] [G loss: 0.327656]\n",
            "Epoch: 94.58 [D loss: 1.292819, acc: 0.00%] [G loss: 0.327650]\n",
            "Epoch: 94.59 [D loss: 1.301416, acc: 0.00%] [G loss: 0.327490]\n",
            "Epoch: 94.60 [D loss: 1.300514, acc: 0.00%] [G loss: 0.327511]\n",
            "Epoch: 94.61 [D loss: 1.312422, acc: 0.00%] [G loss: 0.328134]\n",
            "Epoch: 94.62 [D loss: 1.357175, acc: 0.00%] [G loss: 0.327673]\n",
            "Epoch: 94.63 [D loss: 1.326827, acc: 0.00%] [G loss: 0.327599]\n",
            "Epoch: 94.64 [D loss: 1.349044, acc: 0.00%] [G loss: 0.327366]\n",
            "Epoch: 94.65 [D loss: 1.384873, acc: 0.00%] [G loss: 0.327609]\n",
            "Epoch: 94.66 [D loss: 1.389023, acc: 0.00%] [G loss: 0.327312]\n",
            "Epoch: 94.67 [D loss: 1.342002, acc: 0.00%] [G loss: 0.327919]\n",
            "Epoch: 94.68 [D loss: 1.378432, acc: 0.00%] [G loss: 0.326777]\n",
            "Epoch: 94.69 [D loss: 1.325836, acc: 0.00%] [G loss: 0.326650]\n",
            "Epoch: 94.70 [D loss: 1.349156, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 94.71 [D loss: 1.384141, acc: 0.00%] [G loss: 0.326805]\n",
            "Epoch: 94.72 [D loss: 1.356226, acc: 0.00%] [G loss: 0.327460]\n",
            "Epoch: 94.73 [D loss: 1.380661, acc: 0.00%] [G loss: 0.326261]\n",
            "Epoch: 94.74 [D loss: 1.371172, acc: 0.00%] [G loss: 0.327038]\n",
            "Epoch: 94.75 [D loss: 1.394559, acc: 0.00%] [G loss: 0.327256]\n",
            "Epoch: 94.76 [D loss: 1.381565, acc: 0.00%] [G loss: 0.327398]\n",
            "Epoch: 94.77 [D loss: 1.374086, acc: 0.00%] [G loss: 0.327212]\n",
            "Epoch: 94.78 [D loss: 1.357426, acc: 0.00%] [G loss: 0.327256]\n",
            "Epoch: 94.79 [D loss: 1.430151, acc: 0.00%] [G loss: 0.327280]\n",
            "Epoch: 94.80 [D loss: 1.390106, acc: 0.00%] [G loss: 0.328176]\n",
            "Epoch: 94.81 [D loss: 1.354679, acc: 0.00%] [G loss: 0.326755]\n",
            "Epoch: 94.82 [D loss: 1.326924, acc: 0.00%] [G loss: 0.327301]\n",
            "Epoch: 94.83 [D loss: 1.374814, acc: 0.00%] [G loss: 0.327239]\n",
            "Epoch: 94.84 [D loss: 1.357069, acc: 0.00%] [G loss: 0.327517]\n",
            "Epoch: 94.85 [D loss: 1.383885, acc: 0.00%] [G loss: 0.327697]\n",
            "Epoch: 94.86 [D loss: 1.340667, acc: 0.00%] [G loss: 0.327544]\n",
            "Epoch: 94.87 [D loss: 1.370425, acc: 0.00%] [G loss: 0.327071]\n",
            "Epoch: 94.88 [D loss: 1.340484, acc: 0.00%] [G loss: 0.327081]\n",
            "Epoch: 94.89 [D loss: 1.365038, acc: 0.00%] [G loss: 0.326716]\n",
            "Epoch: 94.90 [D loss: 1.351771, acc: 0.00%] [G loss: 0.327622]\n",
            "Epoch: 94.91 [D loss: 1.358229, acc: 0.00%] [G loss: 0.327489]\n",
            "Epoch: 94.92 [D loss: 1.323703, acc: 0.00%] [G loss: 0.327667]\n",
            "Epoch: 94.93 [D loss: 1.376694, acc: 0.00%] [G loss: 0.326787]\n",
            "Epoch: 94.94 [D loss: 1.334998, acc: 0.00%] [G loss: 0.327618]\n",
            "Epoch: 94.95 [D loss: 1.387281, acc: 0.00%] [G loss: 0.327159]\n",
            "Epoch: 94.96 [D loss: 1.329499, acc: 0.00%] [G loss: 0.327043]\n",
            "Epoch: 94.97 [D loss: 1.325864, acc: 0.00%] [G loss: 0.327077]\n",
            "Epoch: 94.98 [D loss: 1.352683, acc: 0.00%] [G loss: 0.328159]\n",
            "Epoch: 94.99 [D loss: 1.403847, acc: 0.00%] [G loss: 0.325973]\n",
            "Epoch: 94.100 [D loss: 1.410426, acc: 0.00%] [G loss: 0.327193]\n",
            "Epoch: 94.101 [D loss: 1.380596, acc: 0.00%] [G loss: 0.327450]\n",
            "Epoch: 94.102 [D loss: 1.385330, acc: 0.00%] [G loss: 0.326810]\n",
            "Epoch: 94.103 [D loss: 1.369428, acc: 0.00%] [G loss: 0.327843]\n",
            "Epoch: 94.104 [D loss: 1.349300, acc: 0.00%] [G loss: 0.326650]\n",
            "Epoch: 94.105 [D loss: 1.323959, acc: 0.00%] [G loss: 0.327590]\n",
            "Epoch: 94.106 [D loss: 1.354912, acc: 0.00%] [G loss: 0.327247]\n",
            "Epoch: 94.107 [D loss: 1.355788, acc: 0.00%] [G loss: 0.328633]\n",
            "Epoch: 95.0 [D loss: 1.321631, acc: 0.00%] [G loss: 0.326902]\n",
            "Epoch: 95.1 [D loss: 1.339695, acc: 0.00%] [G loss: 0.326988]\n",
            "Epoch: 95.2 [D loss: 1.367150, acc: 0.00%] [G loss: 0.327603]\n",
            "Epoch: 95.3 [D loss: 1.408994, acc: 0.00%] [G loss: 0.327205]\n",
            "Epoch: 95.4 [D loss: 1.372444, acc: 0.00%] [G loss: 0.327243]\n",
            "Epoch: 95.5 [D loss: 1.382749, acc: 0.00%] [G loss: 0.327766]\n",
            "Epoch: 95.6 [D loss: 1.337600, acc: 0.00%] [G loss: 0.326368]\n",
            "Epoch: 95.7 [D loss: 1.428529, acc: 0.00%] [G loss: 0.326651]\n",
            "Epoch: 95.8 [D loss: 1.374404, acc: 0.00%] [G loss: 0.326752]\n",
            "Epoch: 95.9 [D loss: 1.393117, acc: 0.00%] [G loss: 0.327163]\n",
            "Epoch: 95.10 [D loss: 1.352411, acc: 0.00%] [G loss: 0.327187]\n",
            "Epoch: 95.11 [D loss: 1.399519, acc: 0.00%] [G loss: 0.326488]\n",
            "Epoch: 95.12 [D loss: 1.373328, acc: 0.00%] [G loss: 0.327192]\n",
            "Epoch: 95.13 [D loss: 1.344000, acc: 0.00%] [G loss: 0.327941]\n",
            "Epoch: 95.14 [D loss: 1.367160, acc: 0.00%] [G loss: 0.326827]\n",
            "Epoch: 95.15 [D loss: 1.400502, acc: 0.00%] [G loss: 0.326846]\n",
            "Epoch: 95.16 [D loss: 1.355703, acc: 0.00%] [G loss: 0.326900]\n",
            "Epoch: 95.17 [D loss: 1.363710, acc: 0.00%] [G loss: 0.328149]\n",
            "Epoch: 95.18 [D loss: 1.297592, acc: 0.00%] [G loss: 0.326533]\n",
            "Epoch: 95.19 [D loss: 1.334504, acc: 0.00%] [G loss: 0.329199]\n",
            "Epoch: 95.20 [D loss: 1.336792, acc: 0.00%] [G loss: 0.325880]\n",
            "Epoch: 95.21 [D loss: 1.353584, acc: 0.00%] [G loss: 0.326932]\n",
            "Epoch: 95.22 [D loss: 1.420383, acc: 0.00%] [G loss: 0.327647]\n",
            "Epoch: 95.23 [D loss: 1.414913, acc: 0.00%] [G loss: 0.326782]\n",
            "Epoch: 95.24 [D loss: 1.391721, acc: 0.00%] [G loss: 0.327689]\n",
            "Epoch: 95.25 [D loss: 1.403254, acc: 0.00%] [G loss: 0.328466]\n",
            "Epoch: 95.26 [D loss: 1.386195, acc: 0.00%] [G loss: 0.326662]\n",
            "Epoch: 95.27 [D loss: 1.329928, acc: 0.00%] [G loss: 0.327652]\n",
            "Epoch: 95.28 [D loss: 1.324072, acc: 0.00%] [G loss: 0.328211]\n",
            "Epoch: 95.29 [D loss: 1.319306, acc: 0.00%] [G loss: 0.328212]\n",
            "Epoch: 95.30 [D loss: 1.292815, acc: 0.00%] [G loss: 0.326917]\n",
            "Epoch: 95.31 [D loss: 1.355647, acc: 0.00%] [G loss: 0.326562]\n",
            "Epoch: 95.32 [D loss: 1.398121, acc: 0.00%] [G loss: 0.326529]\n",
            "Epoch: 95.33 [D loss: 1.403554, acc: 0.00%] [G loss: 0.326821]\n",
            "Epoch: 95.34 [D loss: 1.350140, acc: 0.00%] [G loss: 0.326360]\n",
            "Epoch: 95.35 [D loss: 1.367362, acc: 0.00%] [G loss: 0.326962]\n",
            "Epoch: 95.36 [D loss: 1.346918, acc: 0.00%] [G loss: 0.326913]\n",
            "Epoch: 95.37 [D loss: 1.328929, acc: 0.00%] [G loss: 0.328308]\n",
            "Epoch: 95.38 [D loss: 1.402183, acc: 0.00%] [G loss: 0.326699]\n",
            "Epoch: 95.39 [D loss: 1.358622, acc: 0.00%] [G loss: 0.326255]\n",
            "Epoch: 95.40 [D loss: 1.393279, acc: 0.00%] [G loss: 0.327217]\n",
            "Epoch: 95.41 [D loss: 1.396699, acc: 0.00%] [G loss: 0.326708]\n",
            "Epoch: 95.42 [D loss: 1.398911, acc: 0.00%] [G loss: 0.327082]\n",
            "Epoch: 95.43 [D loss: 1.327297, acc: 0.00%] [G loss: 0.326562]\n",
            "Epoch: 95.44 [D loss: 1.356506, acc: 0.00%] [G loss: 0.326954]\n",
            "Epoch: 95.45 [D loss: 1.260018, acc: 0.00%] [G loss: 0.327956]\n",
            "Epoch: 95.46 [D loss: 1.313315, acc: 0.00%] [G loss: 0.326546]\n",
            "Epoch: 95.47 [D loss: 1.352344, acc: 0.00%] [G loss: 0.326626]\n",
            "Epoch: 95.48 [D loss: 1.304024, acc: 0.00%] [G loss: 0.327128]\n",
            "Epoch: 95.49 [D loss: 1.392877, acc: 0.00%] [G loss: 0.328707]\n",
            "Epoch: 95.50 [D loss: 1.378744, acc: 0.00%] [G loss: 0.328655]\n",
            "Epoch: 95.51 [D loss: 1.380823, acc: 0.00%] [G loss: 0.326287]\n",
            "Epoch: 95.52 [D loss: 1.360164, acc: 0.00%] [G loss: 0.327014]\n",
            "Epoch: 95.53 [D loss: 1.380366, acc: 0.00%] [G loss: 0.327307]\n",
            "Epoch: 95.54 [D loss: 1.362226, acc: 0.00%] [G loss: 0.326754]\n",
            "Epoch: 95.55 [D loss: 1.371697, acc: 0.00%] [G loss: 0.328123]\n",
            "Epoch: 95.56 [D loss: 1.341000, acc: 0.00%] [G loss: 0.327428]\n",
            "Epoch: 95.57 [D loss: 1.376643, acc: 0.00%] [G loss: 0.327338]\n",
            "Epoch: 95.58 [D loss: 1.319324, acc: 0.00%] [G loss: 0.326539]\n",
            "Epoch: 95.59 [D loss: 1.388997, acc: 0.00%] [G loss: 0.326665]\n",
            "Epoch: 95.60 [D loss: 1.367233, acc: 0.00%] [G loss: 0.326562]\n",
            "Epoch: 95.61 [D loss: 1.377205, acc: 0.00%] [G loss: 0.327878]\n",
            "Epoch: 95.62 [D loss: 1.415404, acc: 0.00%] [G loss: 0.327770]\n",
            "Epoch: 95.63 [D loss: 1.398504, acc: 0.00%] [G loss: 0.326721]\n",
            "Epoch: 95.64 [D loss: 1.400184, acc: 0.00%] [G loss: 0.328039]\n",
            "Epoch: 95.65 [D loss: 1.447362, acc: 0.00%] [G loss: 0.327771]\n",
            "Epoch: 95.66 [D loss: 1.396623, acc: 0.00%] [G loss: 0.328312]\n",
            "Epoch: 95.67 [D loss: 1.387830, acc: 0.00%] [G loss: 0.326638]\n",
            "Epoch: 95.68 [D loss: 1.321186, acc: 0.00%] [G loss: 0.326207]\n",
            "Epoch: 95.69 [D loss: 1.299531, acc: 0.00%] [G loss: 0.329716]\n",
            "Epoch: 95.70 [D loss: 1.315804, acc: 0.00%] [G loss: 0.328299]\n",
            "Epoch: 95.71 [D loss: 1.352859, acc: 0.00%] [G loss: 0.326876]\n",
            "Epoch: 95.72 [D loss: 1.406133, acc: 0.00%] [G loss: 0.328390]\n",
            "Epoch: 95.73 [D loss: 1.406738, acc: 0.00%] [G loss: 0.327475]\n",
            "Epoch: 95.74 [D loss: 1.404007, acc: 0.00%] [G loss: 0.326195]\n",
            "Epoch: 95.75 [D loss: 1.381397, acc: 0.00%] [G loss: 0.327412]\n",
            "Epoch: 95.76 [D loss: 1.387434, acc: 0.00%] [G loss: 0.327681]\n",
            "Epoch: 95.77 [D loss: 1.363225, acc: 0.00%] [G loss: 0.327465]\n",
            "Epoch: 95.78 [D loss: 1.376550, acc: 0.00%] [G loss: 0.326114]\n",
            "Epoch: 95.79 [D loss: 1.369951, acc: 0.00%] [G loss: 0.326789]\n",
            "Epoch: 95.80 [D loss: 1.317863, acc: 0.00%] [G loss: 0.328339]\n",
            "Epoch: 95.81 [D loss: 1.350381, acc: 0.00%] [G loss: 0.328517]\n",
            "Epoch: 95.82 [D loss: 1.342891, acc: 0.00%] [G loss: 0.328594]\n",
            "Epoch: 95.83 [D loss: 1.400008, acc: 0.00%] [G loss: 0.327469]\n",
            "Epoch: 95.84 [D loss: 1.379480, acc: 0.00%] [G loss: 0.326985]\n",
            "Epoch: 95.85 [D loss: 1.392672, acc: 0.00%] [G loss: 0.328496]\n",
            "Epoch: 95.86 [D loss: 1.407289, acc: 0.00%] [G loss: 0.327408]\n",
            "Epoch: 95.87 [D loss: 1.430929, acc: 0.00%] [G loss: 0.329178]\n",
            "Epoch: 95.88 [D loss: 1.363440, acc: 0.00%] [G loss: 0.326829]\n",
            "Epoch: 95.89 [D loss: 1.329029, acc: 0.00%] [G loss: 0.327278]\n",
            "Epoch: 95.90 [D loss: 1.325332, acc: 0.00%] [G loss: 0.327064]\n",
            "Epoch: 95.91 [D loss: 1.260443, acc: 0.00%] [G loss: 0.327808]\n",
            "Epoch: 95.92 [D loss: 1.327607, acc: 0.00%] [G loss: 0.328068]\n",
            "Epoch: 95.93 [D loss: 1.382802, acc: 0.00%] [G loss: 0.327711]\n",
            "Epoch: 95.94 [D loss: 1.372713, acc: 0.00%] [G loss: 0.327580]\n",
            "Epoch: 95.95 [D loss: 1.382810, acc: 0.00%] [G loss: 0.327729]\n",
            "Epoch: 95.96 [D loss: 1.424492, acc: 0.00%] [G loss: 0.327904]\n",
            "Epoch: 95.97 [D loss: 1.389437, acc: 0.00%] [G loss: 0.327748]\n",
            "Epoch: 95.98 [D loss: 1.463550, acc: 0.00%] [G loss: 0.326757]\n",
            "Epoch: 95.99 [D loss: 1.332181, acc: 0.00%] [G loss: 0.328081]\n",
            "Epoch: 95.100 [D loss: 1.313794, acc: 0.00%] [G loss: 0.327880]\n",
            "Epoch: 95.101 [D loss: 1.274757, acc: 0.00%] [G loss: 0.327699]\n",
            "Epoch: 95.102 [D loss: 1.323514, acc: 0.00%] [G loss: 0.328086]\n",
            "Epoch: 95.103 [D loss: 1.329320, acc: 0.00%] [G loss: 0.326485]\n",
            "Epoch: 95.104 [D loss: 1.391181, acc: 0.00%] [G loss: 0.326631]\n",
            "Epoch: 95.105 [D loss: 1.402660, acc: 0.00%] [G loss: 0.327030]\n",
            "Epoch: 95.106 [D loss: 1.417648, acc: 0.00%] [G loss: 0.327300]\n",
            "Epoch: 95.107 [D loss: 1.387569, acc: 0.00%] [G loss: 0.327006]\n",
            "Epoch: 96.0 [D loss: 1.373108, acc: 0.00%] [G loss: 0.329477]\n",
            "Epoch: 96.1 [D loss: 1.387369, acc: 0.00%] [G loss: 0.326294]\n",
            "Epoch: 96.2 [D loss: 1.346405, acc: 0.00%] [G loss: 0.326913]\n",
            "Epoch: 96.3 [D loss: 1.310415, acc: 0.00%] [G loss: 0.327652]\n",
            "Epoch: 96.4 [D loss: 1.264546, acc: 0.00%] [G loss: 0.327299]\n",
            "Epoch: 96.5 [D loss: 1.325977, acc: 0.00%] [G loss: 0.325879]\n",
            "Epoch: 96.6 [D loss: 1.342075, acc: 0.00%] [G loss: 0.327057]\n",
            "Epoch: 96.7 [D loss: 1.352634, acc: 0.00%] [G loss: 0.327834]\n",
            "Epoch: 96.8 [D loss: 1.393064, acc: 0.00%] [G loss: 0.327369]\n",
            "Epoch: 96.9 [D loss: 1.379221, acc: 0.00%] [G loss: 0.327160]\n",
            "Epoch: 96.10 [D loss: 1.395882, acc: 0.00%] [G loss: 0.326889]\n",
            "Epoch: 96.11 [D loss: 1.389764, acc: 0.00%] [G loss: 0.328242]\n",
            "Epoch: 96.12 [D loss: 1.341249, acc: 0.00%] [G loss: 0.326648]\n",
            "Epoch: 96.13 [D loss: 1.332514, acc: 0.00%] [G loss: 0.326357]\n",
            "Epoch: 96.14 [D loss: 1.332853, acc: 0.00%] [G loss: 0.327915]\n",
            "Epoch: 96.15 [D loss: 1.340470, acc: 0.00%] [G loss: 0.328342]\n",
            "Epoch: 96.16 [D loss: 1.385695, acc: 0.00%] [G loss: 0.326571]\n",
            "Epoch: 96.17 [D loss: 1.375166, acc: 0.00%] [G loss: 0.327086]\n",
            "Epoch: 96.18 [D loss: 1.393207, acc: 0.00%] [G loss: 0.329010]\n",
            "Epoch: 96.19 [D loss: 1.443847, acc: 0.00%] [G loss: 0.328388]\n",
            "Epoch: 96.20 [D loss: 1.389257, acc: 0.00%] [G loss: 0.328279]\n",
            "Epoch: 96.21 [D loss: 1.377184, acc: 0.00%] [G loss: 0.327214]\n",
            "Epoch: 96.22 [D loss: 1.404370, acc: 0.00%] [G loss: 0.327440]\n",
            "Epoch: 96.23 [D loss: 1.366080, acc: 0.00%] [G loss: 0.328394]\n",
            "Epoch: 96.24 [D loss: 1.326077, acc: 0.00%] [G loss: 0.326721]\n",
            "Epoch: 96.25 [D loss: 1.312173, acc: 0.00%] [G loss: 0.328226]\n",
            "Epoch: 96.26 [D loss: 1.341390, acc: 0.00%] [G loss: 0.327735]\n",
            "Epoch: 96.27 [D loss: 1.403177, acc: 0.00%] [G loss: 0.327122]\n",
            "Epoch: 96.28 [D loss: 1.419223, acc: 0.00%] [G loss: 0.328589]\n",
            "Epoch: 96.29 [D loss: 1.449337, acc: 0.00%] [G loss: 0.327414]\n",
            "Epoch: 96.30 [D loss: 1.417276, acc: 0.00%] [G loss: 0.327150]\n",
            "Epoch: 96.31 [D loss: 1.369562, acc: 0.00%] [G loss: 0.327043]\n",
            "Epoch: 96.32 [D loss: 1.361052, acc: 0.00%] [G loss: 0.326447]\n",
            "Epoch: 96.33 [D loss: 1.378726, acc: 0.00%] [G loss: 0.327806]\n",
            "Epoch: 96.34 [D loss: 1.330724, acc: 0.00%] [G loss: 0.327723]\n",
            "Epoch: 96.35 [D loss: 1.329150, acc: 0.00%] [G loss: 0.327739]\n",
            "Epoch: 96.36 [D loss: 1.346118, acc: 0.00%] [G loss: 0.326347]\n",
            "Epoch: 96.37 [D loss: 1.352410, acc: 0.00%] [G loss: 0.327096]\n",
            "Epoch: 96.38 [D loss: 1.372673, acc: 0.00%] [G loss: 0.327345]\n",
            "Epoch: 96.39 [D loss: 1.392905, acc: 0.00%] [G loss: 0.327488]\n",
            "Epoch: 96.40 [D loss: 1.351566, acc: 0.00%] [G loss: 0.326718]\n",
            "Epoch: 96.41 [D loss: 1.307505, acc: 0.00%] [G loss: 0.327112]\n",
            "Epoch: 96.42 [D loss: 1.299853, acc: 0.00%] [G loss: 0.328056]\n",
            "Epoch: 96.43 [D loss: 1.346628, acc: 0.00%] [G loss: 0.328263]\n",
            "Epoch: 96.44 [D loss: 1.340953, acc: 0.00%] [G loss: 0.327780]\n",
            "Epoch: 96.45 [D loss: 1.418470, acc: 0.00%] [G loss: 0.327087]\n",
            "Epoch: 96.46 [D loss: 1.446526, acc: 0.00%] [G loss: 0.327383]\n",
            "Epoch: 96.47 [D loss: 1.397034, acc: 0.00%] [G loss: 0.327356]\n",
            "Epoch: 96.48 [D loss: 1.410530, acc: 0.00%] [G loss: 0.327239]\n",
            "Epoch: 96.49 [D loss: 1.357087, acc: 0.00%] [G loss: 0.328375]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-046d829c278c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GAN_weights/gen_%.8f_weights\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-046d829c278c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X_train, epochs, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;31m# Train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2046\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m     \"\"\"\n\u001b[0;32m-> 2048\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m   5548\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5549\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 5550\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   5551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5552\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4533\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4534\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3243\u001b[0m     \"\"\"\n\u001b[1;32m   3244\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3245\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3247\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3210\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3557\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3400\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4508\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4509\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4510\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4511\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4512\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4439\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4440\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4441\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         index_remainder = tf.data.Dataset.from_tensors(tf.slice(\n\u001b[0;32m--> 317\u001b[0;31m             indices, [num_in_full_batch], [self._partial_batch_size]))\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mflat_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_remainder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[0;34m(input_, begin, size, name)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   \"\"\"\n\u001b[0;32m-> 1109\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[0;34m(input, begin, size, name)\u001b[0m\n\u001b[1;32m   9581\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9582\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m-> 9583\u001b[0;31m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[1;32m   9584\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9585\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    370\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m   \u001b[0minput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;31m# Perform input type inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6636\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6637\u001b[0m   \"\"\"\n\u001b[0;32m-> 6638\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6639\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minternal_name_scope_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train(X_train ,epochs, batch_size=64):\n",
        "  print(X_train.shape)\n",
        "\n",
        "  #Rescaling data\n",
        "  batches_per_epo = int(X_train.shape[0] / batch_size)\n",
        "\n",
        "  #Create Y\n",
        "  valid = np.ones((batch_size, 1)) - 0.1\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for j in range(batches_per_epo):\n",
        "      # Get random batch\n",
        "      idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "      imgs = X_train[idx]\n",
        "\n",
        "      # Generate fake images\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "      gen_imgs = generator.predict(noise)\n",
        "\n",
        "      # Train discriminator\n",
        "      d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "      d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "      #inverse y label\n",
        "      g_loss = GAN.train_on_batch(noise, valid)\n",
        "\n",
        "      print(\"Epoch: %d.%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, j, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    if (epoch % 10 == 0):\n",
        "      save_imgs()\n",
        "      discriminator.save(\"GAN_weights/dis_%.8f_weights\" % save_name)\n",
        "      generator.save(\"GAN_weights/gen_%.8f_weights\" % save_name)\n",
        "\n",
        "train(X_train, 500, batch_size = 24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi0ws7hoQL4v",
        "outputId": "ceb611b6-9e22-4418-cf14-6e79a0d6bed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.17909607 0.1505473  0.20603749]\n",
            "[0.17839648 0.14995922 0.20523266]\n"
          ]
        }
      ],
      "source": [
        "noise = np.random.normal(0, 1, (16, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "gen_imgs_1 = (gen_imgs + 1) / 2.0\n",
        "gen_imgs_2 = ((gen_imgs + 1) * 127.5)/256 \n",
        "print(gen_imgs_1[0][0][0]) \n",
        "print(gen_imgs_2[0][0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN Results: Generated vs Real"
      ],
      "metadata": {
        "id": "TM5JeN6syfrb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "DINxRea-SWvK",
        "outputId": "c519f9c8-6404-459c-8c92-0f0a66e0f784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Real Image')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "noise = np.random.normal(0, 1, (16, latent_dim))\n",
        "gen_imgs = generator.predict(noise)\n",
        "random_real = np.random.randint(0,2612)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "axs[0].imshow(gen_imgs[1])\n",
        "axs[1].imshow(X_train[random_real])\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Generated Image')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Real Image')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "BobRossGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}